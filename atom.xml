<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>王敏的博客</title>
  <subtitle>按照自己的方式去度过人生</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://ynuwm.github.io/"/>
  <updated>2018-10-27T00:22:29.000Z</updated>
  <id>http://ynuwm.github.io/</id>
  
  <author>
    <name>王敏</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>你为什么不开心了</title>
    <link href="http://ynuwm.github.io/2018/10/27/%E4%BD%A0%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E5%BC%80%E5%BF%83%E4%BA%86/"/>
    <id>http://ynuwm.github.io/2018/10/27/你为什么不开心了/</id>
    <published>2018-10-27T00:08:59.000Z</published>
    <updated>2018-10-27T00:22:29.000Z</updated>
    
    <content type="html"><![CDATA[<p>我慢慢明白了为什么我不快乐<br>因为我总是期待一个结果<br>看一本书期待它让我变得深刻<br>吃饭游泳期待它让我一斤斤瘦下来<br>发一条短信期待它被回复<br>对别人好期待被回待以好<br>写一个故事说一个心情期待被关注被安慰<br>参加一个活动期待换来充实丰富的经历<br>这些预设的期待如果实现了<br>长舒一口气<br>如果没有实现呢<br>自怨自艾<br>可是小时候也是同一个我<br>用一个下午的时间看蚂蚁搬家……</p>
<p>虚度不是荒废<br>是对紧绷日子的对抗和调和<br>虚度就是做些无用的小事<br>取悦自己<br>当有一天<br>我们开始卸下心上纷纷的欲念<br>才能与本真快乐的另一个自己相逢<br>才能重新发现这珍贵的人间</p>
<p>————马德《允许自己虚度时光》</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我慢慢明白了为什么我不快乐&lt;br&gt;因为我总是期待一个结果&lt;br&gt;看一本书期待它让我变得深刻&lt;br&gt;吃饭游泳期待它让我一斤斤瘦下来&lt;br&gt;发一条短信期待它被回复&lt;br&gt;对别人好期待被回待以好&lt;br&gt;写一个故事说一个心情期待被关注被安慰&lt;br&gt;参加一个活动期待换来充实丰富的经历
    
    </summary>
    
      <category term="日记" scheme="http://ynuwm.github.io/categories/%E6%97%A5%E8%AE%B0/"/>
    
    
      <category term="开心" scheme="http://ynuwm.github.io/tags/%E5%BC%80%E5%BF%83/"/>
    
  </entry>
  
  <entry>
    <title>白色满天星</title>
    <link href="http://ynuwm.github.io/2018/05/29/%E6%80%8E%E4%B9%88%E4%BC%9A%E6%9C%89%E9%82%A3%E4%B9%88%E5%83%8F%E7%9A%84%E4%B8%A4%E4%B8%AA%E4%BA%BA/"/>
    <id>http://ynuwm.github.io/2018/05/29/怎么会有那么像的两个人/</id>
    <published>2018-05-29T13:47:55.000Z</published>
    <updated>2018-06-05T14:34:50.000Z</updated>
    
    <content type="html"><![CDATA[<p>见过那么多的花<br>还是满天星最美</p>
<p><img src="/2018/05/29/怎么会有那么像的两个人/0909.jpg" alt="img"> </p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;见过那么多的花&lt;br&gt;还是满天星最美&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2018/05/29/怎么会有那么像的两个人/0909.jpg&quot; alt=&quot;img&quot;&gt; &lt;/p&gt;

    
    </summary>
    
      <category term="日记" scheme="http://ynuwm.github.io/categories/%E6%97%A5%E8%AE%B0/"/>
    
    
      <category term="心情" scheme="http://ynuwm.github.io/tags/%E5%BF%83%E6%83%85/"/>
    
  </entry>
  
  <entry>
    <title>AI三大领域</title>
    <link href="http://ynuwm.github.io/2018/05/27/AI%E4%B8%89%E5%A4%A7%E9%A2%86%E5%9F%9F/"/>
    <id>http://ynuwm.github.io/2018/05/27/AI三大领域/</id>
    <published>2018-05-27T02:21:30.000Z</published>
    <updated>2018-05-31T13:39:26.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-计算机视觉"><a href="#1-计算机视觉" class="headerlink" title="1,计算机视觉"></a>1,计算机视觉</h3><p>由简到难分为三个层次：处理、识别检测和分析理解。<br>图像处理层面主要是对图像的像素的处理；<br>图像识别检测包含语义信息的简单探索；<br>图像理解还有很多值得研究的地方。</p>
<p>图像处理层面主要包含去噪声、去模糊、超分辨率处理、滤镜处理等；<br>图像识别检测包含图像的分类、定位、检测、分割；<br>图像理解主要有基于文本的图像搜索、图像描述生成、图像问答（给定图像和问题，输出答案）等。</p>
<h3 id="2-语音处理"><a href="#2-语音处理" class="headerlink" title="2,语音处理"></a>2,语音处理</h3><p>完整的语音系统包括前端的信号处理、中间的语音语义识别和对话管理（更多涉及自然语言处理）、以及后期的语音合成。</p>
<p>语音的前端处理中包含几个模块。<br>说话人声检测：有效地检测说话人声开始和结束时刻, 区分说话人声与背景声；<br>回声消除：当音箱在播放音乐时，为了不暂停音乐而进行有效的语音识别，需要消除来自扬声器的音乐干扰；<br>唤醒词识别：人类与机器交流的触发方式，就像日常生活中需要与其他人说话时，你会先喊一下那个人的名字；<br>麦克风阵列处理：对声源进行定位，增强说话人方向的信号、抑制其他方向的噪音信号；<br>语音增强：对说话人语音区域进一步增强,、环境噪声区域进一步抑制,有效降低远场语音的衰减。</p>
<p>语音识别的过程需要经历特征提取、模型自适应、声学模型、语言模型、动态解码等多个过程。除了前面提到的远场识别问题之外，还有许多前沿研究集中在解决“鸡尾酒会问题”。</p>
<p>考虑到语义识别和对话管理环节更多是属于自然语言处理的范畴，剩下的就是语音合成环节。<br>语音合成的几个步骤包括：文本分析、语言学分析、音长估算、发音参数估计等。</p>
<h3 id="3-自然语言处理"><a href="#3-自然语言处理" class="headerlink" title="3,自然语言处理"></a>3,自然语言处理</h3><p>NLP包括知识的获取与表达、自然语言理解、自然语言生成等等，也相应出现了知识图谱、对话管理、机器翻译等研究方向，与前述的处理环节形成多对多的映射关系。</p>
<p>知识图谱是基于语义层面对知识进行组织后得到的结构化结果，可以用来回答简单事实类的问题。 包括语言知识图谱（词义上下位、同义词等）、常识知识图谱（“鸟会飞但兔子不会飞”）、实体关系图谱（“刘德华的妻子是朱丽倩”）。知识图谱的构建过程其实就是获取知识、表示知识、应用知识的过程。  </p>
<p>语义理解是自然语言处理中的最大难题，以中文为例，这里面需要解决4个困难:<br>首先是歧义消除，包括词语层面、短语层面、句子层面的的歧义；<br>其次是上下文关联性，例如“小明欺负小李，所以我批评了他。”，需要依靠上下文才知道我批评的是调皮的小明；<br>第三是意图识别，“晴天”可以指天气也可以指周杰伦的歌<br>最后一块是情感识别，显性与隐性的情感识别（“我不高兴”和“我考试没考好”都是用户在表示心情低落）。</p>
<p>目前对话管理主要包含三种情形，按照涉及知识的通用到专业，依次是闲聊、问答、任务驱动型对话。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;1-计算机视觉&quot;&gt;&lt;a href=&quot;#1-计算机视觉&quot; class=&quot;headerlink&quot; title=&quot;1,计算机视觉&quot;&gt;&lt;/a&gt;1,计算机视觉&lt;/h3&gt;&lt;p&gt;由简到难分为三个层次：处理、识别检测和分析理解。&lt;br&gt;图像处理层面主要是对图像的像素的处理；&lt;br
    
    </summary>
    
      <category term="归纳总结" scheme="http://ynuwm.github.io/categories/%E5%BD%92%E7%BA%B3%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="AI" scheme="http://ynuwm.github.io/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>关系抽取研究现状</title>
    <link href="http://ynuwm.github.io/2018/05/26/%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%E7%A0%94%E7%A9%B6%E7%8E%B0%E7%8A%B6/"/>
    <id>http://ynuwm.github.io/2018/05/26/关系抽取研究现状/</id>
    <published>2018-05-26T13:28:41.000Z</published>
    <updated>2018-05-31T13:34:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>关系抽取就是从文本中识别实体并抽取实体之间的语义关系。</p>
<h3 id="关系抽取的方法有以下三类："><a href="#关系抽取的方法有以下三类：" class="headerlink" title="关系抽取的方法有以下三类："></a>关系抽取的方法有以下三类：</h3><p>1,有监督的学习方法：根据已有数据训练分类器，用训练好的分类器预测关系。有监督的学习方法缺点是需要大量人工标注的语料。<br>2,半监督的学习方法：主要是用Bootstrapping进行关系抽取。对要抽取的关系首先手工设定若干种子实例，然后迭代地从数据中抽取关系对应的关系模板和更多的实例。<br>3,无监督的学习方法：假设拥有相同语义关系的实体对拥有相似的上下文信息。故可以利用每个实体对对应上下文信息来代表该实体对的语义关系，并对所有实体对的语义关系进行聚类。</p>
<h3 id="有监督学习与远程监督"><a href="#有监督学习与远程监督" class="headerlink" title="有监督学习与远程监督"></a>有监督学习与远程监督</h3><p>有监督学习的不足是需要大量人工标注的训练数据，但是人工标注训练数据需要花费大量时间和精力。基于此，Mintz在2009年提出了远程监督（Distant Supervision）的思想。这篇论文将纽约时报新闻文本和大规模知识图谱Freebase(包含7300多个关系和超过9亿的实体)进行实体对齐。远程监督假设，一个同时<br>包含两个实体的句子蕴含了该实体对在 Freebase 中的关系，并将该句子作为该实<br>体对所对应关系的训练正例。论文在远程监督标注的数据上提取文本特征并训练<br>关系分类模型，有效解决了关系抽取的标注数据规模问题。 </p>
<p>远程监督的思想假设的是一个实体只对应一种关系。但是，很多实体之间具有多种关系。因此Hoffmann在2011年提出了采用多实例多标签方法来对关系抽取进行建模，刻画一个实体对可能存在多种关系的情况。</p>
<h3 id="基于深度学习的关系抽取"><a href="#基于深度学习的关系抽取" class="headerlink" title="基于深度学习的关系抽取"></a>基于深度学习的关系抽取</h3><p>Socher在2012年开始使用递归神经网络来解决关系抽取问题，这篇论文首先对句子进行句法解析，然后为句法树上的每个节点学习向量表示。通过递归神经网络，可以从句法树最低端的词向量开始，按照句子的句法结构迭代合并，最终得到该句子的向量表示，并用于关系分类。该方法能够有效考虑句子的句法结构信息，但是无法考虑两个实体在句子中的位置和语义信息。</p>
<p>Zeng在2014年提出用卷积神经网络进行关系抽取。论文此阿勇词汇向量和词的位置向量作为卷积神经网络的输入，通过卷积层、池化层和非线性层得到句子表示。通过考虑实体的位置向量和其他相关的词汇特征，句子中的实体信息能够被较好地考虑到关系抽取中。</p>
<p>Miwa在2016年提出一种端到端神经网络的关系抽取模型。该模型采用BiLSTM和树形LSTM同时对句子和实体进行建模。</p>
<p>与之前基于特征的关系抽取系统类似，神经网络关系抽取模型也面临着人工标注数据较少的问题。对此，Zeng在2015年尝试将基于卷积神经网络的关系抽取模型扩展到远程监督数据上。Zeng假设每个实体对的所有句子中至少存在一个句子反映该实体对的关系，提出了一种新的学习框架：以实体对为单位，对于每个实体对只考虑最能反映其关系的那个句子。该方法在一定程度上解决了神经网络关系抽取模型在远程监督数据上的应用，在NYT10 数据集上取得了远<br>远高于基于特征的关系抽取模型的预测效果。但是，该方法仍然存在一定的缺陷：该模型对于每个实体对只能选用一个句子进行学习和预测， 损失了来自其他大量的有效句子的信息。</p>
<p>有没有可能把实体对对应的有噪音的句子过滤掉，然后利用所有有效句子进行学习和预测呢？ Lin在2016年提出了一种基于句子级别注意力机制的神经网络模型来解决这个问题，该方法能够根据特定关系为实体对的每个句子分配权重，通过不断学习能够使有效句子获得较高的权重， 而有噪音的句子获得较小的权重。 </p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>1,基于句法树的树形LSTM神经网络模型在关系抽取上取得了不错的效果，说明句法信息对关系抽取有一定的帮助，但是目前句法分析仍然存在很多的错误。如果对一个句子考虑多种句法分析树，把这些句法分析树的结果进行整合，准确率也会得到很大的提升。<br>2,目前神经网络的方法主要基于预设的关系集合，而如何面对开放领域进行关系提取，也有很大的限制。所以如何将神经网络引入开放领域的关系提取，自动发现新的关系及事实是一个值得探索的问题。<br>3,目前的关系提取主要基于单语言文本。但是文本这是信息表示的方式之一，如何利用多语言文本、图像和音频信息进行关系提取也是值得思考的问题。</p>
<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p>Mintz, Mike, Steven Bills, Rion Snow, and Dan Jurafsky. “Distant supervision<br>for relation extraction without labeled data.” In Proceedings of ACL-IJCNLP, 2009.</p>
<p>Hoffmann, Raphael, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S.<br>Weld. “Knowledge-based weak supervision for information extraction of<br>overlapping relations.” In Proceedings of ACL-HLT, 2011.</p>
<p>Socher, Richard, et al. “Semantic compositionality through recursive<br>matrix-vector spaces.” Proceedings of EMNLP-CoNLL, 2012</p>
<p>Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, and Jun Zhao. Relation classification via convolutional deep neural network. In Proceedings of COLING.</p>
<p>Makoto Miwa, Mohit Bansa. “End-to-End Relation Extraction using LSTMs on<br>Sequences and Tree Structures” In Proceedings of ACL, 2016.</p>
<p>Daojian Zeng,Kang Liu,Yubo Chen,and Jun Zhao. Distant supervision for relation extraction via piecewise convolutional neural networks. In Proceedings of EMNLP.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;关系抽取就是从文本中识别实体并抽取实体之间的语义关系。&lt;/p&gt;
&lt;h3 id=&quot;关系抽取的方法有以下三类：&quot;&gt;&lt;a href=&quot;#关系抽取的方法有以下三类：&quot; class=&quot;headerlink&quot; title=&quot;关系抽取的方法有以下三类：&quot;&gt;&lt;/a&gt;关系抽取的方法有以下三类
    
    </summary>
    
      <category term="归纳总结" scheme="http://ynuwm.github.io/categories/%E5%BD%92%E7%BA%B3%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="机器学习" scheme="http://ynuwm.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>1219组会小结</title>
    <link href="http://ynuwm.github.io/2017/12/20/1219%E7%BB%84%E4%BC%9A%E5%B0%8F%E7%BB%93-1/"/>
    <id>http://ynuwm.github.io/2017/12/20/1219组会小结-1/</id>
    <published>2017-12-20T12:40:25.000Z</published>
    <updated>2017-12-21T13:15:13.000Z</updated>
    
    <content type="html"><![CDATA[<p>一, CV交叉验证的方差用处不大;<br>二, 句子padding 0的时候mask参数的选择,LSTM和CNN不同;<br>三, 训练不要划分验证集，全部数据都拿来训练，dev-set来验证；<br>四, 分词器(tokenizer)选择:<br>(1)nltk三个<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> TweetTokenizer</div><div class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> StanfordTokenizer</div><div class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> word_tokenize</div></pre></td></tr></table></figure></p>
<p>(2)HappyTokenizing,这里<a href="https://github.com/dlatk/happierfuntokenizing" target="_blank" rel="external">https://github.com/dlatk/happierfuntokenizing</a><br>(3)<em>TweetNLP</em> (Owoputi et al., 2013) <a href="http://www.cs.cmu.edu/~ark/TweetNLP/" target="_blank" rel="external">http://www.cs.cmu.edu/~ark/TweetNLP/</a><br>(4)自己手动写，根据数据集特征。<br>五, 分词考虑: url, http, @ 等;<br>六, 词向量考虑:推特-Glove,fasttext基本用来分类,w2v普遍适用,emoji-embedding;<br>六, MultiTask-learning思想:<br>同时对多个任务学习不同的回归函数。<br>七,adam优化器比sgd收敛快？</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;一, CV交叉验证的方差用处不大;&lt;br&gt;二, 句子padding 0的时候mask参数的选择,LSTM和CNN不同;&lt;br&gt;三, 训练不要划分验证集，全部数据都拿来训练，dev-set来验证；&lt;br&gt;四, 分词器(tokenizer)选择:&lt;br&gt;(1)nltk三个&lt;br
    
    </summary>
    
      <category term="日记" scheme="http://ynuwm.github.io/categories/%E6%97%A5%E8%AE%B0/"/>
    
    
      <category term="开会记录" scheme="http://ynuwm.github.io/tags/%E5%BC%80%E4%BC%9A%E8%AE%B0%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>梯度下降优化算法</title>
    <link href="http://ynuwm.github.io/2017/12/09/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    <id>http://ynuwm.github.io/2017/12/09/梯度下降优化算法/</id>
    <published>2017-12-09T01:32:04.000Z</published>
    <updated>2017-12-09T01:37:35.000Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="深度学习" scheme="http://ynuwm.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="SGD" scheme="http://ynuwm.github.io/tags/SGD/"/>
    
  </entry>
  
  <entry>
    <title>sklearn 特征提取</title>
    <link href="http://ynuwm.github.io/2017/12/09/sklearn-%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/"/>
    <id>http://ynuwm.github.io/2017/12/09/sklearn-特征提取/</id>
    <published>2017-12-09T01:28:27.000Z</published>
    <updated>2017-12-09T03:28:35.000Z</updated>
    
    <content type="html"><![CDATA[<p>官网: <a href="http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection" rel="external" color="blue" target="_blank">Feature selection</a></p>
<h2 id="移除方差较小的特征"><a href="#移除方差较小的特征" class="headerlink" title="移除方差较小的特征"></a>移除方差较小的特征</h2><p>方差阈值（VarianceThreshold）是特征选择的一个简单方法，去掉那些方差没有达到阈值的特征。默认情况下，删除零方差的特征，例如那些只有一个值的样本<br>假设我们有一个有布尔特征的数据集，然后我们想去掉那些超过80%的样本都是0（或者1）的特征。布尔特征是伯努利随机变量，方差为 p(1-p)。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> VarianceThreshold</div><div class="line">X = [[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]]</div><div class="line">sel = VarianceThreshold(threshold=(<span class="number">.8</span> * (<span class="number">1</span> - <span class="number">.8</span>)))</div><div class="line">sel.fit_transform(X)</div><div class="line"></div><div class="line">Output:</div><div class="line">array([[<span class="number">0</span>, <span class="number">1</span>],</div><div class="line">       [<span class="number">1</span>, <span class="number">0</span>],</div><div class="line">       [<span class="number">0</span>, <span class="number">0</span>],</div><div class="line">       [<span class="number">1</span>, <span class="number">1</span>],</div><div class="line">       [<span class="number">1</span>, <span class="number">0</span>],</div><div class="line">       [<span class="number">1</span>, <span class="number">1</span>]])</div></pre></td></tr></table></figure></p>
<p>VarianceThreshold去掉了第一列，第一列里面0的比例为5/6。</p>
<h2 id="单变量特征选择"><a href="#单变量特征选择" class="headerlink" title="单变量特征选择"></a>单变量特征选择</h2><p>单变量特征选择通过单变量统计检验选择特征，可以看作一个估计器的预处理步骤。Sklearn将特征选择视为日常的转换操作：</p>
<ul>
<li>SelectBest 只保留 k 个最高分的特征；</li>
<li>SelectPercentile 只保留用户指定百分比的最高得分的特征；</li>
<li>使用常见的单变量统计检验：假正率SelectFpr，错误发现率selectFdr，或者总体错误率SelectFwe；</li>
<li>GenericUnivariateSelect 通过结构化策略进行特征选择，通过超参数搜索估计器进行特征选择。<br>举个例子，使用卡方检验选择两个最优特征：</li>
</ul>
<p>SelectKBest和SelectPerecntile能够返回特征评价的得分和P值：<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sklearn.feature_selection.SelectPercentile(score_func=&lt;function f_classif&gt;, percentile=10)</div><div class="line">sklearn.feature_selection.SelectKBest(score_func=&lt;function f_classif&gt;, k=10</div></pre></td></tr></table></figure></p>
<p>其中的参数 score_func 有以下选项：<br>(1)<strong>回归</strong><br>f_regression：相关系数，计算每个变量与目标变量的相关系数，然后计算出F值和P值；<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">degrees_of_freedom = y.size - (<span class="number">2</span> <span class="keyword">if</span> center <span class="keyword">else</span> <span class="number">1</span>)</div><div class="line">F = corr ** <span class="number">2</span> / (<span class="number">1</span> - corr ** <span class="number">2</span>) * degrees_of_freedom</div><div class="line">pv = stats.f.sf(F, <span class="number">1</span>, degrees_of_freedom)</div></pre></td></tr></table></figure></p>
<p><strong>mutual_info_regression</strong>：互信息，互信息度量 X 和 Y 共享的信息：它度量知道这两个变量其中一个，对另一个不确定度减少的程度。<br>参考：<a href="http://www.cnblogs.com/gatherstars/p/6004075.html" target="_blank" rel="external">http://www.cnblogs.com/gatherstars/p/6004075.html</a></p>
<p>(2)<strong>分类</strong><br> <strong>chi2</strong>：卡方检验；<br> <strong>f_classif</strong>：方差分析，计算方差分析（ANOVA）的F值 (组间均方 / 组内均方)；<br> <strong>mutual_info_classif</strong>：互信息，互信息方法可以捕捉任何一种统计依赖，但是作为非参数方法，需要更多的样本进行准确的估计。</p>
<h3 id="卡方-Chi2-检验"><a href="#卡方-Chi2-检验" class="headerlink" title="卡方(Chi2)检验"></a>卡方(Chi2)检验</h3><p>经典的卡方检验是检验定性自变量对定性因变量的相关性。比如，我们可以对样本进行一次chi2chi2 测试来选择最佳的两项特征：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> chi2</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>iris = load_iris()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>X, y = iris.data, iris.target</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>X.shape</div><div class="line">(<span class="number">150</span>, <span class="number">4</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>X_new = SelectKBest(chi2, k=<span class="number">2</span>).fit_transform(X, y)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>X_new.shape</div><div class="line">(<span class="number">150</span>, <span class="number">2</span>)</div></pre></td></tr></table></figure></p>
<h3 id="Pearson相关系数-Pearson-Correlation"><a href="#Pearson相关系数-Pearson-Correlation" class="headerlink" title="Pearson相关系数 (Pearson Correlation)"></a>Pearson相关系数 (Pearson Correlation)</h3><p>皮尔森相关系数是一种最简单的，能帮助理解特征和响应变量之间关系的方法，该方法衡量的是变量之间的线性相关性，结果的取值区间为[-1，1]，-1表示完全的负相关，+1表示完全的正相关，0表示没有线性相关。Pearson Correlation速度快、易于计算，经常在拿到数据(经过清洗和特征提取之后的)之后第一时间就执行。Scipy的 pearsonr 方法能够同时计算 相关系数和p-value</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> pearsonr</div><div class="line">np.random.seed(<span class="number">0</span>)</div><div class="line">size = <span class="number">300</span></div><div class="line">x = np.random.normal(<span class="number">0</span>, <span class="number">1</span>, size)</div><div class="line"><span class="comment"># pearsonr(x, y)的输入为特征矩阵和目标向量</span></div><div class="line">print(<span class="string">"Lower noise"</span>, pearsonr(x, x + np.random.normal(<span class="number">0</span>, <span class="number">1</span>, size)))</div><div class="line">print(<span class="string">"Higher noise"</span>, pearsonr(x, x + np.random.normal(<span class="number">0</span>, <span class="number">10</span>, size)))</div><div class="line">&gt;&gt;&gt;</div><div class="line"><span class="comment"># 输出为二元组(sorce, p-value)的数组</span></div><div class="line">Lower noise (<span class="number">0.71824836862138386</span>, <span class="number">7.3240173129992273e-49</span>)</div><div class="line">Higher noise (<span class="number">0.057964292079338148</span>, <span class="number">0.31700993885324746</span>)</div></pre></td></tr></table></figure>
<p>这个例子中，我们比较了变量在加入噪音之前和之后的差异。当噪音比较小的时候，相关性很强，p-value很低。Scikit-learn提供的 f_regrssion 方法能够批量计算特征的f_score和p-value，非常方便，参考sklearn的<a href="http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection" rel="external" color="blue" target="_blank">pipeline</a></p>
<p>Pearson相关系数的一个明显缺陷是，作为特征排序机制，他只对线性关系敏感。如果关系是非线性的，即便两个变量具有一一对应的关系，Pearson相关性也可能会接近0。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">x = np.random.uniform(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">100000</span>)</div><div class="line"><span class="keyword">print</span> pearsonr(x, x**<span class="number">2</span>)[<span class="number">0</span>]</div><div class="line"><span class="number">-0.00230804707612</span></div></pre></td></tr></table></figure>
<h3 id="互信息和最大信息系数"><a href="#互信息和最大信息系数" class="headerlink" title="互信息和最大信息系数"></a>互信息和最大信息系数</h3><h2 id="递归特征淘汰（RFE）"><a href="#递归特征淘汰（RFE）" class="headerlink" title="递归特征淘汰（RFE）"></a>递归特征淘汰（RFE）</h2><p>给特征赋予一个外部模型产生的权重（例如：线性模型系数），RFE递归地使用越来越少的特征来进行特征选择。首先，在原始数据上建立模型并且给每个特征一个 权重；然后，淘汰绝对权重最小的特征，递归地执行这个过程直到达到希望的特征数。RFECV使用交叉验证方法发现最优特征数量。</p>
<h2 id="使用SelectFromModel方法特征选择"><a href="#使用SelectFromModel方法特征选择" class="headerlink" title="使用SelectFromModel方法特征选择"></a>使用SelectFromModel方法特征选择</h2><p> SelectFromModel是一种元转换器，可以与那些有coef_ 或者feature<em>importances</em>属性的模型一起使用。如果coef_ 或者feature<em>importances</em>小于阈值，我们就认为特征是不重要的。除了指定阈值以外，也可以使用启发式的方式。有效的启发式方法包括均值、中位数或者乘以系数，比如 0.1*均值。</p>
<h2 id="基于L1范数的特征选择使用"><a href="#基于L1范数的特征选择使用" class="headerlink" title="基于L1范数的特征选择使用"></a>基于L1范数的特征选择使用</h2><p>## </p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;官网: &lt;a href=&quot;http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection&quot; rel=&quot;external&quot; color=&quot;blue&quot; targ
    
    </summary>
    
      <category term="Python" scheme="http://ynuwm.github.io/categories/Python/"/>
    
    
      <category term="sklearn" scheme="http://ynuwm.github.io/tags/sklearn/"/>
    
  </entry>
  
  <entry>
    <title>考研英语相关</title>
    <link href="http://ynuwm.github.io/2017/12/05/%E8%80%83%E7%A0%94%E8%8B%B1%E8%AF%AD%E7%9B%B8%E5%85%B3/"/>
    <id>http://ynuwm.github.io/2017/12/05/考研英语相关/</id>
    <published>2017-12-05T01:26:01.000Z</published>
    <updated>2017-12-08T06:49:16.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="美国政治常识"><a href="#美国政治常识" class="headerlink" title="美国政治常识"></a>美国政治常识</h2><p><img src="/2017/12/05/考研英语相关/IMG_1701.JPG" alt="img"><br><img src="/2017/12/05/考研英语相关/IMG_1702.JPG" alt="img"><br><img src="/2017/12/05/考研英语相关/IMG_1703.JPG" alt="img"><br><img src="/2017/12/05/考研英语相关/IMG_1704.JPG" alt="img"><br><img src="/2017/12/05/考研英语相关/IMG_1705.JPG" alt="img"><br><img src="/2017/12/05/考研英语相关/IMG_1706.JPG" alt="img"><br><img src="/2017/12/05/考研英语相关/IMG_1707.JPG" alt="img"></p>
<h2 id="超级替换"><a href="#超级替换" class="headerlink" title="超级替换"></a>超级替换</h2><p>weigh（看重）=pay muchcloser attention（密切关注）；<br>reluctant（不情愿）=not to beco-operative（不合作）；<br>unanswered question（未知答案）=uncertain（不确定）；<br>should stay out of the way（别干涉）=<br>have the freedom to  choose their own way（自己有权选择）；<br>legislative initiative（立法举动）=legislative measure （立法措施）；<br>parallel（相似物）=is alsoapplicable（同样适用）；<br>change（改变）=modify（修改）；<br>have… dream（做…梦）=show up in dreams（在梦中展现）；<br>has its way（自有其道）=as usual（顺其自然）。<br>homogenize（v.同化）=assimilate（v.同化）<br>play a role（对…起到作用）=launch（v.发起…某物）<br>threat（v.威胁，危害）=poisonous（a.有毒的，有害的）<br>many fans（很多粉丝）=influential （a.有影响力的）<br>not dark and deteriorating（不黑暗、不颓败）=successful（成功的）<br>doubt（v.怀疑，不相信）=deny（v.拒绝承认）<br>add a penny（增加收入）=contribute（v.做贡献）<br>bring in revenue（带来收益）=spendmore money（花了更多钱）<br>cry poor（哭穷）=not really shortof money（并不真穷）<br>break attendance record（破上座记录）=attendanceis on the rise（人数一直升）<br>same（相同的）=similar（相似的）<br>sea（海洋）=ocean（海洋）<br>stock（存储量）=biomass（生物数量）<br>not conservative（不保守的）=greater（更大的）<br>bummer（痛苦、讨厌）=unpleasant（不高兴的）<br>distasteful（令人反感的）=bitter（苦的）<br>a breath of fresh（清新的空气）=refresh（v.使恢复活力）<br>elite精英=who perform well做得好的人<br>mania疯狂= craze疯狂<br>cognitive exercise认知训练=meaningful processing of information有意识地信息处理<br>be made被制造= owe to nurture基于培养<br>be born被生育= owe to nature基于自然<br>Practice makes perfect熟能生巧= talentis highly overrated天才被高估<br>similar to相似于=analogy类比<br>adult and children’s version成人和儿童版本=formats for adults and children成人和儿童形式<br>are obtained through获得于= is nowbased on基于<br>not be reliable不可靠= may notassess评估不了<br>vulnerable to易受伤害于= lost theparachute丢掉了安全伞<br>less secured payments=depending oninvestment return<br>large new dose of investment risk新的数量的投资风险=<br>increase the families’ investment risk增加投资风险<br>political fallout may not be far政治波及影响不远了=<br>may bring about political problems带来政治问题<br>on the Cliff岌岌可危= reduce to poor重变穷狗<br>data insecurity数据不安全= data leakage数据泄露<br>weak point弱点=potential vulnerabilities潜在的易受伤性<br>go astray丢失了=leakage泄露<br>regulators will act管理者要行动= legalpenalty法律惩罚<br>come as a surprise我的天哪！= failto see不能理解<br>The mystery is我的天呐！= whatpuzzles百思不得其解..<br>vulnerable to易受伤害的= susceptible to易受影响的<br>increased “opportunities” for有更多的…的机会=expose tomore 暴露于更多…之中<br>durable更耐久的=longer更久的<br>frequent频繁的=not one-shot deal不是一锤子买卖<br>from paycheck to paycheck从一个工资单到另一个工资单=<br>barely covered one’s expense几乎支付不起花费<br>depression压力=stress压力<br>traditional process传统过程= It usedto be过去常常<br>upset sb.使不安= make heavy reading for sb.使阅读心情不轻松<br>making access to给创造入口=provides an easier access to提供更容易的入口<br>pay for支付= cover the cost支付<br>change the traditional改变传统= newmode of新模式<br>change变化=alteration变化<br>plays a key role起关键作用=main reason主要原因<br>Living standard生活标准= dietand health饮食和健康<br>condition从条件上限制= cannot easily withstand不能轻松应对<br>reached their limit达到极限=generally stopped growing基本停止增长<br>early days早期时光=infancy婴儿期，初期<br>fragile易碎的=delicate脆弱的<br>complex复杂的= distaste also understood it is bedrock不喜欢但是也知道其重要<br>politically lives depended on政治上依存于=benefit politically from政治上获益于<br>military experience军事经历=<br>observing the bravery of the black soldiers观察到了黑人士兵的勇敢<br>vulnerable to易受伤害的= susceptible to易受影响的<br>increased “opportunities” for有更多的…的机会=exposeto more 暴露于更多…之中<br>durable更耐久的=longer更久的<br>frequent频繁的=not one-shot deal不是一锤子买卖<br>from paycheck to paycheck从一个工资单到另一个工资单=<br>barely covered one’s expense几乎支付不起花费<br>depression压力=stress压力<br>traditional process传统过程= It usedto be过去常常<br>upset sb.使不安= make heavyreading for sb.使阅读心情不轻松<br>making access to给创造入口=provides an easier access to提供更容易的入口<br>pay for支付= cover the cost支付<br>change the traditional改变传统= newmode of新模式<br>change变化=alteration变化<br>plays a key role起关键作用=mainreason主要原因<br>Living standard生活标准= dietand health饮食和健康<br>condition从条件上限制= cannot easily withstand不能轻松应对<br>reached their limit达到极限=generally stopped growing基本停止增长<br>early days早期时光=infancy婴儿期，初期<br>fragile易碎的=delicate脆弱的<br>complex复杂的= distaste also understood it is bedrock不喜欢但是也知道其重要<br>politically lives depended on政治上依存于=benefit politically from政治上获益于<br>military experience军事经历=<br>observing the bravery of the black soldiers观察到了黑人士兵的勇敢<br>no good=undesirable不好的<br>learn from= take a page from学习<br>enough explore= adequately probe充分地挖掘<br>occurs without realizing= unconsciously无意识发生<br>Far less certain=questionable可疑的<br>reneging on= dishonoring食言<br>seek favor= seek permission寻求支持<br>purchase=buy购买<br>management= managerial practices管理实践<br>the limits= how far extend某物的极限<br>should be consequence=be affected被影响<br>uncertainty=be ambiguous模糊<br>complexity=be complicated复杂<br>enlarge=thrive壮大<br>prudent=cautious谨慎<br>rally against=be barrier障碍<br>acclaim=favorable response欢呼喝彩<br>modest=unpretentious谦虚的<br>overestimate=exaggerate夸大<br>accessible=available可用到的<br>lookingfor=pursue追求<br>poach=hunt for 猎<br>invert=get out-dated过时了<br>impinge on=do harm to伤害<br>Charter=standard宪章；标准<br>be rejected= deny access for拒绝<br>public sentiment=poll民意测验<br>notwell reflected in politics = has limited political support没有政治支持<br>consumes=destroy消耗<br>not respect=disregard不考虑<br>not be absolutely= skepticism可疑<br>lenient=less severe轻微的<br>impact=influence影响<br>amount=how much总量<br>oughtto= necessary需要去<br>expensive=high cost贵<br>change=adjustment改变<br>Good=commodity产品</p>
<p>英语二——<br>involved in auctions参与拍卖 = spending money花钱<br>down by nearly 90%下降九成= fall dramatically剧烈下降<br>waiting for confidence期待信心= awaiting better chances等待机会<br>deliver works of art to the market发布作品去艺术品市场<br>=promoting art work circulation促进艺术品发行<br>bull run ended牛市结束=decline衰落<br>complain sth. didn’t抱怨没有做到某事=have the expectation期待<br>wreaking havoc制造大破坏= causing damage制造损害<br>communication交流= conversation对话<br>couple配偶=spouse配偶<br>attach much importance to重视=focus on聚焦于<br>create new behavior制造新的行为=cultivate the habit培养习惯<br>impact影响=influence影响<br>manufacture制造=create制造<br>commercial promotion商业促销= relentless advertising无尽的广告<br>inadequacy不足=have way around可以避开<br>domestic家里=home家里<br>required to do被要求去=ought to do应该去<br>enormous bonus巨额奖金=excessive profit更多的利润<br>have enough independence有充分自主权= independent独立的<br>do less well做地不好= perform worse表现更差<br>attractive offers开出更有吸引力的条件=create incentive创造刺激<br>in a desperate situation处于绝望处境<br>= chronicling their owndoom数着自己的末日时间表<br>reduce cost减少成本=save money省钱<br>dependent onadvertising依靠广告=reliance on ads依赖广告<br>essential本质的，必须的=important重要的<br>had a great influence upon有很大影响= exert enormousinfluence施加很多影响<br>reliant on abundant decoration依赖大量的装饰<br>= derive from abundance来源于装饰<br>shared some characteristics有共同的特征=<br>equivalent of…就是等同于<br>Natural scenes自然景观= landscape风景<br>cheer leaders拉拉队长= supporters支持者<br>fail to reach anagreement没有达到同意= but disagreeabout不同意于<br>something is hopeful有希望的= too soon to write off说抹掉..太操之过急</p>
<h2 id="1575-650"><a href="#1575-650" class="headerlink" title="1575-650"></a>1575-650</h2><p>​​abide [əˈbaɪd]v.遵守<br>capacity[kəˈpæsəti]n.容量;能力;接受力<br>norm[nɔ:m] n.准则,规范,准则<br>normal[ˈnɔ:ml] a.普通的;正规的,标准的<br>establish[ɪˈstæblɪʃ]v.建立;安置,使定居<br>establishment[ɪˈstæblɪʃmənt]n.建立,设立,建立的机构<br>stability[stəˈbɪləti] n.稳定,安定<br>abound [əˈbaʊnd]a.丰富，大量存在<br>abundant [əˈbʌndənt]a.丰富的，充裕的<br>abroad [əˈbrɔ:d]adv.宽广；在国外<br>abrupt [əˈbrʌpt]a.唐突的<br>disrupt[dɪsˈrʌpt]vt.使混乱，使崩溃，使分裂<br>absent [ˈæbsənt]a.缺席的<br>absence [ˈæbsəns]n.缺席<br>extract[ˈekstrækt]v./n.拔出;摘录 n.抽取物<br>attract[əˈtrækt]v.吸引<br>attractive[əˈtræktɪv]a.有吸引力的<br>academic [ˌækəˈdemɪk]a.学院的；学术的<br>academy [əˈkædəmi]n.学院<br>excel[ɪkˈsel]vi.擅长 vt.胜过<br>acceptance [əkˈseptəns]n.接受<br>susceptible[səˈseptəbl] a.易受影响的;易受感动的;易受感染的<br>access [ˈækses]n.入口；享用权 v.接近<br>excessive[ɪkˈsesɪv] a.过多的;过分的;额外<br>predecessor[ˈpri:dɪsesə(r)] n.前辈，前任<br>process[ˈprəʊses] n.过程v.加工,处理<br>succession[səkˈseʃn] n.连续,系列;继任<br>proceed[prəˈsi:d] v.进行,继续下去;发生<br>succeed[səkˈsi:d] vi.成功vt.接替<br>precede[prɪˈsi:d] v.领先(于),在(…之前);优先<br>accident [ˈæksɪdənt]n.事故<br>acclaim [əˈkleɪm]v.欢呼，喝彩<br>claim[kleɪm]v.要求n.要求;断言<br>accompany [əˈkʌmpəni]v.陪伴<br>accomplish [əˈkʌmplɪʃ]v.完成，达到目的<br>accord [əˈkɔ:d]v.一致，符合<br>according to [əˈkɔ:dɪŋ tə]按照<br>account [əˈkaʊnt]n.账目 v.报账；解释<br>accumulate [əˈkju:mjəleɪt]v.积累<br>curious[ˈkjʊəriəs]a.好奇的, 求知的, 古怪的<br>secure[sɪˈkjʊə(r)] a.(from,against)安全的v.得到<br>security[sɪˈkjʊərəti] n.安全(感),防御(物),保证(人)<br>accuse [əˈkju:z]v.谴责<br>accustomed [əˈkʌstəmd]a.习惯的<br>used[ju:st] a.用旧了的，习惯于…；过去惯/经常<br>achieve [əˈtʃi:v]v.成就，成功<br>acknowledge [əkˈnɒlɪdʒ]v.承认<br>acquire [əˈkwaɪə(r)]v.获得，学到<br>acquaintance [əˈkweɪntəns]n.熟人，熟事<br>acquisition [ˌækwɪˈzɪʃn]n.获得；获得物<br>active [ˈæktɪv]a.积极的，活跃的<br>transaction[trænˈzækʃn] n.办理,处理;交易<br>agenda[əˈdʒendə]n.议事日程，待办事项<br>adapt [əˈdæpt]v.使适应，改编<br>property[ˈprɒpəti] n.财产;性质,特性<br>address [əˈdres]n.地址，演讲 v.处理，解决<br>adequate [ˈædɪkwət]a.足够的，相当的<br>equipment[ɪˈkwɪpmənt]n.设备,装置;才能<br>equivalent[ɪˈkwɪvələnta.(to)相等的n.相等物<br>adhere [ədˈhɪə(r)]v.黏贴；坚持<br>coherent[kəʊˈhɪərənt]a.一致的，协调的；(话语等)条理清楚的<br>conjunction[kənˈdʒʌŋkʃn]n.接合，连接；连(接)词<br>subject[ˈsʌbdʒɪkt] n.主题a.隶属的<br>objective[əbˈdʒektɪv] n.目标a.客观的<br>adjust [əˈdʒʌst]v.调整，使适应，校正<br>administration[ədˌmɪnɪˈstreɪʃn]n.管理<br>admission[ədˈmɪʃn]n.承认<br>adopt[əˈdɒpt]v.采纳，收养<br>advance[ədˈvɑ:ns]v.前进<br>advantage[ədˈvɑ:ntɪdʒ]n.优势<br>anticipate[ænˈtɪsɪpeɪt]v.预见，期望<br>advent[ˈædvent]n.到来，来临<br>convention[kənˈvenʃn] n.大会;惯例;公约<br>conversely[ˈkɒnvɜ:sli]ad.相反地<br>reverse[rɪˈvɜ:s] n.相反a.相反的<br>version[ˈvɜ:ʃn] n.版本;译本;说法<br>affect[əˈfekt]v.影响<br>affirm[əˈfɜ:m]v.断言，确认<br>confirm[kənˈfɜ:m]v.使更坚固;(进一步)证实;确认<br>fluctuate[ˈflʌktʃueɪt v.(使)波动;(使)起伏<br>influential[ˌɪnfluˈenʃl] a.有影响的;有权势的<br>afford[əˈfɔ:d]v.负担得起<br>aggravate[ˈægrəveɪt]v.加重<br>alleviate[əˈli:vieɪt]v.减轻<br>congress[ˈkɒŋgres]n.(代表)大会;(美国等国的)国会,议会<br>airline[ˈeəlaɪn]n.航线，航空公司<br>alert[əˈlɜ:t]a.警惕的<br>alien[ˈeɪliən]a.外国的，相异的<br>allege[əˈledʒ]v.宣称，断言<br>linguistic[lɪŋˈgwɪstɪk]a.语言的，语言学的<br>alliance[əˈlaɪəns]n.结盟，同盟<br>allowance[əˈlaʊəns]n.津贴，补贴；默许<br>alter[ˈɔ:ltə(r)]v.变更<br>amiable[ˈeɪmiəbl]a.和蔼的<br>amaze[əˈmeɪz]v.使大吃一惊<br>ambiguous[æmˈbɪgjuəs]a.模棱两可的，含糊的<br>vague[veɪg] a.不明确的,含糊的,暧昧的<br>extravagant[ɪkˈstrævəgənt]a.奢侈的;过分的;(言行等)放肆的<br>amend[əˈmend]v.修改<br>amuse[əˈmju:z]v.使娱乐，使消遣<br>analyze[‘ænəlaɪz]v.分解；分析<br>synthetic[sɪnˈθetɪk] a.合成的,人造的;综合的<br>apart[əˈpɑ:t]a.分开的<br>department[dɪˈpɑ:tmənt]n.部门;系<br>departure[dɪˈpɑ:tʃə(r)]n.离开,起程<br>partial[ˈpɑ:ʃl] a.部分的;偏袒的,偏爱的<br>participate[pɑ:ˈtɪsɪpeɪt] v.(in)参与;分享;含有<br>appeal[əˈpi:l]v.恳求，上诉；吸引<br>plead[pli:d] v.恳求;为…辩护;提出…为理由<br>competitive [kəmˈpetitiv]a.竞争的；好竞争的；(价格等的)有竞争力的<br>application[ˌæplɪˈkeɪʃn]n.申请；应用<br>appraisal[əˈpreɪzl]n.评价，估量<br>appreciate[əˈpri:ʃieɪt]v.欣赏，鉴赏；感激<br>precious[ˈpreʃəs] a.珍贵的,贵重的<br>approach[əˈprəʊtʃ]v.接近 n.途径，方法<br>routine[ru:ˈti:n] n.常规 a.常规的<br>approve[əˈpru:v]v.批准<br>argue[ˈɑ:gju:]v.争论；主张<br>arouse[əˈraʊz]v.唤起<br>arrest[əˈrest]v.逮捕，拘捕<br>arrogant[ˈærəgənt]a.傲慢的，自大的<br>articulate[ɑ:ˈtɪkjuleɪt]a.有关节的；发音清晰的<br>artistic[ɑ:ˈtɪstɪk]a.艺术的<br>descend[dɪˈsend]v.下来,下降;遗传(指财产,气质,权利)<br>transcend[trænˈsend] vt.超出,超越(经验、知识、能力的范围)<br>insult[ɪnˈsʌlt] vt./n.侮辱,凌辱<br>assert[əˈsɜ:t]v.断言，声称<br>assess[əˈses]v.评估，评价<br>asset[ˈæset]n.资产<br>similar[ˈsɪmələ(r)] a.(to)相似的,类似的<br>eliminate[ɪˈlɪmɪneɪt]vt.除去;淘汰;排(删,消)除<br>assist[əˈsɪst]v.协助<br>resistant[rɪˈzɪstənt] a.(to)抵抗的,有抵抗力的<br>associate[əˈsəʊʃieɪt]v.使发生联系<br>assure[əˈʃʊə(r)]v.保证<br>insure[ɪnˈʃʊə(r)] vt.保险,给…保险;保证<br>detach[dɪˈtætʃ]vt.分开, 分遣, 派遣(军队)<br>contain[kənˈteɪn]v.包含；容忍；可被…除尽<br>content[‘kɒntent]n.容量a.(with)满足的<br>entertain[ˌentəˈteɪn]n.事业,企(事)业单位;事业心<br>maintain[meɪnˈteɪn] v.维修,保养,维持<br>obtain[əbˈteɪn] v.获得,得到<br>retain[rɪˈteɪn] v.保持,保留<br>sustain[səˈsteɪn] vt.支撑;维持,经受<br>attempt[əˈtempt]v.试图，尝试<br>tempt[tempt] v.诱惑,引诱;吸引<br>exempt[ɪgˈzempt] a.免除的 v.免除<br>attend[əˈtend]v.出席，参加；注意；<br>contend[kənˈtend]v.竞争,斗争;坚决主张<br>tendency[ˈtendənsi] n.趋势，趋向；倾向<br>trend[trend] n.倾向vi.伸向<br>attitude[ˈætɪtju:d]n.态度<br>attribute[əˈtrɪbju:t]v.归属于 n.属性<br>thrive[θraɪv] v.兴旺,繁荣<br>contribute[kənˈtrɪbju:t]v.(to)贡献,捐助;投稿<br>distribute[dɪˈstrɪbju:t]v.分发;分配; (over)散布<br>augment[ɔ:gˈment]n./v.增大，增强<br>authority[ɔ:ˈθɒrəti]n.权威<br>systematic[ˌsɪstəˈmætɪk] a.(systematical)系统的,有组织的<br>available[əˈveɪləbl]a.可用到的<br>evaluate[ɪˈvæljueɪt]v.估价,评价;求…的值<br>value[ˈvælju:] n.价值v.评价<br>avoid[əˈvɔɪd]v.避免<br>bear[beə(r)]n.熊 v.忍受，支撑<br>benign[bɪˈnaɪn]a.良性的<br>bewilder[bɪˈwɪldə(r)]v.使迷惑，使手足无措<br>bizarre[bɪˈzɑ:(r)]a.奇特的，怪异的<br>flourish[ˈflʌrɪʃ] n./v.繁荣,茂盛,兴旺<br>obscure[əbˈskjʊə(r)] a.暗的,朦胧的;模糊的<br>boost[bu:st]v.推进，促进，提高<br>bother[ˈbɒðə(r)]v.烦扰，打搅<br>brief[bri:f]a.短的<br>browse[braʊz]v.吃嫩枝；浏览<br>budget[ˈbʌdʒɪt]n.预算<br>bureaucracy[bjʊəˈrɒkrəsi]n.官僚主义<br>democracy[dɪˈmɒkrəsi] n.民主,民主制,民主国家<br>democratic[ˌdeməˈkrætɪk] a.民主的<br>demonstrate[ˈdemənstreɪt] v.论证;演示,说明<br>epidemic[ˌepɪˈdemɪk]a.流行性的n.流行病<br>passion[ˈpæʃn] n.热情,激情;激怒<br>passive[ˈpæsɪv] a.被动的,消极的<br>speculate[ˈspekjuleɪt] vi.思索vt.思索<br>campaign[kæmˈpeɪn]n. 战役;运动<br>champion[ˈtʃæmpiən]n.冠军,得胜者;拥护者<br>campus[ˈkæmpəs]n.(大学)校园<br>candidate[ˈkændɪdət]n.候选人,候补者;报考者<br>career[kəˈrɪə(r)] n.(个人的)事业；生涯，职业<br>case[keɪs]n.箱,盒;情况;病例<br>category[ˈkætəgəri]n.种类；范畴，类型<br>cater[ˈkeɪtə(r)]vi.(for/to)满足;(for)提供饮食及服务<br>cause[kɔ:z] n.原因 v.引起<br>cautious[ˈkɔ:ʃəs]a.(of)小心的,谨慎的<br>caution[ˈkɔ:ʃn]n.谨慎vt.劝…小心<br>celebrate[ˈselɪbreɪt] vt.庆祝vi.庆祝<br>celebrity[səˈlebrəti]n.名人；著名，名声<br>challenge[ˈtʃæləndʒ] n.挑战(书) v.向…挑战<br>chaos[ˈkeɪɒs]n.混乱,紊乱<br>characterize[ˈkærəktəraɪz]v.表示…的特性;描述…特性<br>charter[ˈtʃɑ:tə(r)]v.租船,租车n.宪章<br>chemical[ˈkemɪkl]a.化学的 n.(pl.)化学制品<br>circulate[ˈsɜ:kjəleɪt]v.(使)循环,(使)流通<br>classic[ˈklæsɪk]n.(pl.)杰作 a.第一流的<br>climate[ˈklaɪmət]n.气候;风气,社会思潮<br>decline[dɪˈklaɪn]v.下降;拒绝 n.下降;斜面<br>cognitive[ˈkɒgnətɪv]a.认知的，认识能力的<br>collaborate[kəˈlæbəreɪt]vi.协作，合作；(与敌人)勾结<br>corporation[ˌkɔ:pəˈreɪʃn]n.市镇自治机关；法人；公司<br>operate[ˈɒpəreɪt] v.操作,起作用,动手术<br>operational[ˌɒpəˈreɪʃənl] a.操作的,运转的,起作用的<br>collapse[kəˈlæps]v./n.倒塌；崩溃；(价格)暴跌<br>combine[kəmˈbain]v.联合；结合；化合 n.集团；联合企业<br>comment [ˈkɔment] n.注释v.(on)注释<br>mental[ˈmentl] a.精神的,思想的,心理的<br>material[məˈtɪəriəl] n.材料a.物质的<br>commerce [ˈkɔmə:s]n.商业，贸易；交际<br>commercial [kəˈmə:ʃəl]a.商业的n.广告节目<br>commit [kəˈmit]v.把…交托给;犯(错误),干(坏事)<br>transmit[trænsˈmɪt] vt.发射 vi.发射信号<br>commodity [kəˈmɔditi]n.(pl.)日用品；商品；农/矿产品<br>communicate [kəˈmju:nikeit]v.传达;交流;通讯<br>immune[ɪˈmju:n] a.免疫的;有受影响的;豁免的<br>comparable [ˈkɔmpərəbl]a.(with,to)可比较的,比得上的<br>compare [kəmˈpɛə]vt.(to，with)比较；(to)把…比作vi.相比<br>impair[ɪmˈpeə(r)] v.损害,损伤;削弱<br>complicate [ˈkɔmplɪˌkeɪt] v.使..复杂；使..难懂；使（病）恶化<br>duplicate[ˈdju:plɪkeɪt] n.复制品 v.复写<br>explicit[ɪkˈsplɪsɪt]a.详述的,明确的;坦率的<br>implicit[ɪmˈplɪsɪt] a.含蓄的;(in)固有的;无疑问的<br>compromise [ˈkɔmprəmaiz]n.妥协vi.妥协<br>promising[ˈprɒmɪsɪŋ] a.有希望的,有前途的<br>reputation[ˌrepjuˈteɪʃn] n.名誉,名声,声望<br>reveal[rɪˈvi:l] v.展现,显示,揭示<br>revelation[ˌrevəˈleɪʃn] n.揭示,揭露,显示<br>conceive [kənˈsi:v]v.(of)设想;以为;怀胎<br>concept [ˈkɒnsept]n.概念，观念，设想<br>precise[prɪˈsaɪs] a.精确的,准确的<br>condemn [kənˈdem]v.谴责,指责;判刑<br>contempt[kənˈtempt]n.轻视，藐视；受辱<br>conduct[kənˈdʌkt]n.行为v.引导<br>introduce[ˌɪntrəˈdju:s] vt.介绍；引进，传入<br>reproduce[ˌri:prəˈdju:s] v.生殖;翻版;复制<br>offer[ˈɒfə(r)] v.提供n.提议<br>refer[rɪˈfɜ:(r)] v.参考;提到;提交<br>reference[ˈrefrəns] n.提及,涉及;参考书目<br>suffer[ˈsʌfə(r)] v.(from)受痛苦;受损失<br>transfer[trænsˈfɜ:(r)] vt./n.转移；转换；转让<br>profession[prəˈfeʃn] n.职业,专业,表白<br>faith[feɪθ]n.信任;信仰,信条<br>definite[ˈdefɪnət]a.明确的;一定的;意志坚强的<br>conflict[ˈkɒnflɪkt]n.战斗v.(with)抵触<br>format[ˈfɔ:mæt] n.格式 vt.设计<br>perform[pəˈfɔ:m] v.履行,执行;表演<br>performance[pəˈfɔ:məns] n.履行;表演;性能<br>transform[trænsˈfɔ:m] vt.改变,变换;变压;转化;改造,改造<br>confront[kənˈfrʌnt]v.使面临,使遭遇;面对(危险等)<br>gratitude[ˈgrætɪtju:d]n.感激,感谢<br>conscious[ˈkɒnʃəs] a.(of)意识到的,自觉的;神志清醒的<br>consequence[ˈkɒnsɪkwəns] n.结果,后果;重要性<br>sequence[ˈsi:kwəns] n.句子;判决v.宣判<br>subsequent[ˈsʌbsɪkwənt] a.随后的,后来的<br>consensus[kənˈsensəs] n.(意见等的)一致，一致同意，共识<br>sensible[ˈsensəbl] a.明智的;可觉察的,明显的<br>sensitive[ˈsensətɪv] a.(to)敏感的,易受伤害的;灵敏的<br>deserve[dɪˈzɜ:v]v.应受,值得<br>observe[əbˈzɜ:v] v.观察,观测,注意到<br>preserve[prɪˈzɜ:v] v.保护,维持;保存<br>considerable[kənˈsɪdərəbl]a.相当大(或多)的,可观的;值得考虑的<br>prospect[ˈprɒspekt] n.景色;前景,前途<br>retrospect[ˈretrəspekt] v./n.回顾，回想，追溯[反]foresee<br>suspect[səˈspekt] v.猜想a.可疑的<br>conspiracy[kənˈspɪrəsi]n.阴谋，密谋，共谋<br>spirit[ˈspɪrɪt] n.精神;(pl.)情绪;(pl.)酒精<br>constant[ˈkɒnstənt] a.固定的 n.常数<br>constituent[kənˈstɪtjuənt] n.选民a.组成的<br>constitute[ˈkɒnstɪtju:t]vt.组成；设立，建立<br>institution[ˌɪnstɪˈtju:ʃn] n.公共机构;协会;学校<br>substitute[ˈsʌbstɪtju:t] n.代替者v.(for)代替<br>statute[ˈstætʃu:t] n.法令，法规；章程<br>constrain[kənˈstreɪn]vt.限制；克制，抑制<br>restrain[rɪˈstreɪn] v.(from)抑制,制止<br>consume[kənˈsju:m]vt.消耗(with)使着迷<br>contact[ˈkɒntækt]v./n.(使)接触,联系,交往<br>integrity[ɪnˈtegrəti] n.正直,诚实;完整,完全<br>temporary[ˈtemprəri] a.暂时的,临时的<br>contradict[ˌkɒntrəˈdɪkt]v.反驳;同…矛盾,同…抵触<br>dictate[dɪkˈteɪt]v.口授;(使)听写;指令<br>predict[prɪˈdɪkt] v.预言,预测,预告<br>verify[ˈverɪfaɪ] vt.证实,查证;证明<br>contrary[ˈkɒntrəri]a.(to)相反的 n.反对<br>counterpart[ˈkaʊntəpɑ:t]n.对应的人(或物)<br>convey[kənˈveɪ]v.运送;传达,传播<br>coordinate[kəʊ’ɔ:dɪneɪt]a.同等的 n.同等者<br>relevant[ˈreləvənt] a.有关的,中肯的,相应的<br>correspond[ˌkɒrəˈspɒnd] v.通信,(with)符合;(to)相当于<br>criterion[kraɪˈtɪəriən]n.(pl.criteria或criterions)标准，尺度<br>critical[ˈkrɪtɪkl]a.批评的,紧要的;临界的<br>criticize[ˈkrɪtɪsaɪz]v.(criticise)批评,评论<br>crucial[ˈkru:ʃl]a.至关重要的,决定性的<br>cultivate[ˈkʌltɪveɪt]v.耕作,栽培,养殖<br>occur[əˈkɜ:(r)] v.出现;存在;想起<br>occurrence[əˈkʌrəns] n.发生;事件,<br>damage[ˈdæmɪdʒ]v./n.损害,毁坏 n.(pl.)损害赔偿费<br>dash[dæʃ]v./n.突进 n.破折号<br>dazzle[ˈdæzl]v.使惊奇n.耀眼的光<br>decade[ˈdekeɪd]n.十年<br>dedicate[ˈdedɪkeɪt]vt.奉献;献身于<br>profit[ˈprɒfɪt] n.利润v.(by,from)得利<br>generate[ˈdʒenəreɪt] vt.产生,发生;生殖<br>delight[dɪˈlaɪt] n.快乐v.(使)高兴<br>enlighten[ɪnˈlaɪtn]v.启发,启蒙,教导<br>deny[dɪˈnaɪ]v.否认,否定;拒绝<br>negative[ˈnegətɪv] a.消极的n.负数<br>density[ˈdensəti]n.密集,密度,浓度<br>dependent[dɪˈpendənt]a.依靠的,依赖的,从属的<br>deprive[dɪˈpraɪv]vt.剥夺,夺去,使丧失<br>privacy[ˈprɪvəsi] n.(不受干扰的)独处，自由，隐私<br>private[ˈpraɪvət] a.私人的,个人的,秘密的<br>privilege[ˈprɪvəlɪdʒ] n.特权v.给予特权<br>desirable[dɪˈzaɪərəbl]a.值得做的;合意的;期望得到的<br>desperate[ˈdespərət] a.不顾一切的；绝望的<br>despite[dɪˈspaɪt]prep.不管,不顾<br>detect[dɪˈtekt]v.察觉,发觉,侦察<br>determine[dɪˈtɜ:mɪn]v.决心,决定;确定<br>develop[dɪˈveləp]v.发展;显现;发育<br>trivial[ˈtrɪviəl] a.琐碎的;无足轻重的<br>previous[ˈpri:viəs] a.先前的,以前的<br>devote[dɪˈvəʊt]v.(to)奉献,致力<br>vote [vəʊt] n.投票v.表决<br>indifferent[ɪnˈdɪfrənt] a.冷漠的,不关心的,不积极的<br>dilemma[dɪˈlemə]n.(进退两难的)窘境,困境<br>intelligence[ɪnˈtelɪdʒəns] n.智力;情报<br>intellectual[ˌɪntəˈlektʃuəl] n.知识分子 a.智力的<br>discern[dɪˈsɜ:n]v.发现;辨别<br>disgrace[dɪsˈgreɪs]n.失宠v.使失宠<br>disposal[dɪˈspəʊzl]n.处理,处置;布置<br>oppose[əˈpəʊz] v.反对,使对立,使对抗<br>positive[ˈpɒzətɪv] a.肯定的,积极的,绝对的<br>possess[pəˈzes] v.占有,拥有<br>possession[pəˈzeʃn] n.持有;所有权;(pl.)财产<br>obsession[əbˈseʃn] n.迷住, 困扰<br>solution[səˈlu:ʃn] n.解答,解决办法;溶解<br>distinct[dɪˈstɪŋkt]a.清楚的,明显的;(from)截然不同的<br>disturb[dɪˈstɜ:b]v.扰乱,妨碍,使不安<br>turbulent[ˈtɜ:bjələnt] a.狂暴的,无秩序的<br>doctrine[ˈdɒktrɪn]n. 教条，教义；法律原则<br>domain[dəˈmeɪn]n.(活动,思想等)领域,范围;领地<br>dominate[ˈdɒmɪneɪt]v.支配,统治;占优势<br>prominent[ˈprɒmɪnənt] a.突起的,凸出的;突出的<br>masterpiece n.杰作,名著<br>dramatic[drəˈmætɪk]a.戏剧的,戏剧性的;剧烈的<br>rough[rʌf] a.粗糙的,大致的;粗野的<br>tough[tʌf] a.坚韧的,棘手的;强健的<br>dwell[dwel]v.住,居留<br>dynamic[daɪˈnæmɪk]a.动力的,电动的;有生气的<br>status[ˈsteɪtəs] n.地位,身份;情形<br>obstacle[ˈɒbstəkl] n.障碍(物),妨碍,阻碍<br>economic[ˌi:kəˈnɒmɪk]a.经济(上)的,经济学的<br>eligible[ˈelɪdʒəbl]a.符合条件的;(尤指婚姻等)合适(意)的<br>eloquent[ˈeləkwənt]a.雄辩的,有说服力的;善辩的<br>embark[ɪmˈbɑ:k]v.(使)上船(或飞机,汽车等);着手,从事<br>embarrass[ɪmˈbærəs]vt.使困窘,使局促不安;阻碍<br>embody[ɪmˈbɒdi]vt.具体表达;包含,收录<br>emotion[ɪˈməʊʃn]n.情绪,情感,感情<br>mood[mu:d] n.心情,情绪;语气<br>motivate[ˈməʊtɪveɪt] vt.促动；激励，作为…的动机<br>emphasis[ˈemfəsɪs]n.强调,重点<br>empirical[ɪmˈpɪrɪkl]a.凭经验(或观察)的,经验主义的<br>theoretical[ˌθɪəˈretɪkl] a.理论(上)的<br>employ[ɪmˈplɔɪ]n./v.雇用;用,使用<br>endeavor[ɪn’devə]v./n.(endeavour)努力,尽力,力图<br>endurance[inˈdjuərəns]n.忍耐(力),持久(力),耐久(性)<br>endure[inˈdjuə]v.忍受,持久,持续<br>enhance[inˈhɑ:ns]v.实施;强制;支持’<br>entail[ɪnˈteɪl]vt.使承担;需要;把(疾病等)遗传给<br>enthusiasm[ɪnˈθju:ziæzəm]n.热情;狂热;积极性(for)<br>enthusiastic[ɪnˌθju:ziˈæstɪk]a.热情的,热心的<br>entitle[ɪnˈtaɪtl]v.给以权利(或资格);给…称号(题名);授权<br>envisage[ɪnˈvɪzɪdʒ]v.想象，设想，展望<br>revise[rɪˈvaɪz] v.修订,校订;修正<br>survey[ˈsɜ:veɪ] v./n.眺望;调查;测量图<br>visible[ˈvɪzəbl] a.看得见的, 明显的, 显著的<br>vision[ˈvɪʒn] n.视力;远见;幻想,<br>prudent[ˈpru:dnt] a.谨慎的,智慧的,稳健的<br>escape[ɪˈskeɪp]n.逃跑v.逃跑<br>essay[ˈeseɪ]n.文章,短文<br>essence[ˈesns]n.本质,实质<br>essential[ɪˈsenʃl]a.本质的n.本质<br>statistical[stə’tɪstɪkl] a.统计的,统计学的<br>estimate[ˈestɪmət]v./n.估计;评估<br>ethnic[ˈeθnɪk]a.种族的；人种学的<br>evade[ɪˈveɪd]vt.逃避，回避；避开<br>inevitable[ɪnˈevɪtəbl] a.不可避免的,必然发生的<br>revolve[rɪˈvɒlv] v.(使)旋转;考虑;【天】公转,循环<br>exclusive[ɪkˈsklu:sɪv] a.独占的;排他的;孤高的<br>executive[ɪgˈzekjətɪv] n.总经理a.执行的<br>expertise[ˌekspɜ:ˈti:z] n.专门知识(或技能等),专长<br>explode[ɪkˈspləʊd]v.(使)爆炸,(使)爆发<br>exploit[ɪkˈsplɔɪt]v.开发n.功绩<br>explore[ɪkˈsplɔ:(r)]v.勘探,探测;探究<br>fabulous[ˈfæbjələs]a.极好的；寓言中的<br>fade[feɪd] v.褪色;衰减n.淡入(出)<br>fail[feɪl] v.失败,不及格;衰退<br>faint[feɪnt] a.微弱的n./v.昏倒<br>false[fɔ:ls a.谬误的,虚伪的,伪造的<br>fault[fɔ:lt]n.过失,过错;缺点,毛病<br>feature[ˈfi:tʃə(r)]n.特征v.以…为特色<br>figure[ˈfɪgə(r)]n.数字v.描绘<br>fertilizer[ˈfɜ:təlaɪzə(r)]n.(fertiliser)肥料<br>fetch[fetʃ]v.取来;接来;引出n.取得<br>fierce[fɪəs]a.凶猛的,残忍的;狂热的<br>finance[ˈfaɪnæns]n.财政,金融 v.为…提供资金<br>fine[faɪn]a.美好的v./n.罚金,罚款<br>flaw[flɔ:] n.裂缝;缺陷 v.使破裂;使有缺陷<br>forecast[ˈfɔ:kɑ:st] v./n.预测,预报<br>formidable[ˈfɔ:mɪdəbl] a.强大的;令人敬畏的;可怕的<br>frown[fraʊn] v.皱眉<br>fulfill[fʊl’fɪl] v.(fulfil)完成,履行,实践<br>fundamental[ˌfʌndəˈmentl] a.基础的n.(pl.)基本原则<br>profound[prəˈfaʊnd] a.深刻的,意义深远的;渊博的<br>generous[ˈdʒenərəs] a.宽宏大量的,慷慨的<br>genetic[dʒəˈnetɪk] a.遗传(学)的 n.[-s]遗传学<br>glamour[ˈglæmə(r)] n.(glamour)魅力vt.迷惑<br>glowing[ˈgləʊɪŋ] adj. 灼热的v. 发光<br>gorgeous[ˈgɔ:dʒəs] a.华丽的；灿烂的；美丽的<br>grant[grɑ:nt] v.同意n.授予物<br>guarantee[ˌgærənˈti:] n.保证v.保证<br>warrant [ˈwɒrənt ]v.穿着，戴着n.穿，戴<br>safeguard[ˈseɪfgɑ:d] v.维护 n.安全装置<br>guilty[ˈgɪlti] a.(of)有罪的,内疚的<br>harmony[ˈhɑ:məni]n.协调,和谐;融洽<br>inherit[ɪnˈherɪt] vt.继承(金钱等)，经遗传而得(性格、特征)<br>hierarchy[ˈhaɪərɑ:ki]n.等级制度；统治集团，领导层<br>hijack[ˈhaɪdʒæk]v.劫持，劫机，拦路抢劫<br>honor[‘ɒnə(r)] n.(honour)尊敬;荣誉v.尊敬<br>horizon[həˈraɪzn] n.地平线;眼界,见识<br>orient[ˈɔ:rient] n.亚洲 v.为…定位<br>original[əˈrɪdʒənl] a.最初的n.原文<br>originate[əˈrɪdʒɪneɪt] v.(in,from)起源;首创,创造<br>ideology[ˌaɪdiˈɒlədʒi] n.意识形态，(政治或社会的)思想意识<br>illegal[ɪˈli:gl] a.不合法的,非法的<br>legislation[ˌledʒɪsˈleɪʃn] n.法律(规)；立法，法律的制定(或通过)<br>legitimate[lɪˈdʒɪtɪmət] a.合法的vt.使合法<br>license[ˈlaɪsns] n.(licence)许可证v.准许,认可<br>illusion[ɪˈlu:ʒn] n.幻想，错误的观念；错觉，幻觉，假象<br>illustrate[ˈɪləstreɪt] v.举例说明,阐明;图解<br>image[ˈɪmɪdʒ] n.形象；形象的描述，比喻<br>imagine[ɪˈmædʒɪn] v.想象,设想,料想<br>imitate[ˈɪmɪteɪt v.模仿,仿效; 伪造<br>imperative[ɪmˈperətɪv] n.命令a.强制的<br>indispensable[ˌɪndɪˈspensəbl] a.(to,for)必不可少的,必需的<br>pension[ˈpenʃn] n.养老金,年金<br>individual[ˌɪndɪˈvɪdʒuəl] a.个人的n.个人,个体<br>inferior[ɪnˈfɪəriə(r)] a.下等的n.下级<br>superior[su:ˈpɪəriə(r)] a.卓越的n.上级<br>prior[ˈpraɪə(r)] a.优先的,在前的;(to)在…之前<br>priority[praɪˈɒrəti] n.先,前;优先,重点<br>initiative[ɪˈnɪʃətɪv] a.创始的n.第一步<br>innocent[ˈɪnəsnt] a.(of)清白的,无罪的;单纯的<br>innovation[ˌɪnəˈveɪʃn] n.改革，革新；新观念<br>novel[ˈnɒvl] n.(长篇)小说 a.新奇的<br>novelty [ˈnɒvlti] n.新奇,新颖,新奇的事物<br>preside[prɪˈzaɪd] v.(at,over)主持<br>interpret[ɪnˈtɜ:prɪt] vt.解释,说明;口译<br>invalid[ɪnˈvælɪd] n.伤残人 a.伤残的;无效的<br>valid[ˈvælɪd] a.有效的;有根据的;正当的<br>prevail[prɪˈveɪl] v.(over,against)取胜,占优势;流行<br>variable[ˈveəriəbl] a.可变的n.变量<br>variety[vəˈraɪəti] n.种种,多种多样;种类<br>various[ˈveəriəs] a.各种各样的;不同的<br>vary[ˈveəri] vt.改变,变化;使多样化<br>invent[ɪnˈvent] v.发明,创造;捏造<br>journalist[ˈdʒɜ:nəlɪst] n.记者,新闻工作者<br>justify[ˈdʒʌstɪfaɪ] v.证明…正当(或有理、正确)，为…辩护<br>senate[ˈsenət] n.参议院,上院<br>later[ˈleɪtə(r)] ad.后来,过后<br>launch[lɔ:ntʃ] v.发射n.发射<br>leading[ˈli:dɪŋ] a.领导的,指导的;第一位的<br>release[rɪˈli:s] v.释放n.释放<br>reliance[rɪˈlaɪəns] n.信任,信心,依靠<br>religious[rɪˈlɪdʒəs] a.宗教的,信教的,虔诚的<br>liberal[ˈlɪbərəl] a.慷慨的;富足的;自由的<br>literacy[ˈlɪtərəsi] n.有文化,有教养,有读写能力<br>loyalty[ˈlɔɪəlti] n.忠诚,忠心<br>magnificent[mægˈnɪfɪsnt] a.华丽的,高尚的,宏伟的<br>margin[ˈmɑ:dʒɪn] n.页边空白;边缘;余地<br>mass[mæs] n.众多;(pl.)群众;质量<br>massive[ˈmæsɪv] a.厚实的,粗大的;大规模的<br>optimistic[ˌɒptɪˈmɪstɪk] a.乐观主义的<br>pessimistic[ˌpesɪˈmɪstɪk] a.悲观(主义)的<br>mechanism[ˈmekənɪzəm] n.机械装置,机构;机制<br>metal[ˈmetl] n.金属,金属制品<br>miserable[ˈmɪzrəbl] a.痛苦的,悲惨的<br>misery[ˈmɪzəri] n.痛苦,悲惨,不幸<br>mobile[ˈməʊbaɪl] a.可动的,活动的,运动的<br>monopoly[məˈnɒpəli] n.垄断,专卖,专利权<br>youngster [ˈjʌŋstə(r)] n.小伙子，年轻人；少年<br>moral[ˈmɒrəl] a.道德(上)的n.寓意,教育意义<br>mount[maʊnt] v.登上n.支架<br>neat[ni:t] a.整洁的,干净的,优美的<br>nightmare[ˈnaɪtmeə(r)] n.恶梦；可怕的事物，无法摆脱的恐惧<br>nuclear[ˈnju:kliə(r)] a.核心的,中心的;原子核的<br>nuisance[ˈnju:sns] n.讨厌的人(或东西);麻烦事<br>oblige[əˈblaɪdʒ] v.强迫;责成;(使)感激<br>occupy[ˈɒkjupaɪ] v.占用;占据;使忙碌<br>odd[ɒd] a.奇数的;奇怪的;单只的<br>odds[ɒdz] n.不平等,差异;机会<br>upset[ʌpˈset] v.使…心烦意乱a.难过的<br>opponent[əˈpəʊnənt] n.对手a.对立的<br>suppress[səˈpres] v.镇压;抑制,忍住<br>stress[stres] n.压力vt.强调<br>organic[ɔ:ˈgænɪk] a.器官的;有机的;有机体的<br>paradox[ˈpærədɒks] n.似非而是的话,自相矛盾的话,反论<br>outrage[ˈaʊtreɪdʒ] n.暴行v.凌辱<br>overall[ˌəʊvərˈɔ:l] a.全面的n.(pl.)(套头)工作服<br>overseas[ˌəʊvəˈsi:z] a.外国的ad.在海外<br>overturn[ˌəʊvəˈtɜ:n] n.倾覆 v.推翻<br>palm[pɑ:m] n.手掌vt.与…握手,藏…于掌中<br>parallel[ˈpærəlel] a.相同的n.类似<br>parcel[ˈpɑ:sl] n.包裹v.打包<br>pocket[ˈpɒkɪt] n.衣袋 a.袖珍的<br>pattern[ˈpætn] n.模式;图案v.仿制<br>payment[ˈpeɪmənt] n.支付<br>peace[pi:s] n.和平;平静,安宁<br>pebble[ˈpebl] n.卵石<br>specialize[ˈspeʃəlaɪz] v.(specialise)(in)专攻,专门研究,专业化<br>specific[spəˈsɪfɪk] a.明确的,具体的;特定的<br>specify[ˈspesɪfaɪ] v.指定,详细说明<br>permanent[ˈpɜ:mənənt] a.永久的,持久的<br>persuade[pəˈsweɪd] v.说服,劝说;(of)使相信<br>philosophy[fəˈlɒsəfi] n.哲学,哲理,人生观<br>physical[ˈfɪzɪkl] a.物质的;身体的;物理的<br>physician[fɪˈzɪʃn] n.内科医生<br>pilgrim[ˈpɪlgrɪm] n.(在国外)旅游者;朝圣者,最初的移民<br>plague[pleɪg] n.瘟疫vt.折磨<br>political[pəˈlɪtɪkl] a.政治的<br>politician [ˌpɒləˈtɪʃn] n.政治家,政客<br>politics[ˈpɒlətɪks] n.政治,政治学;政纲<br>wonder [ˈwʌndə(r)] n.惊奇, v.(at)想知道<br>portray[pɔ:ˈtreɪ] v.描写，描述；画(人物、景象等)<br>trait[treɪt] n.特征,特点,特性<br>potential[pəˈtenʃl] a.潜在的n.潜能<br>primary[ˈpraɪməri] a.最初的,初级的;首要的<br>prime[praɪm] a.首要的n.青春,全盛期<br>prohibit[prəˈhɪbɪt]v.禁止,不准;阻止<br>prone[prəʊn] a.倾向于,俯伏的,倾斜的<br>prosper[ˈprɒspə(r)] v.成功,兴隆,昌盛<br>protest[ˈprəʊtest] v./n.主张,断言,抗议<br>psychology[saɪˈkɒlədʒi] n.心理,心理学,心理状态<br>purchase[ˈpɜ:tʃəs] v.买,购买 n.购买的物品<br>pursue[pəˈsju:] v.追赶;继续,从事<br>pursuit[pəˈsju:t] n.追赶,追求;职业<br>radical[ˈrædɪkl] a.基本的,重要的;激进的<br>random[ˈrændəm] a.随机的n.随机<br>range[reɪndʒ] n.范围v.排列成行<br>rank[ræŋk] n.社会阶层v.分等级<br>rate[reɪt] n.速率v.估价<br>rational[ˈræʃnəl] a.理性的,合理的<br>reason[ˈri:zn] n.原因v.说服<br>realm[relm] n.国土;领域<br>royal[ˈrɔɪəl] a.王室的;第一流的,高贵的<br>reckless[ˈrekləs] a.,大意的,卤莽的,不顾后果的<br>recruit[rɪˈkru:t] v.补充 n.新成员<br>reflect[rɪˈflekt] v.反射,反映,表现<br>reign[reɪn] n./v.统治 n.统治时期<br>reluctant[rɪˈlʌktənt] a.不愿的,勉强的<br>render[ˈrendə(r)] v.使得,致使;提出<br>represent[ˌreprɪˈzent] v.描述,代表;阐明<br>request[rɪˈkwest] v./n.请求,要求<br>require[rɪˈkwaɪə(r)] v.需要;(of)要求,命令<br>resemble[rɪˈzembl] v.像,类似<br>respond[rɪˈspɒnd] v.回答,响应,作出反应<br>restore[rɪˈstɔ:(r)] v.恢复;归还;修复<br>reward[rɪˈwɔ:d] n.(for)报酬,赏金 v.(for)酬劳;酬谢<br>rigid[ˈrɪdʒɪd] a.刚性的;刻板的;严厉的<br>vigorous[ˈvɪgərəs] a.朝气蓬勃的,精力旺盛的<br>ritual[ˈrɪtʃuəl] a.宗教仪式的n.(宗教)仪式<br>robust[rəʊˈbʌst] a.强健的,雄壮的,精力充沛的<br>root[ru:t] n.根v.(使)生根<br>ruin[ˈru:ɪn] v.毁灭n.毁灭<br>sacred[ˈseɪkrɪd] a.神圣的;宗教的;庄严的<br>satire[ˈsætaɪə(r)] n.讽刺,讽刺文学,讽刺作品<br>scarf[skɑ:f] n.围巾,头巾,领巾<br>scatter[ˈskætə(r)] v.散开,驱散;散布<br>scheme[ski:m] n.计划, v.,策划<br>scrutiny[ˈskru:təni] n.周密的调查；仔细看；监视<br>shame[ʃeɪm] n.羞耻v.使羞愧<br>shelter[ˈʃeltə(r)] n.碎片v.粉碎<br>sincere[sɪnˈsɪə(r)] a.诚挚的,真实的,诚恳的<br>stuff[stʌf] n.原料 v.填满<br>sponsor[ˈspɒnsə(r)] n.发起人v.发起<br>spur[spɜ:(r)] n.刺激v.刺激<br>steep[sti:p] a.陡峭的vt.浸泡<br>stern[stɜ:n] a.严厉的n.船尾<br>strengthen[ˈstreŋθn] v.加强,巩固<br>strike[straɪk] n.击v.抚摸<br>subsidy[ˈsʌbsədi] n.补助金；津贴费<br>subtle[ˈsʌtl] a.精巧的,巧妙的;细微的<br>urban[ˈɜ:bən] a.城市的, 市内的<br>suffice[səˈfaɪs] v.充足；vt.(食物等)使(某人)满足<br>superficial[ˌsu:pəˈfɪʃl] a.表面的;肤浅的,浅薄的<br>supreme[su:ˈpri:m] a.极度的,最重要的;至高的<br>tackle[ˈtækl] n.滑车；工具 v.解决<br>vehicle[ˈvi:əkl] n.交通工具;媒介,载体<br>terrify[ˈterɪfaɪ] v.使害怕,使惊恐<br>tolerance[ˈtɒlərəns] n.宽容；容忍；耐药力<br>traffic[ˈtræfɪk] n.交通,交通量<br>translation[trænsˈleɪʃn] n.翻译;译文,译本<br>tremendous[trəˈmendəs] a.巨大的,极大的<br>ultimate[ˈʌltɪmət] a.最后的,最终的;根本的<br>undergraduate[ˌʌndəˈgrædʒuət] n.大学生,大学肆业生<br>underline[ˌʌndəˈlaɪn] vt.在…下划线;强调<br>undermine[ˌʌndəˈmaɪn] v.暗中破坏,逐渐削弱;侵蚀…的基础<br>unfold[ʌnˈfəʊld] vt.打开;显露;展示 vi.呈现;显示;展示<br>union[ˈju:niən] n.制服a.相同的<br>unique[juˈni:k] a.唯一的,独一无二的<br>update[ˌʌpˈdeɪt] v.更新,使现代化<br>urge[ɜ:dʒ] vt.催促n.强烈欲望,<br>urgent[ˈɜ:dʒənt] a.急迫的，紧要的，紧急的<br>vanish[ˈvænɪʃ] vi.突然不见;消失<br>vanity[ˈvænəti] n.虚荣心,浮华<br>violate[ˈvaɪəleɪt] vt.违背;冒犯;妨碍<br>virtue[ˈvɜ:tʃu:] n.德行;贞操;优点<br>vital[ˈvaɪtl] a.生死攸关的,重大的;生命的,生机的<br>volume[ˈvɒlju:m] n.容积,体积;音量<br>voluntary [ˈvɒləntri] a.自愿的,志愿的<br>volunteer [ˌvɒlənˈtɪə(r)] n./v.自愿(者,兵);自愿(提供)<br>whisper [ˈwɪspə(r)] v.私下说n.耳语;传闻<br>wisdom [ˈwɪzdəm] n.智慧,明智;名言<br>withhold [wɪðˈhəʊld] vt.使停止; vi.忍住<br>worldwide [ˈwɜ:ldwaɪd] a.全世界的ad.遍及全世界<br>yield [ji:ld] v.出产;(to)屈服n.产量<br>zeal [zi:l] n.热心，热忱，热情</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;美国政治常识&quot;&gt;&lt;a href=&quot;#美国政治常识&quot; class=&quot;headerlink&quot; title=&quot;美国政治常识&quot;&gt;&lt;/a&gt;美国政治常识&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;/2017/12/05/考研英语相关/IMG_1701.JPG&quot; alt=&quot;img&quot;&gt;&lt;
    
    </summary>
    
      <category term="英语" scheme="http://ynuwm.github.io/categories/%E8%8B%B1%E8%AF%AD/"/>
    
    
      <category term="考研" scheme="http://ynuwm.github.io/tags/%E8%80%83%E7%A0%94/"/>
    
  </entry>
  
  <entry>
    <title>六级英语相关</title>
    <link href="http://ynuwm.github.io/2017/12/03/%E5%85%AD%E7%BA%A7%E7%9B%B8%E5%85%B3/"/>
    <id>http://ynuwm.github.io/2017/12/03/六级相关/</id>
    <published>2017-12-03T01:07:43.000Z</published>
    <updated>2017-12-05T01:54:29.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="写作主题词"><a href="#写作主题词" class="headerlink" title="写作主题词"></a>写作主题词</h2><p>1.顺境与逆境( Favorable  Circumstances  and  adverse Circumstances)<br>2.勤奋(hard working, diligence, painstaking efforts）<br>3.谨慎(prudence and determination）<br>4.坚持/毅力(perseverance，persistence, determination）<br>5.热情和乐观(enthusiasm and optimism）<br>6.博学和求知(learnedness and seeking knowledge/pursuit of knowledge）<br>7.活力(vitality）<br>8.身强体壮，充满活力(bursting with vitality and good health）<br>9.独立 (independence)<br>10.感恩(gratitude ,gratification)<br>11.创新( creation, innovation, critical mind, critical thinking, unconventional thinking )<br>12.鼓励 (encouragement)13.真诚(sincerity）<br>14.宽容(humanity, love, understanding and tolerance）<br>15.自满和谦逊(Being self-satisfied and being modest）<br>16.勇敢(courage and bravery）<br>17.敬业精神(professional dedication and professional ethics )<br>18.业务水平(competence）<br>19.苦难(suffering and hardship）<br>20.简朴(simplicity）<br>21.谦逊的耐心(moderation and patience）<br>22.适应性(adaptability）<br>23.果敢性(decisiveness）<br>24.羡慕(admiration; )嫉妒(jealousy；envy）<br>25.榜样（example, model）<br>26.时间管理（time management）守时（punctuality） </p>
<h2 id="六级核心救命词"><a href="#六级核心救命词" class="headerlink" title="六级核心救命词"></a>六级核心救命词</h2><h3 id="救命核心名词"><a href="#救命核心名词" class="headerlink" title="救命核心名词"></a>救命核心名词</h3><p>abundance /ə’bʌndəns/ n.丰富，充<br>absorption /əb’sɔ:pʃn/  n. 吸收，专注<br>abbreviation /əˌbri:vi’eiʃn/ n.节略，缩写，缩短<br>hospitality /ˌhɒspi’tæləti/ n.友好，好客<br>pastime /‘pɑ:staim/ n. 消遣，娱乐<br>revenue /‘revənju:/ n. 税收，岁入<br>routine /ru:’ti:n/ n.常规，惯例，例行公事<br>scorn /skɔ:n/ n. 轻蔑，鄙视<br>shortage /‘ʃɔ:tidʒ/ n. 短缺，不足<br>smash /smæʃ/ n. 打碎，粉碎<br>stability /stə’biləti/ n.稳定(性)，稳固<br>approach /ə’prəʊtʃ/ n. 方法，途径，接近 vt. 接近，着手处理 vi. 靠近<br>standard /‘stændəd/ n. 标准<br>survival /sə’vaivl/ n. 幸存，幸存者，残存物<br>temperament /‘temprəmənt/ n. 气质，性格<br>threshold /‘θreʃhəʊld/ n. 开端，入口<br>tolerance /‘tɒlərəns/ n. 容忍，忍耐力<br>transaction /træn’zækʃn/ n.处理，办理，交易<br>trend /trend/ n. 倾向，趋势<br>transition /træn’ziʃn/ n. 过渡，转变<br>variation /ˌveəri’eiʃn/ n. 变化，变动<br>warehouse /‘weəhaʊs/ n. 货仓<br>community /kə’mju:nəti/ n. 社区，（生态） 群落，共同体，团体<br>access /‘ækses/ n. 入口，通路，接触<br>accommodation /əˌkɒmə’deiʃn/ n. 住宿，调节，和解<br>acknowledgment /ək’nɒlidʒmənt/ n. 承认，感谢，致谢<br>pattern /‘pætn/ n. 模式<br>penalty /‘penəlti/ n. 制裁，惩罚<br>pension /‘penʃn/ n. 养老金<br>personality /ˌpɜ:sə’næləti/ n. 人格，人性<br>pledge /pledʒ/ n. 保证，誓言<br>position /pə’ziʃn/ n. 位置，职位，职务<br>predecessor /‘pri:disesə(r)/ n. 前任，原有的事物<br>premise /‘premis/ n. 前提，假设<br>prescription /pri’skripʃn/ n. 处方<br>preservation /ˌprezə’veiʃn/ n. 保护，防护<br>prestige /pre’sti:ʒ/ n. 威信，威望<br>priority /prai’ɒrəti/ n. 优先(权)<br>prospect /‘prɒspekt/ n. 前景，可能性<br>rate /reit/ n. 速度 v. 评价，评估<br>ration /‘ræʃn/ n. 比率，定量，配给量<br>reflection /ri’flekʃn/ n. 反映，表现<br>recession /ri’seʃn/ n. (经济)衰退，不景气，后退<br>reputation /ˌrepju’teiʃn/ n. 名声，声望<br>reservation /ˌrezə’veiʃn/ n. 贮存，贮藏，预订<br>illusion /i’lu:ʒn/ n. 错觉，假象<br>ingredient /in’gri:diənt/ n. 成分<br>insight /‘insait/ n. 理解，洞察力<br>inspection /in’spekʃn/ n. 检查，视察<br>instinct /‘instiŋkt/ n. 本能，直觉<br>integrity /in’tegrəti/ n. 正直，诚实<br>intuition /ˌintju’iʃn/ n. 直觉<br>lease /li:s/ n. 租约，契约<br>legislation /ˌledʒis’leiʃn/ n. 立法，法律<br>limitation /ˌlimi’teiʃn/ n. 局限性，缺点<br>loyalty /‘lɔiəlti/ n. 忠诚，忠心<br>luxury /‘lʌkʃəri/ n. 奢侈，豪华<br>manifestation /ˌmænife’steiʃn/ n. 表现(形式)<br>mechanism /‘mekənizəm/ n. 机械装置<br>minority /mai’nɒrəti/ n. 少数<br>misfortune /ˌmis’fɔ:tʃu:n/ n. 不幸，灾难<br>morality /mə’ræləti/ n. 道德，美德<br>notion /‘nəʊʃn/ n. 概念，观念，理解<br>obligation /ˌɒbli’geiʃn/ n.(法律上或道义上)责任<br>occasion /ə’keiʒn/ n. 场合<br>opponent /ə’pəʊnənt/ n. 敌人，对手<br>ornament /‘ɔ:nəmənt/ n. 装饰，装饰品<br>admiration /ˌædmə’reiʃn/ n. 欣赏<br>advocate /‘ædvəkeit/ n. 提倡者，拥护者<br>allowance /ə’laʊəns/ n. 津贴<br>ambition /æm’biʃn/ n. 野心，雄心<br>analogy /ə’nælədʒi/ n. 相似，模拟，类比<br>anticipation /ænˌtisi’peiʃn/ n. 预期，期望<br>appreciation /əˌpri:ʃi’eiʃn/ n. 感谢，感激<br>array /ə’rei/ n. 陈列，一系列<br>assurance /ə’ʃʊərəns/ n. 保证<br>multivitamin /ˌmʌlti’vitəmin/ adj. 多种维他命的 n. 多种维他命剂<br>blunder /‘blʌndə(r)/ n. 错误，大错<br>budget /‘bʌdʒit/ n. 预算<br>capability /ˌkeipə’biləti/ n. 能力，才能<br>cash /kæʃ/ n. 现金<br>circulation /ˌsɜ:kjə’leiʃn/ n. 流通，传播，发行量<br>commitment /kə’mitmənt/ n. 承诺，许诺<br>compensation /ˌkɒmpen’seiʃn/ n. 补偿，赔偿<br>consideration /kənˌsidə’reiʃn/ n. 考虑<br>distinction /di’stiŋkʃn/ n. 区分，辨别<br>emergency /i’mɜ:dʒənsi/ n. 紧急情况<br>encouragement /in’kʌridʒmənt/ n. 鼓励<br>essence /‘esns/ n. 本质<br>estimate /‘estimət/ n. 估计<br>expenditure / ik’spenditʃə(r) / n. 开支<br>extinction /ik’stiŋkʃn/ n. 消失，消灭，灭绝<br>quality/‘kwɒləti / n. 质量，（统计） 品质，特性，才能<br>flaw /flɔ:/ n. 裂纹，瑕疵<br>fortune /‘fɔ:tʃu:n/ n. 财产，大笔的钱<br>fraction /‘frækʃn/ n. 小部分，一点<br>symptom/‘simptəm/ n.（临床） 症状，征兆<br>guarantee /ˌgærən’ti:/ n. 保修单<br>guilt /gilt/ n. 犯罪<br>harmony /‘hɑ:məni/ n.协调， 一致，和谐  </p>
<h3 id="救命核心形容词"><a href="#救命核心形容词" class="headerlink" title="救命核心形容词"></a>救命核心形容词</h3><p>accessory /ək’sesəri/ n. 配件，附件 adj. 附属的<br>absurd /əb’sɜ:d/ adj.不合理的，荒唐的<br>abstract /‘æbstrækt/  n. 摘要  vt. 摘要，提取  adj. 抽象的<br>absent /‘æbsənt/ adj.不在意的<br>abnormal /æb’nɔ:ml/ adj.不正常的<br>absurd /əb’sɜ:d/ adj.荒缪的<br>abundant /ə’bʌndənt/ adj.丰富的<br>acute /ə’kju:t/ adj.敏锐的，锋利的<br>aggressive /ə’gresiv/ adj.侵略的，好斗的，有进取心的<br>ambiguous /æm’bigjuəs/ adj.模棱两可的，模糊的<br>ambitious /æm’biʃəs/ adj.有雄心的，有抱负的<br>appropriate /ə’prəʊpriət/ adj.合适的，恰当的 vt. 占用，拨出<br>authentic /ɔ:’θentik/ adj.可靠的，可信的<br>average /‘ævəridʒ/ adj. 一般的，普通的<br>barren /‘bærən/ adj.贫瘠的，不毛的<br>bound /baʊnd/ adj.一定的<br>chronic /‘krɒnik/ adj.慢性的<br>commentary /‘kɒməntri/ adj.实况报道<br>compact /kəm’pækt/ adj. 紧凑的，小巧的<br>competitive /kəm’petətiv/ adj.竞争性的，具有竞争力的<br>compulsory /kəm’pʌlsəri/ adj.强迫的，强制的，义务的<br>confidential /ˌkɒnfi’denʃl/ adj. 机紧的，秘密的<br>conservative /kən’sɜ:vətiv/ adj. 保守的，传统的<br>consistent /kən’sistənt/ adj. 和……一致<br>conspicuous /kən’spikjuəs/ adj. 显而易见的，引人注目的<br>crucial /‘kru:ʃl/ adj. 关键的<br>current /‘kʌrənt/ adj. 当前的<br>decent /‘di:snt/ adj. 体面像样的，还不错的<br>delicate /‘delikət/ adj. 精细的，微妙的，纤弱的<br>destructive /di’strʌktiv/ adj. 毁灭的<br>economic /ˌi:kə’nɒmik/ adj. 经济的<br>elegant /‘eligənt/ adj. 优雅的，  优美的，  精致的<br>embarrassing /im’bærəsiŋ/ adj. 令人尴尬的<br>energetic /ˌenə’dʒetik/ adj. 精力充沛的<br>equivalent /i’kwivələnt/ adj. adj. 等价的，相等的，同意义的<br>eternal /i’tɜ:nl/ adj. 永恒的，无休止的<br>exclusive /ik’sklu:siv/ adj. 独有的，排他的<br>extinct /ik’stiŋkt/ adj. 灭绝的<br>fake /feik/ adj. 假的，冒充的<br>fatal /‘feitl/ adj. 致命的，毁灭性的<br>feasible /‘fi:zəbl/ adj. 可行的<br>feeble /‘fi:bl/ adj. 脆弱的，虚弱的<br>gloomy /‘glu:mi/ adj. 暗淡的<br>individual/ˌindi’vidʒuəl/ adj. 个人的，个别的，独特的 n. 个人，个体<br>identical /ai’dentikl/ adj. 相同的，一样的<br>imaginative /i’mædʒinətiv/ adj. 富有想象力的，  爱想象的<br>inaccessible /ˌinæk’sesəbl/ adj.达不到的，难以接近<br>inadequate /in’ædikwət/ adj.不充分的，不适当的<br>incredible /in’kredəbl/ adj. 难以置信的<br>indifference/in’difrəns/ adj. 不关心的，冷漠的<br>indignant /in’dignənt/ adj. 生气的，愤怒的<br>infectious /in’fekʃəs/ adj. 传染的，传染性的<br>inferior /in’fiəriə(r)/ adj.劣质的，地位较低的，较差的<br>inherent /in’hiərənt/ adj. 固有的，生来的<br>inspirational /ˌinspə’reiʃənl/ adj. 灵感的<br>intent /in’tent/ adj. 专心的，专注的<br>intricate /‘intrikət/ adj. 复杂精细的<br>intrinsic /in’trinsik/ adj. 固有的，  本质的，   内在的<br>irreplaceable /ˌiri’pleisəbl/ adj. 不能替换的， 不能代替的<br>literal /‘litərəl/ adj.文字的，  字面的<br>massive /‘mæsiv/ adj. 大规模的，大量的<br>merciful /‘mɜ:sifl/ adj. 仁慈的，宽大的<br>mobile /‘məʊbail/ adj. 活动的，流动的<br>naive /nai’i:v/ adj.天真的，  质朴的<br>negligible /‘neglidʒəbl/ adj.微不足道的<br>notorious /nəʊ’tɔ:riəs/ adj.臭名昭著的，  声名狼藉的<br>obedient /ə’bi:diənt/ adj.服从的，  顺从的<br>obscure /əb’skjʊə(r)/ adj. 模糊不清的<br>optimistic /ˌɒpti’mistik/ adj. 乐观的<br>original /ə’ridʒənl/ adj. 原先的，最早的<br>pathetic /pə’θetik/ adj. 悲哀的，悲惨的<br>persistent /pə’sistənt/ adj. 坚持不懈的<br>potential /pə’tenʃl/ adj. 可能的，潜在的<br>prevalent /‘prevələnt/ adj. 普遍的，流行的<br>primitive /‘primətiv/ adj. 原始的，早期的<br>proficient /prə’fiʃnt/ adj. 熟练的，精通的<br>profound /prə’faʊnd/ adj. 深刻的，深远的<br>prominent /‘prɒminənt/ adj. 突出的，杰出的<br>prompt /prɒmpt/ adj. 即刻的，迅速的<br>raw /rɔ:/ adj. 自然状态的，未加工的<br>relevant /‘reləvənt/ adj. 与…有关的<br>respectable /ri’spektəbl/ adj. 可尊敬的<br>rewarding /ri’wɔ:diŋ/ adj. 值得的<br>rough /rʌf/ adj. 粗略的，不精确的<br>rude /ru:d/ adj. 粗鲁的，不礼貌的<br>sensitive /‘sensətiv/ adj. 敏感的<br>sheer /ʃiə(r)/ adj. 完全的，十足的<br>shrewd /ʃru:d/ adj. 精明的<br>stationary /‘steiʃənri/ adj. 固定的<br>subordinate /sə’bɔ:dinət/ adj. 次要的，从属的<br>subtle /‘sʌtl/ adj. 微妙的， 精巧的，细微的<br>superficial /ˌsu:pə’fiʃl/ adj. 肤浅的<br>suspicious /sə’spiʃəs/ adj. 对…怀疑<br>tedious /‘ti:diəs/ adj. 冗长的，乏味的<br>trivial /‘triviəl/ adj. 琐碎的，  不重要的<br>turbulent /‘tɜ:bjələnt/ adj. 动荡的，混乱的<br>underlying /ˌʌndə’laiiŋ/ adj. 潜在的<br>versatile /‘vɜ:sətail/ adj. 多才多艺的<br>vivid /‘vivid/ adj. 生动的，栩栩如生的<br>void /vɔid/ adj. 无效的<br>vulnerable /‘vʌlnərəbl/ adj. 易受伤的<br>worth /wɜ:θ/ adj. 值得</p>
<h3 id="救命核心动词"><a href="#救命核心动词" class="headerlink" title="救命核心动词"></a>救命核心动词</h3><p>accord /ə’kɔ:d/ v.使符合，使适合<br>abandon /ə’bændən/ v. 抛弃，放弃，放纵，使沉溺于<br>abolish /ə’bɒliʃ/ v.废除，取消<br>acknowledge /ək’nɒlidʒ/ v.承认，答谢，报偿<br>acquaint /ə’kweint/ v. 熟悉，认识<br>acquire /ə’kwaiə(r)/ v. (靠己能力努力行为)获得<br>afford /ə’fɔ:d/ v. 付得起<br>allege /ə’ledʒ/ v. 断言，宣称<br>alternate / ɔ:l’tɜ:nət / v. 交替，轮流  / ɔːl’tɜːnit / adj. 交替的<br>anticipate /æn’tisipeit/ v. 预期<br>applaud /ə’plɔ:d/ v. 赞扬，称赞<br>ascend /ə’send/ v. 上升，攀登<br>ascribe /ə’skraib/ v. 归因于，归功于<br>assemble /ə’sembl/ v. 集合，聚集<br>assign /ə’sain/ v.分派，指派(职务，任务)<br>attribute /ə’tribju:t/ v. 归因于<br>base /beis/ v. 建立在……的基础上<br>bewilder /bi’wildə(r)/ v. 迷惑，弄糊涂<br>breed /bri:d/ v. 培育，养育<br>cling /kliŋ/ v. 坚守，抱紧<br>coincide /ˌkəʊin’said/ v. 相同，相一致<br>collaborate /kə’læbəreit/ v. 合著，合作<br>collide /kə’laid/ v. 互撞，碰撞<br>commence /kə’mens/ v. 开始<br>compensate /‘kɒmpenseit/ v. 补偿，赔偿<br>complement /‘kɒmpliment/ v.与…结合，补充<br>comply /kəm’plai/ v. 遵守<br>conceive /kən’si:v/ v. 想出，设想<br>concern /kən’sɜ:n/ v. 涉及，担忧<br>condense /kən’dens/ v. 压缩，浓缩<br>conflict /‘kɒnflikt/ v. 冲突，战争<br>conform /kən’fɔ:m/ v. 符合，遵守，适应<br>confront /kən’frʌnt/ v. 面对，面临<br>conserve /kən’sɜ:v/ v. 保护，保存<br>consolidate /kən’sɒlideit/ v. 巩固<br>convey /kən’vei/ v. 表达，传达<br>crash /kræʃ/ v. (飞机)坠毁<br>cruise /kru:z/ v. 航行，漫游<br>dazzle /‘dæzl/ v. 使眩目，耀眼<br>deceive /di’si:v/ v. 欺骗，哄骗<br>decline /di’klain/ v. 下降，减少<br>dedicate /‘dedikeit/ v. 奉献，献身，致力于<br>defend /di’fend/ v. 为…辩护<br>defy /di’fai/ v. 违抗，藐视<br>deny /di’nai/ v. 否认<br>deprive /di’praiv/ v. 剥夺<br>derive /di’raiv/ v. 得来，得到<br>descend /di’send/ v. 下落<br>deserve /di’zɜ:v/ v. 值得<br>deviate /‘di:vieit/ v. (使)背离，(使)偏离<br>disguise /dis’gaiz/ v. 假扮，伪装<br>dominate /‘dɒmineit/ v. 统治，占据<br>drain /drein/ v. 渐渐耗尽<br>duplicate /‘dju:plikeit/ v. 复制，重复<br>eliminate /i’limineit/ v. 消除<br>endure /in’djʊə(r)/ v. 忍受，忍耐<br>enhance /in’hɑ:ns/ v. 提高，增加<br>enroll /in’rəʊl/ v.使成为……的成员，注册<br>evoke /i’vəʊk/ v. 引起，唤起<br>immerse /i’mɜ:s/ v. 使浸没<br>impose /im’pəʊz/ v. 征税，把…强加于<br>induce /in’dju:s/ v. 劝诱，诱导<br>indulge /in’dʌldʒ/ v. 纵容，放任<br>intend /in’tend/ v. 意欲<br>interpret /in’tɜ:prit/ v. 解释，说明<br>jeopardize /‘dʒepədaiz/ v. 危及，损坏<br>linger /‘liŋgə(r) / v.逗留，徘徊，拖延<br>locate / ləʊ’keit / v. 位于<br>magnify /‘mægnifai/ v. 放大<br>mean /mi:n/ v. 打算，意欲<br>mingle /‘miŋgl/ v. 混合起来，相混合<br>minimize /‘minimaiz/ v. 使减到最少，最小化<br>monitor /‘mɒnitə(r)/ v. 检测，监测<br>neglect /ni’glekt/ v. 忽视<br>occupy /‘ɒkjupai/ v. 占领，使忙碌<br>oppress /ə’pres/ v. 压迫<br>originate /ə’ridʒineit/ v. 首创，起源<br>overlap /ˌəʊvə’læp/ v. 部分重叠<br>overwhelm /ˌəʊvə’welm/ v.压倒，浸没，使不安<br>permeate /‘pɜ:mieit/ v. 渗入，渗透<br>prescribe /pri’skraib/ v.指示，规定，处方开药<br>preside /pri’zaid/ v. 主持<br>prolong /prə’lɒŋ/ v. 延长，拖延<br>promise /‘prɒmis/ v. 许诺<br>propel /prə’pel/ v. 推进，推动<br>protest /‘prəʊtest/ v. 抗议，反对<br>provoke /prə’vəʊk/ v. 引起，激起<br>radiate /‘reidieit/ v. 辐射状发出，从中心向各方伸展出<br>reconcile /‘rekənsail/ v. 使和好，调解<br>refresh /ri’freʃ/ v.提神，使清新，使精力恢复<br>refute /ri’fju:t/ v. 反驳，驳斥，驳倒<br>remain /ri’mein/ v. 停留，依旧是<br>repel /ri’pel/ v. 抗御，抵拒<br>rescue /‘reskju:/ v. 营救，救援<br>resign /ri’zain/ v. 辞职<br>resort /ri’zɔ:t/ v. 求助，凭借，诉诸<br>resume /ri’zju:m/ v. 重新开始，继续<br>revenge /ri’vendʒ/ v. 报仇，报复<br>scan /skæn/ v. 细察，审视<br>scrape /skreip/ v. 剥下，刮下<br>scratch /skrætʃ/ v. 抓，搔<br>shrink /ʃriŋk/ v. 收缩，减少<br>standardize /‘stændədaiz / v. 使标准化<br>steer /stiə(r)/ v. 驾驶，引导<br>strengthen /‘streŋθn/ v. 加强，使更强壮<br>stretch /stretʃ/ v. 伸展<br>subscribe /səb’skraib/ v. 预订，订阅<br>suck /sʌk/ v. (用嘴)吸，吞噬，卷入<br>suppress /sə’pres/ v. 镇压<br>sustain /sə’stein/ v. 承受，维持<br>tackle /‘tækl/ v. 解决，处理<br>tempt /tempt/ v. 引诱，劝诱<br>terminate /‘tɜ:mineit/ v. 终止，结束<br>transmit /træns’mit/ v. 传播，传递<br>verify /‘verifai/ v. 证实，证明<br>view /vju:/ v. 视为，看做<br>wreck /rek/ v. (船只)失事</p>
<h3 id="救命核心副词"><a href="#救命核心副词" class="headerlink" title="救命核心副词"></a>救命核心副词</h3><p>deliberately /di’libərətli/ adv. 故意地，深思熟虑地，审慎地<br>exclusively /ik’sklu:sivli/ adv. 唯一地，专有地，排外地<br>explicitly /ik’splisitli/ adv. 明确地<br>forcibly /‘fɔ:səbli/ adv. 强行地，有力地<br>formerly /‘fɔ:məli/ adv. 原先地，以前，从前<br>increasingly /in’kri:siŋli/ adv.越来越多地<br>inevitably /in’evitəbli/ adv.必然地，  不可避免地<br>intentionally /in’tenʃənəli/ adv.有意地，故意地<br>optimistically /ˌɒpti’mistikli/ adv. 乐观地<br>outwardly /‘aʊtwədli/ adv.表面上，外表上地<br>presumably /pri’zju:məbli/ adv.大概可能，据推测<br>simultaneously /ˌsiməl’teiniəsli/ adv.同时发生地<br>somewhat /‘sʌmwɒt/ adv. 颇为，稍稍，有几分<br>spontaneously /spɒn’teiniəsli/ adv.自发地，  自然产生<br>startlingly /‘stɑ:rtliŋli/ adv. 惊人地<br>triumphantly /trai’ʌmfəntli/ adv.(欣喜)胜利，成功地<br>unexpectedly /ˌʌnik’spektidli/ adv. 意外地<br>virtually /‘vɜ:tʃuəli/ adv. 事实上，实际地</p>
<h3 id="救命核心短语"><a href="#救命核心短语" class="headerlink" title="救命核心短语"></a>救命核心短语</h3><p>adhere to 忠于<br>after all 毕竟，归根结底<br>at random 随机地，任意地<br>break out 突然发生，爆发<br>break up 打碎<br>but for 要不是<br>by far 到目前为止；远，非常<br>by no means 决不，一点也不<br>catch on 理解，明白<br>catch up with 赶上<br>collide with 碰撞，冲突<br>come up with 想出，提出，追及，赶上<br>comment on 评论<br>contrary to 与……相反<br>contribute to 有助于，促成<br>cope with 应付，妥善处理<br>cut short 打断，制止<br>do away with 消灭，废除，去掉<br>do credit to 为……带来光荣<br>due to 因为<br>go in for 从事，致力于<br>go off 爆炸<br>hang by a thread 千钧一发，岌岌可危<br>heap praise upon 对……大加称赞<br>in accordance with 与……一致，按照，根据<br>in between 在两者之间<br>in case of 防备，以防<br>in honor of 为纪念<br>in response to 响应，反应<br>in terms of 根据，从……方面来说<br>in that 因为<br>in the vicinity of 在附近<br>keep off 远离，抑制<br>lay off (暂时)解雇<br>let alone 更不必说<br>look into 调查<br>look on 看待<br>lose no time 立即<br>make sense of sth. 讲得通言之有理<br>of no avail 无用，无效<br>on file 存档<br>on no account 决不，绝对不<br>on the decline 衰落中，衰退中<br>out of stock 无现货的，脱销的<br>provided that 假如，若是<br>pull up 使停下<br>put away 放好，放起来<br>regardless of 不管，不顾<br>result in 导致，结果是，发生<br>see to 照料，注意<br>show to 引导，引领<br>stand for 容忍，接受<br>take on 承担，接受<br>take over  接管，接收<br>take to 对…产生好感，开始喜欢<br>talk into 说服<br>that is 即，也就是<br>turn in 上交<br>turn out 生产出<br>turn to 求助于<br>ward off  防止，避开<br>with reference to 关于，有关<br>work out 想出，制订出<br>worth one’s while 值得花时间做</p>
<p>待续…</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;写作主题词&quot;&gt;&lt;a href=&quot;#写作主题词&quot; class=&quot;headerlink&quot; title=&quot;写作主题词&quot;&gt;&lt;/a&gt;写作主题词&lt;/h2&gt;&lt;p&gt;1.顺境与逆境( Favorable  Circumstances  and  adverse Circumstan
    
    </summary>
    
      <category term="英语" scheme="http://ynuwm.github.io/categories/%E8%8B%B1%E8%AF%AD/"/>
    
    
      <category term="六级" scheme="http://ynuwm.github.io/tags/%E5%85%AD%E7%BA%A7/"/>
    
  </entry>
  
  <entry>
    <title>把信送给加西亚</title>
    <link href="http://ynuwm.github.io/2017/11/24/%E6%8A%8A%E4%BF%A1%E9%80%81%E7%BB%99%E5%8A%A0%E8%A5%BF%E4%BA%9A/"/>
    <id>http://ynuwm.github.io/2017/11/24/把信送给加西亚/</id>
    <published>2017-11-24T12:25:17.000Z</published>
    <updated>2017-12-01T09:03:47.000Z</updated>
    
    <content type="html"><![CDATA[<p>在所有与古巴有关的事情中，有一个人常常令我无法忘怀。<br>美西战争爆发以后，美国必须马上与西班牙反抗军首领加西亚将军取得联系。加西亚将军隐藏在古巴辽阔的崇山峻岭中——没有人知道确切的地点，因而无法送信给他。但是，美国总统必须尽快地与他建立合作关系。怎么办呢？<br>有人对总统推荐说：“有一个名叫罗文的人，如果有人能找到加西亚将军，那个人一定就是他。”<br>于是，他们将罗文找来，交给他一封信——写给加西亚的信。关于那个名叫罗文的人，如何拿了信，将它装进一个油纸袋里，打封，吊在胸口藏好，如何在3个星期之后，徒步穿越一个危机四伏的国家，将信交到加西亚手上——这些细节都不是我想说明的，我要强调的重点是：<br>美国总统将一封写给加西亚的信交给了罗文，罗文接过信后，并没有问：“他在哪里？”</p>
<p>喜欢原文中这句话：年轻人所需要的不只是学习书本上的知识，也不只是聆听他人种种的指导，而是更需要一种敬业精神，对上级的托付，立即采取行动，全心全意去完成任务——把信送给加西亚。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在所有与古巴有关的事情中，有一个人常常令我无法忘怀。&lt;br&gt;美西战争爆发以后，美国必须马上与西班牙反抗军首领加西亚将军取得联系。加西亚将军隐藏在古巴辽阔的崇山峻岭中——没有人知道确切的地点，因而无法送信给他。但是，美国总统必须尽快地与他建立合作关系。怎么办呢？&lt;br&gt;有人对
    
    </summary>
    
      <category term="读书笔记" scheme="http://ynuwm.github.io/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="职场" scheme="http://ynuwm.github.io/tags/%E8%81%8C%E5%9C%BA/"/>
    
  </entry>
  
  <entry>
    <title>面试南航</title>
    <link href="http://ynuwm.github.io/2017/11/24/%E5%8D%97%E8%88%AA%E9%9D%A2%E8%AF%95%E7%BB%8F%E5%8E%86/"/>
    <id>http://ynuwm.github.io/2017/11/24/南航面试经历/</id>
    <published>2017-11-24T08:33:01.000Z</published>
    <updated>2018-05-31T13:53:41.000Z</updated>
    
    <content type="html"><![CDATA[<p>我是3,4月份投的简历，那个时候来软院宣讲，研究生很少，去的基本是软院大三的应届生，估计的话收的简历有三四十份。那次说的是要招暑假实习生，但是投完简历就没消息了。直到10月份，突然接到一个广州的电话，但是我没接到，打过去一直在忙，就没怎么管。十月份自己脚崴了，一直在休息，拄着拐杖。10月22号那周的周三也就是10月18号突然收到短信，说我的简历通过了，叫我参加笔试，叫我准备一下，考试时间就是10月22（周日），准备个屁啊，时间太紧我直接裸考的，上学期投完简历我是有买托业资料，做了一段时间没收到消息也就没做了。</p>
<p>说说笔试吧。21号的晚上就去了市区，拄着拐杖太不方便了，地上是湿的刚刚下完雨。在小菜园立交桥那个地方就迷路了，道路太复杂，以前又没怎么去过，边打听边走。走了一个多小时才找到住的地方，累的全身都出汗了，想着就去打打酱油吧，本来就没准备。第二天考试就下大雨了，比较幸运是等我到考点后下起来的。考试的话，30个人只去了13个。行测比较坑，上面显示的是38道题，等提交的时候发现后面还有题，大家好像都没做。英语很简单，偏应用，听力也简单，提前十多分钟做完了。考完直接回来了。然后就等通知。</p>
<p>10月30号收到面试通知说11月1号要过去面试，再次打电话确认我参加不，这次电话又没有接到，不过我打过去了说参加的。然后就是面试，我是6号，一共有9个通过了笔试，其中一个没来，也就是只剩8个，有两个女生报的行销，跟我们不是一个岗位，剩下6个全是男生都是信息开发岗。面试分两轮：第一轮小组讨论，全部6个人坐在一起，讨论15分钟，最后派一个代表总结。抽到的题目是做一个网页系统，同时要面对海量用户，要综合考虑数据的准确性和及时性，问要用到哪些技术。其实面试官这里是不太管你们讨论出什么结果的，他们看的是你的思路，你在小组里扮演的角色。我自己的话，就没说很多话，但是每次我说的都是他们讨论之外的东西，<br>他们讨论热烈的时候，我就说海量用户，那用户数据安全性怎么设计，并发执行的话我们有哪些要注意的等等。 然后小组派一个人总结了。<br>第二轮，多对一面试，有三个面试官，一个是技术，一个是行销的，另一个是其他的。在我前面面完的人，有一个出来我们就问他都问了哪些问题，他说数据结构什么的，最后他说还讲了毕业设计。我当时想这哥们应该没戏了，都研究生了还在讲毕业设计，不显得很low吗，而且面试你给人家讲毕设不觉得这个很普遍吗，研究生阶段很多东西可以讲啊，局限在毕设在我看来已经失败了。刚进去做自我介绍，问我的第一个问题是我的两篇论文，我讲了很多估计他们也不太懂，神经网络LSTM什么的。接着问我说进去之后要用Java我说学过，他说你现在Python用的多，这两个语言的区别，我又balala说了一堆。第三个问题我是想继续读还是出来工作，我想都没想说不想读书了<br>我要工作。好像就没问别的问题了，另外两个人也没有问，当时真的很慌，面试我的时间我觉得是最短的，而且另外两个都没问问题这就完了？然后就出来了。对了，其他7个人都穿正装了，就我没穿，短信上没有说，招聘官网说要穿，由于没有懒得借就没穿。对了笔试的时候碰到了同班同学，他们是最近一个月投的简历。</p>
<p>11月9号收到MAP职业性格测试。认识一起面试的两个，应该是所有面试的都收到了这个测试。</p>
<p>11月22收到Offer。那个说毕设的哥们跟我预测的一样，没收到短信。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我是3,4月份投的简历，那个时候来软院宣讲，研究生很少，去的基本是软院大三的应届生，估计的话收的简历有三四十份。那次说的是要招暑假实习生，但是投完简历就没消息了。直到10月份，突然接到一个广州的电话，但是我没接到，打过去一直在忙，就没怎么管。十月份自己脚崴了，一直在休息，拄
    
    </summary>
    
      <category term="日记" scheme="http://ynuwm.github.io/categories/%E6%97%A5%E8%AE%B0/"/>
    
    
      <category term="面试" scheme="http://ynuwm.github.io/tags/%E9%9D%A2%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>SemEval2017-Task4前几名思路与技巧</title>
    <link href="http://ynuwm.github.io/2017/11/18/SemEval2017%E5%89%8D%E5%87%A0%E5%90%8D%E6%80%9D%E8%B7%AF%E4%B8%8E%E6%8A%80%E5%B7%A7/"/>
    <id>http://ynuwm.github.io/2017/11/18/SemEval2017前几名思路与技巧/</id>
    <published>2017-11-18T07:14:50.000Z</published>
    <updated>2017-12-01T09:03:46.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="No-1-BB-twtr"><a href="#No-1-BB-twtr" class="headerlink" title="No.1 BB_twtr"></a>No.1 BB_twtr</h2><p>第一，预处理<br>url to ‘url’<br>emotions to ‘smile’,’sadness’…<br>‘sooooo’ to ‘soo’<br>lowercased</p>
<p>第二，100million unlabeled tweets 预训练词向量(Twitter API)</p>
<p>第三，模型(参考论文Ye Zhang and Byron Wallace,2015)<br>卷积核大小２,3,4，每种有２个，对同一个句子卷积，得到６个univariate vectors,然后concat在一起，再经全连接层和softmax层分类<br>LSTM 类似的处理<br>第四，数据<br>Task-A(49693　labeled)<br>Task-BD(30849)<br>Task-CE(18948)</p>
<p>第五，ensemble<br>10 CNNs and 10 LSTMs together through soft voting</p>
<h2 id="No-2-DataStories"><a href="#No-2-DataStories" class="headerlink" title="No.2 DataStories"></a>No.2 DataStories</h2><p>第一，自己写的文本分词器<br>第二，TaskA两层双向LSTM+Att(基于message)<br>第三，TaskBCDE句子和话题分别通过BiLSTM然后concat+att-context(基于话题)</p>
<h2 id="No-3-LIA"><a href="#No-3-LIA" class="headerlink" title="No.3 LIA"></a>No.3 LIA</h2><p>ensemble CNN 和 LSTM<br>第一，Word Embedding<br>Lexical embedding<br>Sentiment embeddings(Multitask-learning)<br>Sentiment embeddings(distant-supervision)<br>Sentiment embeddings(negative-sampling)</p>
<p>第二，句子层特征提取<br>Lexicons:MPQA+NRC<br>Emoticons:number of emoticons grouped in pos,neg,neu<br>All-caps:number of words in all-caps<br>Elongated units:words in which characters are repeated more than wtice(eg,looooool)<br>Punctuation:number of contiguous sequences of severl periods.exclaimation marks and question marks</p>
<h2 id="No-4-Senti17"><a href="#No-4-Senti17" class="headerlink" title="No.4 Senti17"></a>No.4 Senti17</h2><p>第一，HappyTokenizer 处理文本<br>第二，十个卷积网络投票，每个网络训练数据一样，词向量一样，不同的是初始权重</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;No-1-BB-twtr&quot;&gt;&lt;a href=&quot;#No-1-BB-twtr&quot; class=&quot;headerlink&quot; title=&quot;No.1 BB_twtr&quot;&gt;&lt;/a&gt;No.1 BB_twtr&lt;/h2&gt;&lt;p&gt;第一，预处理&lt;br&gt;url to ‘url’&lt;br&gt;emot
    
    </summary>
    
      <category term="深度学习" scheme="http://ynuwm.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="semeval" scheme="http://ynuwm.github.io/tags/semeval/"/>
    
  </entry>
  
  <entry>
    <title>一句喜欢的话</title>
    <link href="http://ynuwm.github.io/2017/11/17/%E4%B8%80%E5%8F%A5%E5%96%9C%E6%AC%A2%E7%9A%84%E8%AF%9D/"/>
    <id>http://ynuwm.github.io/2017/11/17/一句喜欢的话/</id>
    <published>2017-11-17T01:35:35.000Z</published>
    <updated>2017-12-09T01:42:57.000Z</updated>
    
    <content type="html"><![CDATA[<p>用简单的代码描绘丰富多彩的大学生活。</p>
<p>想起以前书里看过的一句话：<br>程序员是值得尊敬的。<br>程序员的双手是魔术师的双手。<br>他们把枯燥无味的代码变成了丰富多彩的软件。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;用简单的代码描绘丰富多彩的大学生活。&lt;/p&gt;
&lt;p&gt;想起以前书里看过的一句话：&lt;br&gt;程序员是值得尊敬的。&lt;br&gt;程序员的双手是魔术师的双手。&lt;br&gt;他们把枯燥无味的代码变成了丰富多彩的软件。&lt;/p&gt;

    
    </summary>
    
      <category term="日记" scheme="http://ynuwm.github.io/categories/%E6%97%A5%E8%AE%B0/"/>
    
    
      <category term="心情" scheme="http://ynuwm.github.io/tags/%E5%BF%83%E6%83%85/"/>
    
  </entry>
  
  <entry>
    <title>这几天的云大</title>
    <link href="http://ynuwm.github.io/2017/11/15/%E8%BF%99%E5%87%A0%E5%A4%A9%E7%9A%84%E4%BA%91%E5%A4%A7/"/>
    <id>http://ynuwm.github.io/2017/11/15/这几天的云大/</id>
    <published>2017-11-15T04:14:44.000Z</published>
    <updated>2017-12-04T12:37:08.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2017/11/15/这几天的云大/psb1.jpg" alt="img"><br><img src="/2017/11/15/这几天的云大/psb2.jpg" alt="img"><br><img src="/2017/11/15/这几天的云大/psb3.jpg" alt="img"><br><img src="/2017/11/15/这几天的云大/psb4.jpg" alt="img"><br><img src="/2017/11/15/这几天的云大/psb5.jpg" alt="img"><br><img src="/2017/11/15/这几天的云大/psb6.jpg" alt="img"> </p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/2017/11/15/这几天的云大/psb1.jpg&quot; alt=&quot;img&quot;&gt;&lt;br&gt;&lt;img src=&quot;/2017/11/15/这几天的云大/psb2.jpg&quot; alt=&quot;img&quot;&gt;&lt;br&gt;&lt;img src=&quot;/2017/11/15/这几天的云大/ps
    
    </summary>
    
      <category term="日记" scheme="http://ynuwm.github.io/categories/%E6%97%A5%E8%AE%B0/"/>
    
    
      <category term="照片" scheme="http://ynuwm.github.io/tags/%E7%85%A7%E7%89%87/"/>
    
      <category term="云大" scheme="http://ynuwm.github.io/tags/%E4%BA%91%E5%A4%A7/"/>
    
  </entry>
  
  <entry>
    <title>综述自然语言处理NLP</title>
    <link href="http://ynuwm.github.io/2017/11/15/%E7%BB%BC%E8%BF%B0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86NLP/"/>
    <id>http://ynuwm.github.io/2017/11/15/综述自然语言处理NLP/</id>
    <published>2017-11-15T02:42:33.000Z</published>
    <updated>2017-12-01T09:03:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>前言</p>
<p>自然语言处理是文本挖掘的研究领域之一，是人工智能和语言学领域的分支学科。在此领域中探讨如何处理及运用自然语言。</p>
<p>对于自然语言处理的发展历程，可以从哲学中的经验主义和理性主义说起。基于统计的自然语言处理是哲学中的经验主义，基于规则的自然语言处理是哲学中的理性主义。在哲学领域中经验主义与理性主义的斗争一直是此消彼长，这种矛盾与斗争也反映在具体科学上，如自然语言处理。</p>
<p>早期的自然语言处理具有鲜明的经验主义色彩。如 1913 年马尔科夫提出马尔科夫随机过程与马尔科夫模型的基础就是“手工查频”，具体说就是统计了《欧根·奥涅金》长诗中元音与辅音出现的频度；1948 年香农把离散马尔科夫的概率模型应用于语言的自动机，同时采用手工方法统计英语字母的频率。</p>
<p>然而这种经验主义到了乔姆斯基时出现了转变。</p>
<p>1956 年乔姆斯基借鉴香农的工作，把有限状态机用作刻画语法的工具，建立了自然语言的有限状态模型，具体来说就是用“代数”和“集合”将语言转化为符号序列，建立了一大堆有关语法的数学模型。这些工作非常伟大，为自然语言和形式语言找到了一种统一的数学描述理论，一个叫做“形式语言理论”的新领域诞生了。这个时代，“经验主义”被全盘否定，“理性主义”算是完胜。</p>
<p>然而在 20 世纪 50 年代末到 60 年代中期，经验主义东山再起了。多数学者普遍认为只有详尽的历史语料才能带来靠谱的结论。于是一些比较著名的理论与算法就诞生了，如贝叶斯方法（Bayesian Method）、隐马尔可夫、最大熵、Viterbi 算法、支持向量机之类。世界上第一个联机语料库也是在那个时候的 Brown University 诞生的。</p>
<p>但是总的来说，这个时代依然是基于规则的理性主义的天下，经验主义虽然取得了不俗的成就，却依然没有受到太大的重视。但是金子总会发光的。</p>
<p>90 年代以来，基于统计的自然语言处理就开始大放异彩了。首先是在机器翻译领域取得了突破，因为引入了许多基于语料库的方法（哈钦斯，英国著名学者）。1990 年在芬兰赫尔辛基举办的第 13 届国际计算语言学会议确定的主题是“处理大规模真实文本的理论、方法与工具”，大家的重心开始转向大规模真实文本了，传统的仅仅基于规则的自然语言处理显然力不从心了。学者们认为，大规模语料至少是对基于规则方法有效的补充。</p>
<p>到了 1994~1999 年，经验主义就开始空前繁荣了。如句法剖析、词类标注、参照消解、话语处理的算法几乎把“概率”与“数据”作为标准方法，成为了自然语言处理的主流。 </p>
<p>总之，理性主义在自然语言处理的发展史上是有重要地位的，也辉煌了几十年，历史事物常常是此消彼长的，至于谁好谁坏，不是固定的，取决于不同时代的不同历史任务。总的来说，基于规则的理性主义在这个时代被提及得比较少，用的也比较少，主要是由于以下几个缺陷：</p>
<p>• 鲁棒性差，过于严格的规则导致对非本质错误的零容忍（这一点在最近的一些新的剖析技术上有所改善）；</p>
<p>• 研究强度大，泛化能力差。一个研究要语言学家、语音学家和各种领域的专家配合，在当前大规模文本处理的时间、资源要求下太不划算。且机器学习的方法很难应用，难以普及；</p>
<p>• 实践性差。基于统计的经验主义方法可以根据数据集不断对参数进行优化，而基于规则的方法就不可以，这在当前数据量巨大的情况下，影响是致命的，因为前者常常可以通过增大训练集来获得更好的效果，后者则死板许多，结果往往不尽人意。</p>
<p>但理性主义还是有很多优点的，同样经验主义也有很多缺陷，算是各有所长、各有所短。不同学科有不同学科的研究角度，只能说某些角度在某个特定的历史时期对提高生产力“更有用”，所以重视的人更多。但“有用”不代表胜利，暂时的“无用”更不能说是科学层面上的“失败”。尤其是在当前中文自然语言处理发展还不甚成熟的时期，私以为基于统计的方法在很多方面并不完美，“理性主义”的作用空间还很大，需要更多的人去关注、助力。<br>——《统计自然语言处理》宗成庆</p>
<p>自然语言处理涉及的范畴如下（维基百科）：</p>
<p>• 中文自动分词（Chinese word segmentation）<br>• 词性标注（Part-of-speech tagging）<br>• 句法分析（Parsing）<br>• 自然语言生成（Natural language generation）<br>• 文本分类（Text categorization）<br>• 信息检索（Information retrieval）<br>• 信息抽取（Information extraction）<br>• 文字校对（Text-proofing）<br>• 问答系统（Question answering）<br>• 机器翻译（Machine translation）<br>• 自动摘要（Automatic summarization）</p>
<p>本文针对其中几个主要领域的研究现状和进展，通过论文、博客等资料，结合自身的学习和实践经历进行浅显地介绍。由于个人实践经验不足，除中文分词、自动文摘、文本分类、情感分析和话题模型方面进行过实际业务的实践，其他方面经验欠缺，若有不当之处，欢迎童鞋们批评指正！</p>
<p>目录</p>
<p><img src="/2017/11/15/综述自然语言处理NLP/000.png" alt="img"></p>
<p>一. 中文分词</p>
<p>中文分词主要包括词的歧义切分和未登录词识别，主要可以分为基于词典和基于统计的方法，最新的方法是多种方法的混合。从目前汉语分词研究的总体水平看，F1 值已经达到 95% 左右，主要分词错误是由新词造成的，尤其对领域的适应性较差。下面主要介绍一下中文分词存在的主要问题和分词方法。</p>
<ol>
<li>问题</li>
</ol>
<p>1.1 歧义切分</p>
<p>切分歧义处理包括两部分内容：</p>
<p>• 切分歧义的检测；</p>
<p>• 切分歧义的消解。</p>
<p>这两部分在逻辑关系上可分成两个相对独立的步骤。</p>
<p>• 切分歧义的检测。“最大匹配法”（精确的说法应该叫“最长词优先匹配法”） 是最早出现、同时也是最基本的汉语自动分词方法。依扫描句子的方向，又分正向最大匹配 MM（从左向右）和逆向最大匹配 RMM（从右向左）两种。</p>
<p>最大匹配法实际上将切分歧义检测与消解这两个过程合二为一，对输入句子给出唯一的切分可能性，并以之为解。从最大匹配法出发导出了“双向最大匹配法”，即 MM＋ RMM。双向最大匹配法存在着切分歧义检测盲区。</p>
<p>针对切分歧义检测，另外两个有价值的工作是“最少分词法”，这种方法歧义检测能力较双向最大匹配法要强些，产生的可能切分个数仅略有增加；和“全切分法”，这种方法穷举所有可能的切分，实现了无盲区的切分歧义检测，但代价是导致大量的切分“垃圾”。</p>
<p>• 切分歧义的消解。典型的方法包括句法统计和基于记忆的模型。句法统计将自动分词和基于 Markov 链的词性自动标注技术结合起来，利用从人工标注语料库中提取出的词性二元统计规律来消解切分歧义，基于记忆的模型对伪歧义型高频交集型歧义切分，可以把它们的正确（唯一）切分形式预先记录在一张表中，其歧义消解通过直接查表即可实现。</p>
<p>1.2 未登录词识别</p>
<p>未登录词大致包含两大类：</p>
<p>• 新涌现的通用词或专业术语等；</p>
<p>• 专有名词。如中国人名、外国译名、地名、机构名（泛指机关、团体和其它企事业单位）等。</p>
<p>前一种未登录词理论上是可预期的，能够人工预先添加到词表中（但这也只是理想状态，在真实环境下并不易做到）；后一种未登录词则完全不可预期，无论词表多么庞大，也无法囊括。</p>
<p>真实文本中（即便是大众通用领域），未登录词对分词精度的影响超过了歧义切分。未登录词处理在实用型分词系统中占的份量举足轻重。</p>
<p>• 新涌现的通用词或专业术语。对这类未登录词的处理，一般是在大规模语料库的支持下，先由机器根据某种算法自动生成一张候选词表（无监督的机器学习策略），再人工筛选出其中的新词并补充到词表中。</p>
<p>鉴于经过精加工的千万字、甚至亿字级的汉语分词语料库目前还是水月镜花，所以这个方向上现有的研究无一不以从极大规模生语料库中提炼出的 n 元汉字串之分布（n≥2）为基础。其中汉字之间的结合力通过全局统计量包括互信息、t- 测试差、卡方统计量、字串频等来表示。</p>
<p>• 专有名词。对专有名词的未登录词的处理，首先依据从各类专有名词库中总结出的统计知识 （如姓氏用字及其频度）和人工归纳出的专有名词的某些结构规则，在输入句子中猜测可能成为专有名词的汉字串并给出其置信度，之后利用对该类专有名词有标识意义的紧邻上下文信息（如称谓），以及全局统计量和局部统计量（局部统计量是相对全局统计量而言的，是指从当前文章得到且其有效范围一般仅限于该文章的统计量，通常为字串频），进行进一步的鉴定。</p>
<p>已有的工作涉及了四种常见的专有名词：中国人名的识别、外国译名的识别、中国地名的识别及机构名的识别。</p>
<p>从各家报告的实验结果来看，外国译名的识别效果最好，中国人名次之，中国地名再次之，机构名最差。而任务本身的难度实质上也是遵循这个顺序由小增大。 沈达阳、孙茂松等（1997b）特别强调了局部统计量在未登录词处理中的价值。</p>
<ol>
<li>方法</li>
</ol>
<p>2.1 基于词典的方法</p>
<p>在基于词典的方法中，对于给定的词，只有词典中存在的词语能够被识别，其中最受欢迎的方法是最大匹配法（MM），这种方法的效果取决于词典的覆盖度，因此随着新词不断出现，这种方法存在明显的缺点。</p>
<p>2.2 基于统计的方法</p>
<p>基于统计的方法由于使用了概率或评分机制而非词典对文本进行分词而被广泛应用。这种方法主要有三个缺点：</p>
<p>一是这种方法只能识别 OOV（out-of-vocabulary）词而不能识别词的类型，比如只能识别为一串字符串而不能识别出是人名；二是统计方法很难将语言知识融入分词系统，因此对于不符合语言规范的结果需要额外的人工解析；三是在许多现在分词系统中，OOV 词识别通常独立于分词过程。</p>
<p>二. 词性标注</p>
<p>词性标注是指为给定句子中的每个词赋予正确的词法标记，给定一个切好词的句子，词性标注的目的是为每一个词赋予一个类别，这个类别称为词性标记（part-of-speech tag），比如，名词（noun）、动词（verb）、形容词（adjective）等。</p>
<p>它是自然语言处理中重要的和基础的研究课题之一，也是其他许多智能信息处理技术的基础，已被广泛的应用于机器翻译、文字识别、语音识别和信息检索等领域。</p>
<p>词性标注对于后续的自然语言处理工作是一个非常有用的预处理过程，它的准确程度将直接影响到后续的一系列分析处理任务的效果。 </p>
<p>长期以来，兼类词的词性歧义消解和未知词的词性识别一直是词性标注领域需要解决的热点问题。当兼类词的词性歧义消解变得困难时，词性的标注就出现了不确定性的问题。而对那些超出了词典收录范围的词语或者新涌现的词语的词性推测，也是一个完整的标注系统所应具备的能力。</p>
<ol>
<li>词性标注方法</li>
</ol>
<p>词性标注是一个非常典型的序列标注问题。最初采用的方法是隐马尔科夫生成式模型， 然后是判别式的最大熵模型、支持向量机模型，目前学术界通常采用结构感知器模型和条件随机场模型。</p>
<p>近年来，随着深度学习技术的发展，研究者们也提出了很多有效的基于深层神经网络的词性标注方法。</p>
<p>迄今为止，词性标注主要分为基于规则的和基于统计的方法。</p>
<p>• 规则方法能准确地描述词性搭配之间的确定现象，但是规则的语言覆盖面有限，庞大的规则库的编写和维护工作则显得过于繁重，并且规则之间的优先级和冲突问题也不容易得到满意的解决。</p>
<p>• 统计方法从宏观上考虑了词性之间的依存关系，可以覆盖大部分的语言现象，整体上具有较高的正确率和稳定性，不过其对词性搭配确定现象的描述精度却不如规则方法。</p>
<p>针对这样的情况，如何更好地结合利用统计方法和规则处理手段，使词性标注任务既能够有效地利用语言学家总结的语言规则，又可以充分地发挥统计处理的优势成为了词性标注研究的焦点。</p>
<ol>
<li>词性标注研究进展</li>
</ol>
<p>• 词性标注和句法分析联合建模：研究者们发现，由于词性标注和句法分析紧密相关，词性标注和句法分析联合建模可以同时显著提高两个任务准确率。</p>
<p>• 异构数据融合：汉语数据目前存在多个人工标注数据，然而不同数据遵守不同的标注规范，因此称为多源异构数据。近年来，学者们就如何利用多源异构数据提高模型准确率，提出了很多有效的方法，如基于指导特征的方法、基于双序列标注的方法、以及基于神经网络共享表示的方法。</p>
<p>• 基于深度学习的方法：传统词性标注方法的特征抽取过程主要是将固定上下文窗口的词进行人工组合，而深度学习方法能够自动利用非线性激活函数完成这一目标。进一步，如果结合循环神经网络如双向 LSTM，则抽取到的信息不再受到固定窗口的约束，而是考虑整个句子。</p>
<p>除此之外，深度学习的另一个优势是初始词向量输入本身已经刻画了词语之间的相似度信息，这对词性标注非常重要。</p>
<p>三. 句法分析</p>
<p>语言语法的研究有非常悠久的历史，可以追溯到公元前语言学家的研究。不同类型的句法分析体现在句法结构的表示形式不同，实现过程的复杂程度也有所不同。因此，科研人员采用不同的方法构建符合各个语法特点的句法分析系统。其主要分类如下图所示：</p>
<p><img src="/2017/11/15/综述自然语言处理NLP/00１.png" alt="img"></p>
<p>下文主要对句法分析技术方法和研究现状进行总结分析：</p>
<ol>
<li>依存句法分析</li>
</ol>
<p>依存语法存在一个共同的基本假设：句法结构本质上包含词和词之间的依存（修饰）关系。一个依存关系连接两个词，分别是核心词（head）和依存词（dependent）。依存关系可以细分为不同的类型，表示两个词之间的具体句法关系。</p>
<p>目前研究主要集中在数据驱动的依存句法分析方法，即在训练实例集合上学习得到依存句法分析器，而不涉及依存语法理论的研究。数据驱动的方法的主要优势在于给定较大规模的训练数据，不需要过多的人工干预，就可以得到比较好的模型。因此，这类方法很容易应用到新领域和新语言环境。</p>
<p>数据驱动的依存句法分析方法主要有两种主流方法：基于图（ graph-based）的分析方法和基于转移（ transition-based）的分析方法。</p>
<p>2.1 基于图的依存句法分析方法</p>
<p>基于图的方法将依存句法分析问题看成从完全有向图中寻找最大生成树的问题。一棵依存树的分值由构成依存树的几种子树的分值累加得到。</p>
<p>根据依存树分值中包含的子树的复杂度，基于图的依存分析模型可以简单区分为一阶和高阶模型。高阶模型可以使用更加复杂的子树特征，因此分析准确率更高，但是解码算法的效率也会下降。</p>
<p>基于图的方法通常采用基于动态规划的解码算法，也有一些学者采用柱搜索（beam search）来提高效率。学习特征权重时，通常采用在线训练算法，如平均感知器（averaged perceptron）。</p>
<p>2.2 基于转移的依存句法分析方法</p>
<p>基于转移的方法将依存树的构成过程建模为一个动作序列，将依存分析问题转化为寻找最优动作序列的问题。早期，研究者们使用局部分类器（如支持向量机等）决定下一个动作。近年来，研究者们采用全局线性模型来决定下一个动作，一个依存树的分值由其对应的动作序列中每一个动作的分值累加得到。</p>
<p>特征表示方面，基于转移的方法可以充分利用已形成的子树信息，从而形成丰富的特征，以指导模型决策下一个动作。模型通过贪心搜索或者柱搜索等解码算法找到近似最优的依存树。和基于图的方法类似，基于转移的方法通常也采用在线训练算法学习特征权重。</p>
<p>2.3 多模型融合的依存句法分析方法</p>
<p>基于图和基于转移的方法从不同的角度解决问题，各有优势。基于图的模型进行全局搜索但只能利用有限的子树特征，而基于转移的模型搜索空间有限但可以充分利用已构成的子树信息构成丰富的特征。详细比较发现，这两种方法存在不同的错误分布。</p>
<p>因此，研究者们使用不同的方法融合两种模型的优势，常见的方法有：stacked learning；对多个模型的结果加权后重新解码（re-parsing）；从训练语料中多次抽样训练多个模型（bagging）。 </p>
<ol>
<li>短语结构句法分析</li>
</ol>
<p>分词，词性标注技术一般只需对句子的局部范围进行分析处理，目前已经基本成熟，其标志就是它们已经被成功地用于文本检索、文本分类、信息抽取等应用之中，而句法分析、语义分析技术需要对句子进行全局分析，目前，深层的语言分析技术还没有达到完全实用的程度。</p>
<p>短语结构句法分析的研究基于上下文无关文法（Context Free Grammar，CFG）。上下文无关文法可以定义为四元组，其中 T 表示终结符的集合（即词的集合），N 表示非终结符的集合（即文法标注和词性标记的集合），S 表示充当句法树根节点的特殊非终结符，而 R 表示文法规则的集合，其中每条文法规则可以表示为 Ni®g ，这里的 g 表示由非终结符与终结符组成的一个序列（允许为空）。</p>
<p>根据文法规则的来源不同，句法分析器的构建方法总体来说可以分为两大类：</p>
<p>• 人工书写规则</p>
<p>• 从数据中自动学习规则</p>
<p>人工书写规则受限于规则集合的规模：随着书写的规则数量的增多，规则与规则之间的冲突加剧，从而导致继续添加规则变得困难。</p>
<p>与人工书写规模相比，自动学习规则的方法由于开发周期短和系统健壮性强等特点，加上大规模人工标注数据，比如宾州大学的多语种树库的推动作用，已经成为句法分析中的主流方法。</p>
<p>而数据驱动的方法又推动了统计方法在句法分析领域中的大量应用。为了在句法分析中引入统计信息，需要将上下文无关文法扩展成为概率上下文无关文法（Probabilistic Context Free Grammar，PCFG），即为每条文法规则指定概率值。</p>
<p>概率上下文无关文法与非概率化的上下文无关文法相同，仍然表示为四元组，区别在于概率上下文无关文法中的文法规则必须带有概率值。</p>
<p>获得概率上下文无关文法的最简单的方法是直接从树库中读取规则，利用最大似然估计（Maximum Likelihood Estimation，MLE）计算得到每条规则的概率值。使用该方法得到的文法可以称为简单概率上下文无关文法。在解码阶段，CKY 10 等解码算法就可以利用学习得到的概率上下文无关文法搜索最优句法树。</p>
<p>虽然基于简单概率上下文无关文法的句法分析器的实现比较简单，但是这类分析器的性能并不能让人满意。</p>
<p>性能不佳的主要原因在于上下文无关文法采取的独立性假设过强：一条文法规则的选择只与该规则左侧的非终结符有关，而与任何其它上下文信息无关。文法中缺乏其它信息用于规则选择的消歧。因此后继研究工作的出发点大都基于如何弱化上下文无关文法中的隐含独立性假设。</p>
<ol>
<li>总结</li>
</ol>
<p>分词，词性标注技术一般只需对句子的局部范围进行分析处理，目前已经基本成熟，其标志就是它们已经被成功地用于文本检索、文本分类、信息抽取等应用之中，而句法分析、语义分析技术需要对句子进行全局分析，目前，深层的语言分析技术还没有达到完全实用的程度。</p>
<p>四. 文本分类</p>
<p>文本分类是文本挖掘的核心任务，一直以来倍受学术界和工业界的关注。文本分类（Text Classification）的任务是根据给定文档的内容或主题，自动分配预先定义的类别标签。</p>
<p>对文档进行分类，一般需要经过两个步骤：</p>
<p>• 文本表示</p>
<p>• 学习分类</p>
<p>文本表示是指将无结构化的文本内容转化成结构化的特征向量形式，作为分类模型的输入。在得到文本对应的特征向量后，就可以采用各种分类或聚类模型，根据特征向量训练分类器或进行聚类。因此，文本分类或聚类的主要研究任务和相应关键科学问题如下：</p>
<ol>
<li>任务</li>
</ol>
<p>1.1 构建文本特征向量</p>
<p>构建文本特征向量的目的是将计算机无法处理的无结构文本内容转换为计算机能够处理的特征向量形式。文本内容特征向量构建是决定文本分类和聚类性能的重要环节。</p>
<p>为了根据文本内容生成特征向量，需要首先建立特征空间。其中典型代表是文本词袋（Bag of Words）模型，每个文档被表示为一个特征向量，其特征向量每一维代表一个词项。所有词项构成的向量长度一般可以达到几万甚至几百万的量级。</p>
<p>这样高维的特征向量表示如果包含大量冗余噪音，会影响后续分类聚类模型的计算效率和效果。</p>
<p>因此，我们往往需要进行特征选择（Feature Selection）与特征提取（Feature Extraction），选取最具有区分性和表达能力的特征建立特征空间，实现特征空间降维；或者，进行特征转换（Feature Transformation），将高维特征向量映射到低维向量空间。特征选择、提取或转换是构建有效文本特征向量的关键问题。</p>
<p>1.2 建立分类或聚类模型</p>
<p>在得到文本特征向量后，我们需要构建分类或聚类模型，根据文本特征向量进行分类或聚类。</p>
<p>其中，分类模型旨在学习特征向量与分类标签之间的关联关系，获得最佳的分类效果； 而聚类模型旨在根据特征向量计算文本之间语义相似度，将文本集合划分为若干子集。 分类和聚类是机器学习领域的经典研究问题。</p>
<p>我们一般可以直接使用经典的模型或算法解决文本分类或聚类问题。例如，对于文本分类，我们可以选用朴素贝叶斯、决策树、k-NN、逻辑回归（Logistic Regression）、支持向量机（Support Vector Machine, SVM）等分类模型。 </p>
<p>对于文本聚类，我们可以选用 k-means、层次聚类或谱聚类（spectral clustering）等聚类算法。 这些模型算法适用于不同类型的数据而不仅限于文本数据。</p>
<p>但是，文本分类或聚类会面临许多独特的问题，例如，如何充分利用大量无标注的文本数据，如何实现面向文本的在线分类或聚类模型，如何应对短文本带来的表示稀疏问题，如何实现大规模带层次分类体系的分类功能，如何充分利用文本的序列信息和句法语义信息，如何充分利用外部语言知识库信息，等等。这些问题都是构建文本分类和聚类模型所面临的关键问题。</p>
<ol>
<li>模型</li>
</ol>
<p>2.1 文本分类模型</p>
<p>近年来，文本分类模型研究层出不穷，特别是随着深度学习的发展，深度神经网络模型 也在文本分类任务上取得了巨大进展。我们将文本分类模型划分为以下三类：</p>
<p>• 基于规则的分类模型</p>
<p>基于规则的分类模型旨在建立一个规则集合来对数据类别进行判断。这些规则可以从训练样本里自动产生，也可以人工定义。给定一个测试样例，我们可以通过判断它是否满足某 些规则的条件，来决定其是否属于该条规则对应的类别。</p>
<p>典型的基于规则的分类模型包括决策树（Decision Tree）、随机森林（Random Forest）、 RIPPER 算法等。</p>
<p>• 基于机器学习的分类模型</p>
<p>典型的机器学习分类模型包括贝叶斯分类器（Naïve Bayes）、线性分类器（逻辑回归）、 支持向量机（Support Vector Machine, SVM）、最大熵分类器等。</p>
<p>SVM 是这些分类模型中比较有效、使用较为广泛的分类模型。它能够有效克服样本分布不均匀、特征冗余以及过拟合等问题，被广泛应用于不同的分类任务与场景。通过引入核函数，SVM 还能够解决原始特征空间线性不可分的问题。</p>
<p>除了上述单分类模型，以 Boosting 为代表的分类模型组合方法能够有效地综合多个弱分类模型的分类能力。在给定训练数据集合上同时训练这些弱分类模型，然后通过投票等机制综合多个分类器的预测结果，能够为测试样例预测更准确的类别标签。</p>
<p>• 基于神经网络的方法</p>
<p>以人工神经网络为代表的深度学习技术已经在计算机视觉、语音识别等领域取得了巨大成功，在自然语言处理领域，利用神经网络对自然语言文本信息进行特征学习和文本分类，也成为文本分类的前沿技术。</p>
<p>前向神经网络：多层感知机（Multilayer Perceptron, MLP）是一种典型的前向神经网络。它能够自动学习多层神经网络，将输入特征向量映射到对应的类别标签上。</p>
<p>通过引入非线性激活层，该模型能够实现非线性的分类判别式。包括多层感知机在内的文本分类模型均使用了词袋模型假设，忽略了文本中词序和结构化信息。对于多层感知机模型来说，高质量的初始特征表示是实现有效分类模型的必要条件。</p>
<p>为了更加充分地考虑文本词序信息，利用神经网络自动特征学习的特点，研究者后续提出了卷积神经网络（Convolutional Neural Network, CNN）和循环神经网络（Recurrent Neural Network, RNN）进行文本分类。</p>
<p>基于 CNN 和 RNN 的文本分类模型输入均为原始的词序列，输出为该文本在所有类别上的概率分布。这里，词序列中的每个词项均以词向量的形式作为输入。</p>
<p>卷积神经网络（CNN）：卷积神经网络文本分类模型的主要思想是，对词向量形式的文本输入进行卷积操作。CNN 最初被用于处理图像数据。与图像处理中选取二维域进行卷积操作不同，面向文本的卷积操作是针对固定滑动窗口内的词项进行的。</p>
<p>经过卷积层、 池化层和非线性转换层后，CNN 可以得到文本特征向量用于分类学习。CNN 的优势在于在计算文本特征向量过程中有效保留有用的词序信息。</p>
<p>针对 CNN 文本分类模型还有许多改进工作， 如基于字符级 CNN 的文本分类模型、将词位置信息加入到词向量。</p>
<p>循环神经网络（RNN）：循环神经网络将文本作为字符或词语序列{x0 , … , xN}，对于第 t 时刻输入的字符或词语 xt，都会对应产生新的低维特征向量 st。如图 3 所示，st 的取值会受到 xt 和上个时刻特征向量 st-1 的共同影响，st 包含了文本序列从 x0 到 xt 的语义信息。因此，我们可以利用 sN 作为该文本序列的特征向量，进行文本分类学习。</p>
<p>与 CNN 相比，RNN 能够更自然地考虑文本的词序信息，是近年来进行文本表示最流行的方案之一。</p>
<p>为了提升 RNN 对文本序列的语义表示能力，研究者提出很多扩展模型。</p>
<p>例如，长短时记忆网络（LSTM）提出记忆单元结构，能够更好地处理文本序列中的长程依赖，克服循环神经网络梯度消失问题。如图 4 是 LSTM 单元示意图，其中引入了三个门（input gate, output gate, forget gate）来控制是否输入输出以及记忆单元更新。</p>
<p>提升 RNN 对文本序列的语义表示能力的另外一种重要方案是引入选择注意力机制 (Selective Attention)，可以让模型根据具体任务需求对文本序列中的词语给予不同的关注度。</p>
<ol>
<li>应用</li>
</ol>
<p>文本分类技术在智能信息处理服务中有着广泛的应用。例如，大部分在线新闻门户网站（如新浪、搜狐、腾讯等）每天都会产生大量新闻文章，如果对这些新闻进行人工整理非常耗时耗力，而自动对这些新闻进行分类，将为新闻归类以及后续的个性化推荐等都提供巨大帮助。</p>
<p>互联网还有大量网页、论文、专利和电子图书等文本数据，对其中文本内容进行分类，是实现对这些内容快速浏览与检索的重要基础。此外，许多自然语言分析任务如观点挖掘、垃圾邮件检测等，也都可以看作文本分类或聚类技术的具体应用。</p>
<p>对文档进行分类，一般需要经过两个步骤：（1）文本表示，以及（2）学习。文本表示是指将无结构化的文本内容转化成结构化的特征向量形式，作为分类模型的输入。在得到文本对应的特征向量后，就可以采用各种分类或聚类模型，根据特征向量训练分类器</p>
<p>五. 信息检索</p>
<p>信息检索（Information Retrieval, IR）是指将信息按一定的方式加以组织，并通过信息查找满足用户的信息需求的过程和技术。</p>
<p>1951 年，Calvin Mooers 首次提出了“信息检索”的概念，并给出了信息检索的主要任务：协助信息的潜在用户将信息需求转换为一张文献来源列表，而这些文献包含有对其有用的信息。</p>
<p>信息检索学科真正取得长足发展是在计算机诞生并得到广泛应用之后，文献数字化使得信息的大规模共享及保存成为现实，而检索就成为了信息管理与应用中必不可少的环节。</p>
<p>互联网的出现和计算机硬件水平的提高使得人们存储和处理信息的能力得到巨大的提高，从而加速了信息检索研究的进步，并使其研究对象从图书资料和商用数据扩展到人们生活的方方面面。</p>
<p>伴随着互联网及网络信息环境的迅速发展，以网络信息资源为主要组织对象的信息检索系统：搜索引擎应运而生，成为了信息化社会重要的基础设施。</p>
<p>2016 年初，中文搜索引擎用户数达到 5.66 亿人，这充分说明搜索引擎在应用层次取得的巨大成功，也使得信息检索，尤其是网络搜索技术的研究具有了重要的政治、经济和社会价值。</p>
<ol>
<li>内容结构</li>
</ol>
<p>检索用户、信息资源和检索系统三个主要环节组成了信息检索应用环境下知识获取与信息传递的完整结构，而当前影响信息获取效率的因素也主要体现在这几个环节，即：</p>
<p>• 检索用户的意图表达</p>
<p>• 信息资源（尤其是网络信息资源）的质量度量</p>
<p>• 需求与资源的合理匹配</p>
<p>具体而言，用户有限的认知能力导致其知识结构相对大数据时代的信息环境而言往往存在缺陷，进而影响信息需求的合理组织和清晰表述；数据资源的规模繁杂而缺乏管理，在互联网“注意力经济”盛行的环境下，不可避免地存在欺诈作弊行为，导致检索系统难以准确感知其质量；用户与资源提供者的知识结构与背景不同，对于相同或者相似事物的描述往往存在较大差异，使得检索系统传统的内容匹配技术难以很好应对，无法准确度量资源与需求的匹配程度。</p>
<p>上述技术挑战互相交织，本质上反映了用户个体有限的认知能力与包含近乎无限信息的数据资源空间之间的不匹配问题。</p>
<p>概括地讲，当前信息检索的研究包括如下四个方面的研究内容及相应的关键科学问题：</p>
<p>1.1 信息需求理解</p>
<p>面对复杂的泛在网络空间，用户有可能无法准确表达搜索意图；即使能够准确表达，搜索引擎也可能难以正确理解；即使能够正确理解，也难以与恰当的网络资源进行匹配。这使得信息需求理解成为了影响检索性能提高的制约因素，也构成了检索技术发展面临的第一个关键问题。</p>
<p>1.2 资源质量度量</p>
<p>资源质量管理与度量在传统信息检索研究中并非处于首要的位置，但随着互联网信息资源逐渐成为检索系统的主要查找对象，网络资源特有的缺乏编审过程、内容重复度高、质量参差不齐等问题成为了影响检索质量的重要因素。</p>
<p>目前，搜索引擎仍旧面临着如何进行有效的资源质量度量的挑战，这构成了当前信息检索技术发展面临的第二个关键问题。</p>
<p>1.3 结果匹配排序</p>
<p>近年来，随着网络技术的进步，信息检索系统（尤其是搜索引擎）涉及的数据对象相应 的变得多样化、异质化，这也造成了传统的以文本内容匹配为主要手段的结果排序方法面临着巨大的挑战。</p>
<p>高度动态繁杂的泛在网络内容使得文本相似度计算方法无法适用；整合复杂异构网络资源作为结果使得基于同质性假设构建的用户行为模型难以应对；多模态的交互方式则使得传统的基于单一维度的结果分布规律的用户行为假设大量失效。</p>
<p>因此，在大数据时代信息进一步多样化、异质化的背景下，迫切需要构建适应现代信息资源环境的检索结果匹配排序方法，这是当前信息检索技术发展面临的第三个关键问题。</p>
<p>1.4 信息检索评价</p>
<p>信息检索评价是信息检索和信息获取领域研究的核心问题之一。信息检索和信息获取系统核心的目标是帮助用户获取到满足他们需求的信息，而评价系统的作用是帮助和监督研究开发人员向这一核心目标前进，以逐步开发出更好的系统，进而缩小系统反馈和用户需求之间的差距，提高用户满意度。</p>
<p>因此，如何设计合理的评价框架、评价手段、评价指标，是当前信息检索技术发展面临的第四个关键问题。</p>
<ol>
<li>个性化搜索</li>
</ol>
<p>现有的主要个性化搜索算法可分为基于内容分析的算法、基于链接分析的方法和基于协作过滤的算法。</p>
<p>• 基于内容的个性化搜索算法通过比较用户兴趣爱好和结果文档的内容相似性来对文档的用户相关性进行判断进而对搜索结果进行重排。</p>
<p>用户模型一般表述为关键词或主题向量或层次的形式。个性化算法通过比较用户模型和文档的相似性，判断真实的搜索意图，并估计文档对用户需求的匹配程度。</p>
<p>• 基于链接分析的方法主要是利用互联网上网页之间的链接关系，并假设用户点击和访问过的网页为用户感兴趣的网页，通过链接分析算法进行迭代最终计算出用户对每个网页的喜好度。</p>
<p>• 基于协作过滤的个性化搜索算法主要借鉴了基于协作过滤的推荐系统的思想，这种方法考虑到能够收集到的用户的个人信息有限，因此它不仅仅利用用户个人的信息，还利用与用户相似的其它用户或群组的信息，并基于用户群组和相似用户的兴趣偏好来个性化当前用户的搜索结果。用户之间的相似性可以通过用户的兴趣爱好、历史查询、点击过的网页等内容计算得出。</p>
<ol>
<li>语义搜索技术</li>
</ol>
<p>随着互联网信息的爆炸式增长，传统的以关键字匹配为基础的搜索引擎，已越来越难以满足用户快速查找信息的需求。同时由于没有知识引导及对网页内容的深入整理，传统网页搜索返回的网页结果也不能精准给出所需信息。</p>
<p>针对这些问题，以知识图谱为代表的语义搜索（Semantic Search）将语义 Web 技术和传统的搜索引擎技术结合，是一个很有研究价值 但还处于初期阶段的课题。</p>
<p>在未来的一段时间，结合互联网应用需求的实际和技术、产品运营能力的实际发展水平，语义搜索技术的发展重点将有可能集中在以各种情境的垂直搜索资源为基础，知识化推理为检索运行方式，自然语言多媒体交互为手段的智能化搜索与推荐技术。</p>
<p>首先将包括各类垂直搜索资源在内的深度万维网数据源整合成为提供搜索服务的资源池；随后利用广泛分布在公众终端计算设备上的浏览器作为客户端载体，通过构建的复杂情境知识库来开发多层次查询技术，并以此管理、调度、整合搜索云端的搜索服务资源，满足用户的多样化、多模态查询需求；最后基于面向情境体验的用户行为模型构建，以多模态信息推荐的形式实现对用户信息需求的主动满足。</p>
<p>六. 信息抽取</p>
<p>信息抽取（Information Extraction）是指从非结构化/半结构化文本（如网页、新闻、 论文文献、微博等）中提取指定类型的信息（如实体、属性、关系、事件、商品记录等）， 并通过信息归并、冗余消除和冲突消解等手段将非结构化文本转换为结构化信息的一项综合技术。例如:</p>
<p>• 从相关新闻报道中抽取出恐怖事件信息：时间、地点、袭击者、受害人、袭击 目标、后果等；</p>
<p>• 从体育新闻中抽取体育赛事信息：主队、客队、赛场、比分等；</p>
<p>• 从论文和医疗文献中抽取疾病信息：病因、病原、症状、药物等</p>
<p>被抽取出来的信息通常以结构化的形式描述，可以为计算机直接处理，从而实现对海量非结构化数据的分析、组织、管理、计算、 查询和推理，并进一步为更高层面的应用和任务（如自然语言理解、知识库构建、智能问答系统、舆情分析系统）提供支撑。</p>
<p>目前信息抽取已被广泛应用于舆情监控、网络搜索、智能问答等多个重要领域。与此同时，信息抽取技术是中文信息处理和人工智能的核心技术，具有重要的科学意义。</p>
<p>一直以来，人工智能的关键核心部件之一是构建可支撑类人推理和自然语言理解的大规模常识知识库。然而，由于人类知识的复杂性、开放性、多样性和巨大的规模，目前仍然无法构建满足上述需求的大规模知识库。</p>
<p>信息抽取技术通过结构化自然语言表述的语义知识，并整合来自海量文本中的不同语义知识，是构建大规模知识库最有效的技术之一。</p>
<p>每一段文本内所包含的寓意可以描述为其中的一组实体以及这些实体相互之间的关联和交互，因此抽取文本中的实体和它们之间的语义关系也就成为了理解文本意义的基础。</p>
<p>信息抽取可以通过抽取实体和实体之间的语义关系，表示这些语义关系承载的信息，并基于这些信息进行计算和推理来有效的理解一段文本所承载的语义。</p>
<ol>
<li>命名实体识别</li>
</ol>
<p>命名实体识别的目的是识别文本中指定类别的实体，主要包括人名、地名、机构名、专有名词等的任务。</p>
<p>命名实体识别系统通常包含两个部分：实体边界识别和实体分类。</p>
<p>其中实体边界识别判断一个字符串是否是一个实体，而实体分类将识别出的实体划分到预先给定的不同类别中去。</p>
<p>命名实体识别是一项极具实用价值的技术，目前中英文上通用命名实体识别（人名、地名、机构名）的 F1 值都能达到 90% 以上。命名实体识别的主要难点在于表达不规律、且缺乏训练语料的开放域命名实体类别（如电影、歌曲名）等。</p>
<ol>
<li>关系抽取</li>
</ol>
<p>关系抽取指的是检测和识别文本中实体之间的语义关系，并将表示同一语义关系的提及（mention）链接起来的任务。关系抽取的输出通常是一个三元组（实体 1，关系类别，实体 2），表示实体 1 和实体 2 之间存在特定类别的语义关系。</p>
<p>例如，句子“北京是中国的首都、政治中心和文化中心”中表述的关系可以表示为（中国，首都，北京），（中国，政治中心，北京）和（中国，文化中心，北京）。语义关系类别可以预先给定（如 ACE 评测中的七大类关系），也可以按需自动发现（开放域信息抽取）。</p>
<p>关系抽取通常包含两个核心模块：关系检测和关系分类。</p>
<p>其中关系检测判断两个实体之间是否存在语义关系，而关系分类将存在语义关系的实体对划分到预先指定的类别中。</p>
<p>在某些场景和任务下，关系抽取系统也可能包含关系发现模块，其主要目的是发现实体和实体之间存在的语义关系类别。例如，发现人物和公司之间存在雇员、CEO、CTO、创始人、董事长等关系类别。</p>
<ol>
<li>事件抽取</li>
</ol>
<p>事件抽取指的是从非结构化文本中抽取事件信息，并将其以结构化形式呈现出来的任务。</p>
<p>例如，从“毛泽东 1893 年出生于湖南湘潭”这句话中抽取事件{类型：出生， 人物：毛泽东，时间：1893 年，出生地：湖南湘潭}。</p>
<p>事件抽取任务通常包含事件类型识别和事件元素填充两个子任务。</p>
<p>事件类型识别判断一句话是否表达了特定类型的事件。事件类型决定了事件表示的模板，不同类型的事件具有不同的模板。</p>
<p>例如出生事件的模板是{人物， 时间，出生地}，而恐怖袭击事件的模板是{地点，时间，袭击者，受害者，受伤人数,…}。 事件元素指组成事件的关键元素，事件元素识别指的是根据所属的事件模板，抽取相应的元素，并为其标上正确元素标签的任务。</p>
<ol>
<li>信息集成</li>
</ol>
<p>实体、关系和事件分别表示了单篇文本中不同粒度的信息。在很多应用中，需要将来自不同数据源、不同文本的信息综合起来进行决策，这就需要研究信息集成技术。</p>
<p>目前，信息抽取研究中的信息集成技术主要包括共指消解技术和实体链接技术。</p>
<p>共指消解指的是检测同一实体/关系/事件的不同提及，并将其链接在一起的任务，例如，识别“乔布斯是苹果的创始人之一，他经历了苹果公司几十年的起落与兴衰”这句话中的“乔布斯”和“他”指的是同一实体。</p>
<p>实体链接的目的是确定实体名所指向的真实世界实体。例如识别上一句话中的“苹果”和“乔布斯”分别指向真实世界中的苹果公司和其 CEO 史蒂夫·乔布斯。</p>
<p>七. 问答系统</p>
<p>自动问答（Question Answering, QA）是指利用计算机自动回答用户所提出的问题以满足用户知识需求的任务。不同于现有搜索引擎，问答系统是信息服务的一种高级形式，系统返回用户的不再是基于关键词匹配排序的文档列表，而是精准的自然语言答案。</p>
<p>近年来，随着人工智能的飞速发展，自动问答已经成为倍受关注且发展前景广泛的研究方向。自动问答的研究历史可以溯源到人工智能的原点。</p>
<p>1950 年，人工智能之父阿兰图灵（Alan M. Turing）在《Mind》上发表文章《Computing Machinery and Intelligence》，文章开篇提出通过让机器参与一个模仿游戏（Imitation Game）来验证“机器”能否“思考”，进而提出了经典的图灵测试（Turing Test），用以检验机器是否具备智能。</p>
<p>同样，在自然语言处理研究领域，问答系统被认为是验证机器是否具备自然语言理解能力的四个任务之一（其它三个是机器翻译、复述和文本摘要）。</p>
<p>自动问答研究既有利于推动人工智能相关学科的发展，也具有非常重要的学术意义。从应用上讲，现有基于关键词匹配和浅层语义分析的信息服务技术已经难以满足用户日益增长的精准化和智能化信息需求，已有的信息服务范式急需一场变革。</p>
<p>2011 年，华盛顿大学图灵中心主任 Etzioni 在 Nature 上发表的《Search Needs a Shake-Up》中明确指出：在万维网诞生 20 周年之际，互联网搜索正处于从简单关键词搜索走向深度问答的深刻变革的风口浪尖上。以直接而准确的方式回答用户自然语言提问的自动问答系统将构成下一代搜索引擎的基本形态。</p>
<p>同一年，以深度问答技术为核心的 IBM Watson 自动问答机器人在美国智力竞赛节目 Jeopardy 中战胜人类选手，引起了业内的巨大轰动。Watson 自动问答系统让人们看到已有信息服务模式被颠覆的可能性，成为了问答系统发展的一个里程碑。</p>
<p>此外，随着移动互联网崛起与发展，以苹果公司 Siri、Google Now、微软 Cortana 等为代表的移动生活助手爆发式涌现，上述系统都把以自然语言为基本输入方式的问答系统看作是下一代信息服务的新形态和突破口，并均加大人员、资金的投入，试图在这一次人工智能浪潮中取得领先。</p>
<ol>
<li>关键问题</li>
</ol>
<p>自动问答系统在回答用户问题时，需要正确理解用户所提的自然语言问题，抽取其中的关键语义信息，然后在已有语料库、知识库或问答库中通过检索、匹配、推理的手段获取答案并返回给用户。</p>
<p>上述过程涉及词法分析、句法分析、语义分析、信息检索、逻辑推理、知识工程、语言生成等多项关键技术。传统自动问答多集中在限定领域，针对限定类型的问题进行回答。伴随着互联网和大数据的飞速发展，现有研究趋向于开放域、面向开放类型问题的自动问答。概括地讲，自动问答的主要研究任务和相应关键科学问题如下。</p>
<p>1.1 问句理解</p>
<p>给定用户问题，自动问答首先需要理解用户所提问题。用户问句的语义理解包含词法分析、句法分析、语义分析等多项关键技术，需要从文本的多个维度理解其中包含的语义内容。</p>
<p>在词语层面，需要在开放域环境下，研究命名实体识别（Named Entity Recognition）、术语识别（Term Extraction）、词汇化答案类型词识别（Lexical Answer Type Recognition）、 实体消歧（Entity Disambiguation）、关键词权重计算（Keyword Weight Estimation）、答案集中词识别（Focused Word Detection）等关键问题。</p>
<p>在句法层面，需要解析句子中词与词之间、短语与短语之间的句法关系，分析句子句法结构。在语义层面，需要根据词语层面、句法层面的分析结果，将自然语言问句解析成可计算、结构化的逻辑表达形式（如一阶谓词逻辑表达式）。</p>
<p>1.2 文本信息抽取</p>
<p>给定问句语义分析结果，自动问答系统需要在已有语料库、知识库或问答库中匹配相关的信息，并抽取出相应的答案。</p>
<p>传统答案抽取构建在浅层语义分析基础之上，采用关键词匹配策略，往往只能处理限定类型的答案，系统的准确率和效率都难以满足实际应用需求。为保证信息匹配以及答案抽取的准确度，需要分析语义单元之间的语义关系，抽取文本中的结构化知识。</p>
<p>早期基于规则模板的知识抽取方法难以突破领域和问题类型的限制，远远不能满足开放领域自动问答的知识需求。为了适应互联网实际应用的需求，越来越多的研究者和开发者开始关注开放域知识抽取技术，其特点在于：</p>
<p>• 文本领域开放：处理的文本是不限定领域的网络文本</p>
<p>• 内容单元类型开放：不限定所抽取的内容单元类型，而是自动地从网络中挖掘内容单元的类型，例如实体类型、事件类型和关系类型等。</p>
<p>1.3 知识推理</p>
<p>自动问答中，由于语料库、知识库和问答库本身的覆盖度有限，并不是所有问题都能直 接找到答案。这就需要在已有的知识体系中，通过知识推理的手段获取这些隐含的答案。</p>
<p>例如，知识库中可能包括了一个人的“出生地”信息，但是没包括这个人的“国籍”信息，因此无法直接回答诸如“某某人是哪国人?”这样的问题。但是一般情况下，一个人的“出生地”所属的国家就是他（她）的“国籍”。</p>
<p>在自动问答中，就需要通过推理的方式学习到这样的模式。传统推理方法采用基于符号的知识表示形式，通过人工构建的推理规则得到答案。</p>
<p>但是面对大规模、开放域的问答场景，如何自动进行规则学习，如何解决规则冲突仍然是亟待解决的难点问题。目前，基于分布式表示的知识表示学习方法能够将实体、概念以及它们之间的语义关系表示为低维空间中的对象（向量、矩阵等），并通过低维空间中的数值计算完成知识推理任务。</p>
<p>虽然这类推理的效果离实用还有距离，但是我们认为这是值得探寻的方法，特别是如何将已有的基于符号表示的逻辑推理与基于分布式表示的数值推理相结合，研究融合符号逻辑和表示学习的知识推理技术，是知识推理任务中的关键科学问题。</p>
<ol>
<li>技术方法</li>
</ol>
<p>根据目标数据源的不同，已有自动问答技术大致可以分为三类：</p>
<p>• 检索式问答；<br>• 社区问答;<br>• 知识库问答。</p>
<p>以下分别就这几个方面对研究现状进行简要阐述。</p>
<p>2.1 检索式问答</p>
<p>检索式问答研究伴随搜索引擎的发展不断推进。1999 年，随着 TREC QA 任务的发起， 检索式问答系统迎来了真正的研究进展。TREC QA 的任务是给定特定 WEB 数据集，从中找到能够回答问题的答案。这类方法是以检索和答案抽取为基本过程的问答系统，具体过程包括问题分析、篇章检索和答案抽取。</p>
<p>根据抽取方法的不同，已有检索式问答可以分为基于模式匹配的问答方法和基于统计文本信息抽取的问答方法。</p>
<p>• 基于模式匹配的方法往往先离线地获得各类提问答案的模式。在运行阶段，系统首先判断当前提问属于哪一类，然后使用这类提问的模式来对抽取的候选答案进行验证。同时为了提高问答系统的性能，人们也引入自然语言处理技术。由于自然语言处理的技术还未成熟，现有大多数系统都基于浅层句子分析。</p>
<p>• 基于统计文本信息抽取的问答系统的典型代表是美国 Language Computer Corporation 公司的 LCC 系统。该系统使用词汇链和逻辑形式转换技术，把提问句和答案句转化成统一的逻辑形式（Logic Form），通过词汇链，实现答案的推理验证。</p>
<p>LCC 系统在 TREC QA Track 2001 ~ 2004 连续三年的评测中以较大领先优势获得第一名的成绩。 2011 年，IBM 研发的问答机器人 Watson 在美国智力竞赛节目《危险边缘 Jeopardy!》中战胜人类选手，成为问答系统发展的一个里程碑。</p>
<p>Watson 的技术优势大致可以分为以下三个方面：</p>
<p>• 强大的硬件平台：包括 90 台 IBM 服务器，分布式计算环境；</p>
<p>• 强大的知识资源：存储了大约 2 亿页的图书、新闻、电影剧本、辞海、文选和《世界图书百科全书》等资料；</p>
<p>• 深层问答技术（DeepQA）：涉及统计机器学习、句法分析、主题分析、信息抽取、 知识库集成和知识推理等深层技术。</p>
<p>然而，Watson 并没有突破传统问答式检索系统的局限性，使用的技术主要还是检索和匹配，回答的问题类型大多是简单的实体或词语类问题，而推理能力不强。</p>
<p>2.2 社区问答</p>
<p>随着 Web2.0 的兴起，基于用户生成内容（User-Generated Content, UGC）的互联网服务越来越流行，社区问答系统应运而生，例如 Yahoo! Answers、百度知道等。</p>
<p>问答社区的出现为问答技术的发展带来了新的机遇。据统计 2010 年 Yahoo! Answers 上已解决的问题量达到 10 亿，2011 年“百度知道”已解决的问题量达到 3 亿，这些社区问答数据覆盖了方方面面的用户知识和信息需求。</p>
<p>此外，社区问答与传统自动问答的另一个显著区别是：社区问答系统有大量的用户参与，存在丰富的用户行为信息，例如用户投票信息、用户评价信息、回答者的问题采纳率、用户推荐次数、页面点击次数以及用户、问题、答案之间的相互关联信息等等，这些用户行为信息对于社区中问题和答案的文本内容分析具有重要的价值。</p>
<p>一般来讲，社区问答的核心问题是从大规模历史问答对数据中找出与用户提问问题语义相似的历史问题并将其答案返回提问用户。</p>
<p>假设用户查询问题为 q0,用于检索的问答对数据为 SQ,A = {(q1 , a1 ), (q2 , a2 )}, … , (qn, an)}}，相似问答对检索的目标是从 SQ,A 中检索出能够解答问题 q0 的问答对 (qi , ai)。 针对这一问题，传统的信息检索模型，如向量空间模型、语言模型等，都可以得到应用。</p>
<p>但是，相对于传统的文档检索，社区问答的特点在于：用户问题和已有问句相对来说都非常短，用户问题和已有问句之间存在“词汇鸿沟”问题，基于关键词匹配的检索模型很难达到较好的问答准确度。</p>
<p>目前，很多研究工作在已有检索框架中针对这一问题引入单语言翻译概率模型，通过 IBM 翻译模型，从海量单语问答语料中获得同种语言中两个不同词语之间的语义转换概率，从而在一定程度上解决词汇语义鸿沟问题。</p>
<p>例如和“减肥”对应的概率高的相关词有“瘦身”、“跑步”、“饮食”、“健康”、“远动”等等。 除此之外，也有许多关于问句检索中词重要性的研究和基于句法结构的问题匹配研究。</p>
<p>2.3 知识库问答</p>
<p>检索式问答和社区问答尽管在某些特定领域或者商业领域有所应用，但是其核心还是关键词匹配和浅层语义分析技术，难以实现知识的深层逻辑推理，无法达到人工智能的高级目标。</p>
<p>因此，近些年来，无论是学术界或工业界，研究者们逐步把注意力投向知识图谱或知识库（Knowledge Graph）。其目标是把互联网文本内容组织成为以实体为基本语义单元（节点）的图结构，其中图上的边表示实体之间语义关系。</p>
<p>目前互联网中已有的大规模知识库包括 DBpedia、Freebase、YAGO 等。这些知识库多是以“实体-关系-实体”三元组为基本单元所组成的图结构。</p>
<p>基于这样的结构化知识，问答系统的任务就是要根据用户问题的语义直接在知识库上查找、推理出相匹配的答案，这一任务称为面向知识库的问答系统或知识库问答。要完成在结构化数据上的查询、匹配、推理等操作，最有效的方式是利用结构化的查询语句，例如：SQL、SPARQL 等。</p>
<p>然而，这些语句通常是由专家编写，普通用户很难掌握并正确运用。对普通用户来说，自然语言仍然是最自然的交互方式。因此，如何把用户的自然语言问句转化为结构化的查询语句是知识库问答的核心所在，其关键是对于自然语言问句进行语义理解。</p>
<p>目前，主流方法是通过语义分析，将用户的自然语言问句转化成结构化的语义表示，如范式和 DCS-Tree。相对应的语义解析语法或方法包括组合范畴语法（ Category Compositional Grammar, CCG ）以 及 依 存 组 合 语 法（ Dependency-based Compositional Semantics, DCS）等。</p>
<p>八. 机器翻译</p>
<ol>
<li>理论应用</li>
</ol>
<p>机器翻译（machine translation，MT）是指利用计算机实现从一种自然语言到另外一种自然语言的自动翻译。被翻译的语言称为源语言（source language），翻译到的语言称作目标语言（target language）。 </p>
<p>简单地讲，机器翻译研究的目标就是建立有效的自动翻译方法、模型和系统，打破语言壁垒，最终实现任意时间、任意地点和任意语言的自动翻译，完成人们无障碍自由交流的梦想。</p>
<p>人们通常习惯于感知（听、看和读）自己母语的声音和文字，很多人甚至只能感知自己的母语，因此，机器翻译在现实生活和工作中具有重要的社会需求。</p>
<p>从理论上讲，机器翻译涉及语言学、计算语言学、人工智能、机器学习，甚至认知语言学等多个学科，是一个典型的多学科交叉研究课题，因此开展这项研究具有非常重要的理论意义，既有利于推动相关学科的发展，揭示人脑实现跨语言理解的奥秘，又有助于促进其他自然语言处理任务，包括中文信息处理技术的快速发展。</p>
<p>从应用上讲，无论是社会大众、政府企业还是国家机构，都迫切需要机器翻译技术。特别是在“互联网+”时代，以多语言多领域呈现的大数据已成为我们面临的常态问题，机器翻译成为众多应用领域革新的关键技术之一。</p>
<p>例如，在商贸、体育、文化、旅游和教育等各个领域，人们接触到越来越多的外文资料，越来越频繁地与持各种语言的人通信和交流，从而对机器翻译的需求越来越强烈；在国家信息安全和军事情报领域，机器翻译技术也扮演着非常重要的角色。</p>
<p>可以说离开机器翻译，基于大数据的多语言信息获取、挖掘、分析和决策等其他应用都将成为空中楼阁。</p>
<p>尤其值得提出的是，在未来很长一段时间里，建立于丝绸之路这一历史资源之上的“一带一路”将是我国与周边国家发展政治、经济，进行文化交流的主要战略。据统计，“一带一路”涉及 60 多个国家、44 亿人口、53 种语言，可见机器翻译是“一带一路”战略实施中不可或缺的重要技术。</p>
<ol>
<li>技术现状</li>
</ol>
<p>基于规则的机器翻译方法需要人工设计和编纂翻译规则，统计机器翻译方法能够自动获取翻译规则，但需要人工定义规则的形式，而端到端的神经网络机器翻译方法可以直接通过编码网络和解码网络自动学习语言之间的转换算法。</p>
<p>从某种角度讲，其自动化程度和智能化程度在不断提升，机器翻译质量也得到了显著改善。机器翻译技术的研究现状可从欧盟组织的国际机器翻译评测（WMT）的结果中窥得一斑。</p>
<p>该评测主要针对欧洲语言之间的互译，2006 年至 2016 年每年举办一次。对比法语到英语历年的机器翻译评测结果可以发现，译文质量已经在自动评价指标 BLEU 值上从最初小于 0.3 到目前接近 0.4（大量的人工评测对比说明，BLEU 值接近 0.4 的译文能够达到人类基本可以理解的程度）。</p>
<p>另外，中国中文信息学会组织的全国机器翻译评测（CWMT）每两年组织一次， 除了英汉、日汉翻译评测以外，CWMT 还关注我国少数民族语言（藏、蒙、维）和汉语之间的翻译。</p>
<p>相对而言，由于数据规模和语言复杂性的问题，少数民族与汉语之间的翻译性能要低于汉英、汉日之间的翻译性能。虽然机器翻译系统评测的分值呈逐年增长的趋势，译文质量越来越好，但与专业译员的翻译结果相比，机器翻译还有很长的路要走，可以说，在奔向“信、达、雅”翻译目标的征程上，目前的机器翻译基本挣扎在“信”的阶段，很多理论和技术问题仍有待于更深入的研究和探索。</p>
<p>九. 自动摘要</p>
<p>自动文摘（又称自动文档摘要）是指通过自动分析给定的一篇文档或多篇文档，提炼、总结其中的要点信息，最终输出一篇长度较短、可读性良好的摘要（通常包含几句话或数百字），该摘要中的句子可直接出自原文，也可重新撰写所得。</p>
<p>简言之，文摘的目的是通过对原文本进行压缩、提炼，为用户提供简明扼要的文字描述。用户可以通过阅读简短的摘要而知晓原文中所表达的主要内容，从而大幅节省阅读时间。</p>
<p>自动文摘研究的目标是建立有效的自动文摘方法与模型，实现高性能的自动文摘系统。近二十年来，业界提出了各类自动文摘方法与模型，用于解决各类自动摘要问题，在部分自动摘要问题的研究上取得了明显的进展，并成功将自动文摘技术应用于搜索引擎、新闻阅读 等产品与服务中。</p>
<p>例如谷歌、百度等搜索引擎均会为每项检索结果提供一个短摘要，方便用 户判断检索结果相关性。在新闻阅读软件中，为新闻事件提供摘要也能够方便用户快速了解 该事件。2013 年雅虎耗资 3000 万美元收购了一项自动新闻摘要应用 Summly，则标志着自动文摘技术的应用走向成熟。</p>
<p>自动文摘的研究在图书馆领域和自然语言处理领域一直都很活跃，最早的应用需求来自于图书馆。图书馆需要为大量文献书籍生成摘要，而人工摘要的效率很低，因此亟需自动摘要方法取代人工高效地完成文献摘要任务。</p>
<p>随着信息检索技术的发展，自动文摘在信息检索系统中的重要性越来越大，逐渐成为研究热点之一。经过数十年的发展，同时在 DUC 与 TAC 等自动文摘国际评测的推动下，文本摘要技术已经取得长足的进步。国际上自动文摘方面比较著名的几个系统包括 ISI 的 NeATS 系统，哥伦比亚大学的 NewsBlaster 系统，密歇根大学的 NewsInEssence 系统等。</p>
<ol>
<li>方法</li>
</ol>
<p>自动文摘所采用的方法从实现上考虑可以分为抽取式摘要（extractive summarization） 和生成式摘要（abstractive summarization）。</p>
<p>抽取式方法相对比较简单，通常利用不同方法对文档结构单元（句子、段落等）进行评价，对每个结构单元赋予一定权重，然后选择最重要的结构单元组成摘要。而生成式方法通常需要利用自然语言理解技术对文本进行语法、 语义分析，对信息进行融合，利用自然语言生成技术生成新的摘要句子。</p>
<p>目前的自动文摘方法主要基于句子抽取，也就是以原文中的句子作为单位进行评估与选取。抽取式方法的好处是易于实现，能保证摘要中的每个句子具有良好的可读性。</p>
<p>为解决如前所述的要点筛选和文摘合成这两个关键科学问题，目前主流自动文摘研究工作大致遵循如下技术框架： 内容表示 → 权重计算 → 内容选择 → 内容组织。</p>
<p>首先将原始文本表示为便于后续处理的表达方式，然后由模型对不同的句法或语义单元 进行重要性计算，再根据重要性权重选取一部分单元，经过内容上的组织形成最后的摘要。</p>
<p>1.1 内容表示与权重计算</p>
<p>原文档中的每个句子由多个词汇或单元构成，后续处理过程中也以词汇等元素为基本单位，对所在句子给出综合评价分数。</p>
<p>以基于句子选取的抽取式方法为例，句子的重要性得分由其组成部分的重要性衡量。由于词汇在文档中的出现频次可以在一定程度上反映其重要性， 我们可以使用每个句子中出现某词的概率作为该词的得分，通过将所有包含词的概率求和得到句子得分。</p>
<p>也有一些工作考虑更多细节，利用扩展性较强的贝叶斯话题模型，对词汇本身的话题相关性概率进行建模。一些方法将每个句子表示为向量，维数为总词表大小。通常使用加权频数作为句子向量相应维上的取值。加权频数的定义可以有多种，如信息检索中常用的词频-逆文档频率 （TF-IDF）权重。</p>
<p>也有研究工作考虑利用隐语义分析或其他矩阵分解技术，得到低维隐含语义表示并加以利用。得到向量表示后计算两两之间的某种相似度（例如余弦相似度）。随后根据计算出的相似度构建带权图，图中每个节点对应每个句子。</p>
<p>在多文档摘要任务中，重要的句子可能和更多其他句子较为相似，所以可以用相似度作为节点之间的边权，通过迭代求解基于图的排序算法来得到句子的重要性得分。</p>
<p>也有很多工作尝试捕捉每个句子中所描述的概念，例如句子中所包含的命名实体或动词。</p>
<p>出于简化考虑，现有工作中更多将二元词（bigram）作为概念。近期则有工作提出利用频繁图挖掘算法从文档集中挖掘得到深层依存子结构作为语义表示单元。</p>
<p>另一方面，很多摘要任务已经具备一定数量的公开数据集，可用于训练有监督打分模型。</p>
<p>例如对于抽取式摘要，我们可以将人工撰写的摘要贪心匹配原文档中的句子或概念，从而得到不同单元是否应当被选作摘要句的数据。然后对各单元人工抽取若干特征，利用回归模型或排序学习模型进行有监督学习，得到句子或概念对应的得分。</p>
<p>文档内容描述具有结构性，因此也有利用隐马尔科夫模型（HMM）、条件随机场（CRF）、结构化支持向量机（Structural SVM）等常见序列标注或一般结构预测模型进行抽取式摘要有监督训练的工作。</p>
<p>所提取的特征包括所在位置、包含词汇、与邻句的相似度等等。对特定摘要任务一般也会引入与具体设定相关的特征，例如查询相关摘要任务中需要考虑与查询的匹配或相似程度。</p>
<p>1.2 内容选择</p>
<p>无论从效果评价还是从实用性的角度考虑，最终生成的摘要一般在长度上会有限制。在获取到句子或其他单元的重要性得分以后，需要考虑如何在尽可能短的长度里容纳尽可能多的重要信息，在此基础上对原文内容进行选取。内容选择方法包括贪心选择和全局优化。</p>
<ol>
<li>技术现状</li>
</ol>
<p>相比机器翻译、自动问答、知识图谱、情感分析等热门领域，自动文摘在国内并没有受到足够的重视。</p>
<p>国内早期的基础资源与评测举办过中文单文档摘要的评测任务，但测试集规模比较小，而且没有提供自动化评价工具。2015 年 CCF 中文信息技术专委会组织了 NLPCC 评测，其中包括了面向中文微博的新闻摘要任务，提供了规模相对较大的样例数据和测试数据，并采用自动评价方法，吸引了多支队伍参加评测，目前这些数据可以公开获得。</p>
<p>但上述中文摘要评测任务均针对单文档摘要任务，目前还没有业界认可的中文多文档摘要数据，这在事实上阻碍了中文自动摘要技术的发展。</p>
<p>近些年，市面上出现了一些文本挖掘产品，能够提供中文文档摘要功能（尤其是单文档 摘要），例如方正智思、拓尔思（TRS），海量科技等公司的产品。百度等搜索引擎也能为检索到的文档提供简单的单文档摘要。这些文档摘要功能均被看作是系统的附属功能，其实现方法均比较简单。</p>
<p>十. 学习资料</p>
<ol>
<li>书籍</li>
</ol>
<p>1.1 李航《统计学习方法》<br>这本经典书值得反复读，从公式推导到定理证明逻辑严谨，通俗易懂。<br>推荐指数：★★★★★</p>
<p>1.1  宗成庆《统计自然语言处理》<br>推荐指数：★★★★☆</p>
<ol>
<li>博客</li>
</ol>
<p>斯坦福 cs224d：<br><a href="http://cs224d.stanford.edu/syllabus.html" target="_blank" rel="external">http://cs224d.stanford.edu/syllabus.html</a> </p>
<ol>
<li>会议</li>
</ol>
<p>ACL 2015:<br><a href="http://acl2015.org/accepted_papers.html" target="_blank" rel="external">http://acl2015.org/accepted_papers.html</a><br>ACL 2016:<br><a href="http://acl2016.org/index.php?article_id=13#long_papers" target="_blank" rel="external">http://acl2016.org/index.php?article_id=13#long_papers</a><br>EMNLP 2015:<br><a href="http://www.emnlp2015.org/accepted-papers.html" target="_blank" rel="external">http://www.emnlp2015.org/accepted-papers.html</a> </p>
<ol>
<li>实践案例</li>
</ol>
<p><a href="https://github.com/carpedm20/lstm-char-cnn-tensorflow" target="_blank" rel="external">https://github.com/carpedm20/lstm-char-cnn-tensorflow</a><br><a href="https://github.com/zoneplus/DL4NLP" target="_blank" rel="external">https://github.com/zoneplus/DL4NLP</a><br><a href="https://github.com/HIT-SCIR/scir-training-day" target="_blank" rel="external">https://github.com/HIT-SCIR/scir-training-day</a> </p>
<p>十一. 进一步学习</p>
<p>论文下载地址：</p>
<p><a href="http://ccl.pku.edu.cn/alcourse/nlp/LectureNotes/An%20Overview%20on%20Chinese%20Word%20Segmentation%20(Sun%20Maosong).pdf" target="_blank" rel="external">http://ccl.pku.edu.cn/alcourse/nlp/LectureNotes/An%20Overview%20on%20Chinese%20Word%20Segmentation%20(Sun%20Maosong).pdf</a><br><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2017/01/cl-05.gao_.pdf" target="_blank" rel="external">https://www.microsoft.com/en-us/research/wp-content/uploads/2017/01/cl-05.gao_.pdf</a><br><a href="http://www.voidcn.com/blog/forever1dreamsxx/article/p-1295137.html" target="_blank" rel="external">http://www.voidcn.com/blog/forever1dreamsxx/article/p-1295137.html</a><br><a href="http://cleanbugs.com/item/the-syntactic-structure-of-nlp-three-chinese-syntactic-structure-cips2016-413620.html" target="_blank" rel="external">http://cleanbugs.com/item/the-syntactic-structure-of-nlp-three-chinese-syntactic-structure-cips2016-413620.html</a><br><a href="http://cips-upload.bj.bcebos.com/cips2016.pdf" target="_blank" rel="external">http://cips-upload.bj.bcebos.com/cips2016.pdf</a></p>
<p>本文经授权转载自公众号「数据派THU」</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前言&lt;/p&gt;
&lt;p&gt;自然语言处理是文本挖掘的研究领域之一，是人工智能和语言学领域的分支学科。在此领域中探讨如何处理及运用自然语言。&lt;/p&gt;
&lt;p&gt;对于自然语言处理的发展历程，可以从哲学中的经验主义和理性主义说起。基于统计的自然语言处理是哲学中的经验主义，基于规则的自然语言处
    
    </summary>
    
      <category term="深度学习" scheme="http://ynuwm.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="NLP" scheme="http://ynuwm.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Faker采访的时候说</title>
    <link href="http://ynuwm.github.io/2017/10/30/Faker%E9%87%87%E8%AE%BF%E7%9A%84%E6%97%B6%E5%80%99%E8%AF%B4/"/>
    <id>http://ynuwm.github.io/2017/10/30/Faker采访的时候说/</id>
    <published>2017-10-30T08:15:19.000Z</published>
    <updated>2017-12-04T12:43:05.000Z</updated>
    
    <content type="html"><![CDATA[<p>10月28日，半决SKT VS RNG赛后Faker接受采访说：我们的目标一直都是冠军。如果在总决赛上我们输给了对面，那亚军对我来说毫无意义。<br><img src="/2017/10/30/Faker采访的时候说/000.jpg" alt="img"></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;10月28日，半决SKT VS RNG赛后Faker接受采访说：我们的目标一直都是冠军。如果在总决赛上我们输给了对面，那亚军对我来说毫无意义。&lt;br&gt;&lt;img src=&quot;/2017/10/30/Faker采访的时候说/000.jpg&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;

    
    </summary>
    
      <category term="日记" scheme="http://ynuwm.github.io/categories/%E6%97%A5%E8%AE%B0/"/>
    
    
      <category term="LOL" scheme="http://ynuwm.github.io/tags/LOL/"/>
    
  </entry>
  
  <entry>
    <title>Learning PyTorch With Examples</title>
    <link href="http://ynuwm.github.io/2017/10/11/Learning-PyTorch-With-Examples/"/>
    <id>http://ynuwm.github.io/2017/10/11/Learning-PyTorch-With-Examples/</id>
    <published>2017-10-11T11:22:56.000Z</published>
    <updated>2017-12-04T12:31:20.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h1><h2 id="Warm-up-numpy"><a href="#Warm-up-numpy" class="headerlink" title="Warm-up:numpy"></a>Warm-up:numpy</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create random input and output data</span></div><div class="line">x = np.random.randn(N, D_in)</div><div class="line">y = np.random.randn(N, D_out)</div><div class="line"></div><div class="line"><span class="comment"># Randomly initialize weights</span></div><div class="line">w1 = np.random.randn(D_in, H)</div><div class="line">w2 = np.random.randn(H, D_out)</div><div class="line"></div><div class="line">learning_rate = <span class="number">1e-6</span></div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">    <span class="comment"># Forward pass: compute predicted y</span></div><div class="line">    h = x.dot(w1)</div><div class="line">    h_relu = np.maximum(h, <span class="number">0</span>)</div><div class="line">    y_pred = h_relu.dot(w2)</div><div class="line"></div><div class="line">    <span class="comment"># Compute and print loss</span></div><div class="line">    loss = np.square(y_pred - y).sum()</div><div class="line">    print(t, loss)</div><div class="line"></div><div class="line">    <span class="comment"># Backprop to compute gradients of w1 and w2 with respect to loss</span></div><div class="line">    grad_y_pred = <span class="number">2.0</span> * (y_pred - y)</div><div class="line">    grad_w2 = h_relu.T.dot(grad_y_pred)</div><div class="line">    grad_h_relu = grad_y_pred.dot(w2.T)</div><div class="line">    grad_h = grad_h_relu.copy()</div><div class="line">    grad_h[h &lt; <span class="number">0</span>] = <span class="number">0</span></div><div class="line">    grad_w1 = x.T.dot(grad_h)</div><div class="line"></div><div class="line">    <span class="comment"># Update weights</span></div><div class="line">    w1 -= learning_rate * grad_w1</div><div class="line">    w2 -= learning_rate * grad_w2</div></pre></td></tr></table></figure>
<h2 id="PyTorch-Tensor"><a href="#PyTorch-Tensor" class="headerlink" title="PyTorch:Tensor"></a>PyTorch:Tensor</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> torch</div><div class="line"></div><div class="line"></div><div class="line">dtype = torch.FloatTensor</div><div class="line"><span class="comment"># dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU</span></div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create random input and output data</span></div><div class="line">x = torch.randn(N, D_in).type(dtype)</div><div class="line">y = torch.randn(N, D_out).type(dtype)</div><div class="line"></div><div class="line"><span class="comment"># Randomly initialize weights</span></div><div class="line">w1 = torch.randn(D_in, H).type(dtype)</div><div class="line">w2 = torch.randn(H, D_out).type(dtype)</div><div class="line"></div><div class="line">learning_rate = <span class="number">1e-6</span></div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">    <span class="comment"># Forward pass: compute predicted y</span></div><div class="line">    h = x.mm(w1)</div><div class="line">    h_relu = h.clamp(min=<span class="number">0</span>)</div><div class="line">    y_pred = h_relu.mm(w2)</div><div class="line"></div><div class="line">    <span class="comment"># Compute and print loss</span></div><div class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum()</div><div class="line">    print(t, loss)</div><div class="line"></div><div class="line">    <span class="comment"># Backprop to compute gradients of w1 and w2 with respect to loss</span></div><div class="line">    grad_y_pred = <span class="number">2.0</span> * (y_pred - y)</div><div class="line">    grad_w2 = h_relu.t().mm(grad_y_pred)</div><div class="line">    grad_h_relu = grad_y_pred.mm(w2.t())</div><div class="line">    grad_h = grad_h_relu.clone()</div><div class="line">    grad_h[h &lt; <span class="number">0</span>] = <span class="number">0</span></div><div class="line">    grad_w1 = x.t().mm(grad_h)</div><div class="line"></div><div class="line">    <span class="comment"># Update weights using gradient descent</span></div><div class="line">    w1 -= learning_rate * grad_w1</div><div class="line">    w2 -= learning_rate * grad_w2</div></pre></td></tr></table></figure>
<h1 id="Autograd"><a href="#Autograd" class="headerlink" title="Autograd"></a>Autograd</h1><h2 id="PyTorch-Variables-and-autograd"><a href="#PyTorch-Variables-and-autograd" class="headerlink" title="PyTorch:Variables and autograd"></a>PyTorch:Variables and autograd</h2><p>PyTorch中所有的神经网络都来自于autograd包<br>在上面的例子中，我们必须手动实现神经网络的向前和向后遍。手动实施后向传递对于小型双层网络来说不是一件大事，但是对于大型复杂网络来说，可以很快地得到很多毛病。</p>
<p>幸运的是，我们可以使用自动区分 来自动计算神经网络中的向后遍。PyTorch中的 autograd包提供了这个功能。使用自动格式时，网络的正向传递将定义一个 计算图 ; 图中的节点将是Tensors，边缘将是从输入Tensors生成输出Tensors的函数。通过此图反向传播，您可以轻松地计算渐变。</p>
<p>这听起来很复杂，在实践中使用起来很简单。我们将PyTorch Tensors包装在可变对象中; 变量表示计算图中的节点。如果x是变量，则x.data是Tensor，并且x.grad是另一个变量，x其相对于某个标量值保存渐变 。</p>
<p>PyTorch变量与PyTorch Tensors具有相同的API（几乎）您可以在Tensor上执行的任何操作也适用于变量; 区别在于使用变量定义计算图，允许您自动计算渐变。</p>
<p>这里我们使用PyTorch变量和自动调整来实现我们的两层网络; 现在我们不再需要手动实现向后通过网络：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line">dtype = torch.FloatTensor</div><div class="line"><span class="comment"># dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU</span></div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create random Tensors to hold input and outputs, and wrap them in Variables.</span></div><div class="line"><span class="comment"># Setting requires_grad=False indicates that we do not need to compute gradients</span></div><div class="line"><span class="comment"># with respect to these Variables during the backward pass.</span></div><div class="line">x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=<span class="keyword">False</span>)</div><div class="line">y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="comment"># Create random Tensors for weights, and wrap them in Variables.</span></div><div class="line"><span class="comment"># Setting requires_grad=True indicates that we want to compute gradients with</span></div><div class="line"><span class="comment"># respect to these Variables during the backward pass.</span></div><div class="line">w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=<span class="keyword">True</span>)</div><div class="line">w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=<span class="keyword">True</span>)</div><div class="line"></div><div class="line">learning_rate = <span class="number">1e-6</span></div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">    <span class="comment"># Forward pass: compute predicted y using operations on Variables; these</span></div><div class="line">    <span class="comment"># are exactly the same operations we used to compute the forward pass using</span></div><div class="line">    <span class="comment"># Tensors, but we do not need to keep references to intermediate values since</span></div><div class="line">    <span class="comment"># we are not implementing the backward pass by hand.</span></div><div class="line">    y_pred = x.mm(w1).clamp(min=<span class="number">0</span>).mm(w2)</div><div class="line"></div><div class="line">    <span class="comment"># Compute and print loss using operations on Variables.</span></div><div class="line">    <span class="comment"># Now loss is a Variable of shape (1,) and loss.data is a Tensor of shape</span></div><div class="line">    <span class="comment"># (1,); loss.data[0] is a scalar value holding the loss.</span></div><div class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum()</div><div class="line">    print(t, loss.data[<span class="number">0</span>])</div><div class="line"></div><div class="line">    <span class="comment"># Use autograd to compute the backward pass. This call will compute the</span></div><div class="line">    <span class="comment"># gradient of loss with respect to all Variables with requires_grad=True.</span></div><div class="line">    <span class="comment"># After this call w1.grad and w2.grad will be Variables holding the gradient</span></div><div class="line">    <span class="comment"># of the loss with respect to w1 and w2 respectively.</span></div><div class="line">    loss.backward()</div><div class="line"></div><div class="line">    <span class="comment"># Update weights using gradient descent; w1.data and w2.data are Tensors,</span></div><div class="line">    <span class="comment"># w1.grad and w2.grad are Variables and w1.grad.data and w2.grad.data are</span></div><div class="line">    <span class="comment"># Tensors.</span></div><div class="line">    w1.data -= learning_rate * w1.grad.data</div><div class="line">    w2.data -= learning_rate * w2.grad.data</div><div class="line"></div><div class="line">    <span class="comment"># Manually zero the gradients after updating weights</span></div><div class="line">    w1.grad.data.zero_()</div><div class="line">    w2.grad.data.zero_()</div></pre></td></tr></table></figure></p>
<h2 id="PyTorch-Defining-new-autograd-functions"><a href="#PyTorch-Defining-new-autograd-functions" class="headerlink" title="PyTorch:Defining new autograd functions"></a>PyTorch:Defining new autograd functions</h2><p>在PyTorch中，我们可以通过定义一个子类torch.autograd.Function并实现forward 和backward函数来轻松地定义自己的autograd运算符。然后，我们可以通过构造一个实例并将其称为函数，传递包含输入数据的变量来使用我们的新的自动格式运算符。</p>
<p>在这个例子中，我们定义了我们自己的自定义自整定函数来执行ReLU非线性，并用它来实现我们的两层网络：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyReLU</span><span class="params">(torch.autograd.Function)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    We can implement our own custom autograd Functions by subclassing</div><div class="line">    torch.autograd.Function and implementing the forward and backward passes</div><div class="line">    which operate on Tensors.</div><div class="line">    """</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        In the forward pass we receive a Tensor containing the input and return a</div><div class="line">        Tensor containing the output. You can cache arbitrary Tensors for use in the</div><div class="line">        backward pass using the save_for_backward method.</div><div class="line">        """</div><div class="line">        self.save_for_backward(input)</div><div class="line">        <span class="keyword">return</span> input.clamp(min=<span class="number">0</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, grad_output)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        In the backward pass we receive a Tensor containing the gradient of the loss</div><div class="line">        with respect to the output, and we need to compute the gradient of the loss</div><div class="line">        with respect to the input.</div><div class="line">        """</div><div class="line">        input, = self.saved_tensors</div><div class="line">        grad_input = grad_output.clone()</div><div class="line">        grad_input[input &lt; <span class="number">0</span>] = <span class="number">0</span></div><div class="line">        <span class="keyword">return</span> grad_input</div><div class="line"></div><div class="line"></div><div class="line">dtype = torch.FloatTensor</div><div class="line"><span class="comment"># dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU</span></div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create random Tensors to hold input and outputs, and wrap them in Variables.</span></div><div class="line">x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=<span class="keyword">False</span>)</div><div class="line">y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="comment"># Create random Tensors for weights, and wrap them in Variables.</span></div><div class="line">w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=<span class="keyword">True</span>)</div><div class="line">w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=<span class="keyword">True</span>)</div><div class="line"></div><div class="line">learning_rate = <span class="number">1e-6</span></div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">    <span class="comment"># Construct an instance of our MyReLU class to use in our network</span></div><div class="line">    relu = MyReLU()</div><div class="line"></div><div class="line">    <span class="comment"># Forward pass: compute predicted y using operations on Variables; we compute</span></div><div class="line">    <span class="comment"># ReLU using our custom autograd operation.</span></div><div class="line">    y_pred = relu(x.mm(w1)).mm(w2)</div><div class="line"></div><div class="line">    <span class="comment"># Compute and print loss</span></div><div class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum()</div><div class="line">    print(t, loss.data[<span class="number">0</span>])</div><div class="line"></div><div class="line">    <span class="comment"># Use autograd to compute the backward pass.</span></div><div class="line">    loss.backward()</div><div class="line"></div><div class="line">    <span class="comment"># Update weights using gradient descent</span></div><div class="line">    w1.data -= learning_rate * w1.grad.data</div><div class="line">    w2.data -= learning_rate * w2.grad.data</div><div class="line"></div><div class="line">    <span class="comment"># Manually zero the gradients after updating weights</span></div><div class="line">    w1.grad.data.zero_()</div><div class="line">    w2.grad.data.zero_()</div></pre></td></tr></table></figure></p>
<h2 id="TensorFlow-Static-Graphs"><a href="#TensorFlow-Static-Graphs" class="headerlink" title="TensorFlow: Static Graphs"></a>TensorFlow: Static Graphs</h2><p>PyTorch autograd看起来很像TensorFlow：在两个框架中我们定义一个计算图，并使用自动差分来计算梯度。两者之间的最大区别在于TensorFlow的计算图是静态的，PyTorch使用 动态计算图。</p>
<p>在TensorFlow中，我们定义了一次计算图，然后一遍又一遍地执行相同的图，可能会将不同的输入数据提供给图形。在PyTorch中，每个前进传递定义了一个新的计算图。</p>
<p>静态图是很好的，因为你可以优化前面的图形; 例如，框架可能决定融合一些图形操作以获得效率，或者提出一种将图形分布在多个GPU或许多机器上的策略。如果您一遍又一遍地重复使用相同的图表，那么这个潜在的昂贵的前期优化可以被分摊，因为同一个图表一遍又一遍地重新运行。</p>
<p>静态和动态图不同的一个方面是控制流程。对于某些型号，我们可能希望对每个数据点执行不同的计算; 例如，对于每个数据点，可以展开不同数量的时间步长的循环网络; 这个展开可以被实现为循环。使用静态图形，循环构造需要是图形的一部分; 由于这个原因，TensorFlow提供了诸如tf.scan将循环嵌入图中的操作符。使用动态图表，情况更简单：由于我们为每个示例动态构建图表，因此我们可以使用正常的命令式流程控制来执行每个输入不同的计算。</p>
<p>为了与上面的PyTorch autograd示例进行对比，我们使用TensorFlow来简化两层网络：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment"># First we set up the computational graph:</span></div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create placeholders for the input and target data; these will be filled</span></div><div class="line"><span class="comment"># with real data when we execute the graph.</span></div><div class="line">x = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, D_in))</div><div class="line">y = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, D_out))/</div><div class="line"></div><div class="line"><span class="comment"># Create Variables for the weights and initialize them with random data.</span></div><div class="line"><span class="comment"># A TensorFlow Variable persists its value across executions of the graph.</span></div><div class="line">w1 = tf.Variable(tf.random_normal((D_in, H)))</div><div class="line">w2 = tf.Variable(tf.random_normal((H, D_out)))</div><div class="line"></div><div class="line"><span class="comment"># Forward pass: Compute the predicted y using operations on TensorFlow Tensors.</span></div><div class="line"><span class="comment"># Note that this code does not actually perform any numeric operations; it</span></div><div class="line"><span class="comment"># merely sets up the computational graph that we will later execute.</span></div><div class="line">h = tf.matmul(x, w1)</div><div class="line">h_relu = tf.maximum(h, tf.zeros(<span class="number">1</span>))</div><div class="line">y_pred = tf.matmul(h_relu, w2)</div><div class="line"></div><div class="line"><span class="comment"># Compute loss using operations on TensorFlow Tensors</span></div><div class="line">loss = tf.reduce_sum((y - y_pred) ** <span class="number">2.0</span>)</div><div class="line"></div><div class="line"><span class="comment"># Compute gradient of the loss with respect to w1 and w2.</span></div><div class="line">grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])</div><div class="line"></div><div class="line"><span class="comment"># Update the weights using gradient descent. To actually update the weights</span></div><div class="line"><span class="comment"># we need to evaluate new_w1 and new_w2 when executing the graph. Note that</span></div><div class="line"><span class="comment"># in TensorFlow the the act of updating the value of the weights is part of</span></div><div class="line"><span class="comment"># the computational graph; in PyTorch this happens outside the computational</span></div><div class="line"><span class="comment"># graph.</span></div><div class="line">learning_rate = <span class="number">1e-6</span></div><div class="line">new_w1 = w1.assign(w1 - learning_rate * grad_w1)</div><div class="line">new_w2 = w2.assign(w2 - learning_rate * grad_w2)</div><div class="line"></div><div class="line"><span class="comment"># Now we have built our computational graph, so we enter a TensorFlow session to</span></div><div class="line"><span class="comment"># actually execute the graph.</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    <span class="comment"># Run the graph once to initialize the Variables w1 and w2.</span></div><div class="line">    sess.run(tf.global_variables_initializer())</div><div class="line"></div><div class="line">    <span class="comment"># Create numpy arrays holding the actual data for the inputs x and targets</span></div><div class="line">    <span class="comment"># y</span></div><div class="line">    x_value = np.random.randn(N, D_in)</div><div class="line">    y_value = np.random.randn(N, D_out)</div><div class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">        <span class="comment"># Execute the graph many times. Each time it executes we want to bind</span></div><div class="line">        <span class="comment"># x_value to x and y_value to y, specified with the feed_dict argument.</span></div><div class="line">        <span class="comment"># Each time we execute the graph we want to compute the values for loss,</span></div><div class="line">        <span class="comment"># new_w1, and new_w2; the values of these Tensors are returned as numpy</span></div><div class="line">        <span class="comment"># arrays.</span></div><div class="line">        loss_value, _, _ = sess.run([loss, new_w1, new_w2],</div><div class="line">                                    feed_dict=&#123;x: x_value, y: y_value&#125;)</div><div class="line">        print(loss_value)</div></pre></td></tr></table></figure></p>
<h1 id="nn-module"><a href="#nn-module" class="headerlink" title="nn module"></a>nn module</h1><h2 id="PyTorch-nn"><a href="#PyTorch-nn" class="headerlink" title="PyTorch: nn"></a>PyTorch: nn</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create random Tensors to hold inputs and outputs, and wrap them in Variables.</span></div><div class="line">x = Variable(torch.randn(N, D_in))</div><div class="line">y = Variable(torch.randn(N, D_out), requires_grad=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="comment"># Use the nn package to define our model as a sequence of layers. nn.Sequential</span></div><div class="line"><span class="comment"># is a Module which contains other Modules, and applies them in sequence to</span></div><div class="line"><span class="comment"># produce its output. Each Linear Module computes output from input using a</span></div><div class="line"><span class="comment"># linear function, and holds internal Variables for its weight and bias.</span></div><div class="line">model = torch.nn.Sequential(</div><div class="line">    torch.nn.Linear(D_in, H),</div><div class="line">    torch.nn.ReLU(),</div><div class="line">    torch.nn.Linear(H, D_out),</div><div class="line">)</div><div class="line"></div><div class="line"><span class="comment"># The nn package also contains definitions of popular loss functions; in this</span></div><div class="line"><span class="comment"># case we will use Mean Squared Error (MSE) as our loss function.</span></div><div class="line">loss_fn = torch.nn.MSELoss(size_average=<span class="keyword">False</span>)</div><div class="line"></div><div class="line">learning_rate = <span class="number">1e-4</span></div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">    <span class="comment"># Forward pass: compute predicted y by passing x to the model. Module objects</span></div><div class="line">    <span class="comment"># override the __call__ operator so you can call them like functions. When</span></div><div class="line">    <span class="comment"># doing so you pass a Variable of input data to the Module and it produces</span></div><div class="line">    <span class="comment"># a Variable of output data.</span></div><div class="line">    y_pred = model(x)</div><div class="line"></div><div class="line">    <span class="comment"># Compute and print loss. We pass Variables containing the predicted and true</span></div><div class="line">    <span class="comment"># values of y, and the loss function returns a Variable containing the</span></div><div class="line">    <span class="comment"># loss.</span></div><div class="line">    loss = loss_fn(y_pred, y)</div><div class="line">    print(t, loss.data[<span class="number">0</span>])</div><div class="line"></div><div class="line">    <span class="comment"># Zero the gradients before running the backward pass.</span></div><div class="line">    model.zero_grad()</div><div class="line"></div><div class="line">    <span class="comment"># Backward pass: compute gradient of the loss with respect to all the learnable</span></div><div class="line">    <span class="comment"># parameters of the model. Internally, the parameters of each Module are stored</span></div><div class="line">    <span class="comment"># in Variables with requires_grad=True, so this call will compute gradients for</span></div><div class="line">    <span class="comment"># all learnable parameters in the model.</span></div><div class="line">    loss.backward()</div><div class="line"></div><div class="line">    <span class="comment"># Update the weights using gradient descent. Each parameter is a Variable, so</span></div><div class="line">    <span class="comment"># we can access its data and gradients like we did before.</span></div><div class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</div><div class="line">        param.data -= learning_rate * param.grad.data</div></pre></td></tr></table></figure>
<h2 id="PyTorch-optim"><a href="#PyTorch-optim" class="headerlink" title="PyTorch: optim"></a>PyTorch: optim</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create random Tensors to hold inputs and outputs, and wrap them in Variables.</span></div><div class="line">x = Variable(torch.randn(N, D_in))</div><div class="line">y = Variable(torch.randn(N, D_out), requires_grad=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="comment"># Use the nn package to define our model and loss function.</span></div><div class="line">model = torch.nn.Sequential(</div><div class="line">    torch.nn.Linear(D_in, H),</div><div class="line">    torch.nn.ReLU(),</div><div class="line">    torch.nn.Linear(H, D_out),</div><div class="line">)</div><div class="line">loss_fn = torch.nn.MSELoss(size_average=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="comment"># Use the optim package to define an Optimizer that will update the weights of</span></div><div class="line"><span class="comment"># the model for us. Here we will use Adam; the optim package contains many other</span></div><div class="line"><span class="comment"># optimization algoriths. The first argument to the Adam constructor tells the</span></div><div class="line"><span class="comment"># optimizer which Variables it should update.</span></div><div class="line">learning_rate = <span class="number">1e-4</span></div><div class="line">optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">    <span class="comment"># Forward pass: compute predicted y by passing x to the model.</span></div><div class="line">    y_pred = model(x)</div><div class="line"></div><div class="line">    <span class="comment"># Compute and print loss.</span></div><div class="line">    loss = loss_fn(y_pred, y)</div><div class="line">    print(t, loss.data[<span class="number">0</span>])</div><div class="line"></div><div class="line">    <span class="comment"># Before the backward pass, use the optimizer object to zero all of the</span></div><div class="line">    <span class="comment"># gradients for the variables it will update (which are the learnable weights</span></div><div class="line">    <span class="comment"># of the model)</span></div><div class="line">    optimizer.zero_grad()</div><div class="line"></div><div class="line">    <span class="comment"># Backward pass: compute gradient of the loss with respect to model</span></div><div class="line">    <span class="comment"># parameters</span></div><div class="line">    loss.backward()</div><div class="line"></div><div class="line">    <span class="comment"># Calling the step function on an Optimizer makes an update to its</span></div><div class="line">    <span class="comment"># parameters</span></div><div class="line">    optimizer.step()</div></pre></td></tr></table></figure>
<h2 id="PyTorch-Custom-nn-Modules"><a href="#PyTorch-Custom-nn-Modules" class="headerlink" title="PyTorch: Custom nn Modules"></a>PyTorch: Custom nn Modules</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">TwoLayerNet</span><span class="params">(torch.nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, D_in, H, D_out)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        In the constructor we instantiate two nn.Linear modules and assign them as</div><div class="line">        member variables.</div><div class="line">        """</div><div class="line">        super(TwoLayerNet, self).__init__()</div><div class="line">        self.linear1 = torch.nn.Linear(D_in, H)</div><div class="line">        self.linear2 = torch.nn.Linear(H, D_out)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        In the forward function we accept a Variable of input data and we must return</div><div class="line">        a Variable of output data. We can use Modules defined in the constructor as</div><div class="line">        well as arbitrary operators on Variables.</div><div class="line">        """</div><div class="line">        h_relu = self.linear1(x).clamp(min=<span class="number">0</span>)</div><div class="line">        y_pred = self.linear2(h_relu)</div><div class="line">        <span class="keyword">return</span> y_pred</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create random Tensors to hold inputs and outputs, and wrap them in Variables</span></div><div class="line">x = Variable(torch.randn(N, D_in))</div><div class="line">y = Variable(torch.randn(N, D_out), requires_grad=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="comment"># Construct our model by instantiating the class defined above</span></div><div class="line">model = TwoLayerNet(D_in, H, D_out)</div><div class="line"></div><div class="line"><span class="comment"># Construct our loss function and an Optimizer. The call to model.parameters()</span></div><div class="line"><span class="comment"># in the SGD constructor will contain the learnable parameters of the two</span></div><div class="line"><span class="comment"># nn.Linear modules which are members of the model.</span></div><div class="line">criterion = torch.nn.MSELoss(size_average=<span class="keyword">False</span>)</div><div class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-4</span>)</div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">    <span class="comment"># Forward pass: Compute predicted y by passing x to the model</span></div><div class="line">    y_pred = model(x)</div><div class="line"></div><div class="line">    <span class="comment"># Compute and print loss</span></div><div class="line">    loss = criterion(y_pred, y)</div><div class="line">    print(t, loss.data[<span class="number">0</span>])</div><div class="line"></div><div class="line">    <span class="comment"># Zero gradients, perform a backward pass, and update the weights.</span></div><div class="line">    optimizer.zero_grad()</div><div class="line">    loss.backward()</div><div class="line">    optimizer.step()</div></pre></td></tr></table></figure>
<h2 id="PyTorch-Control-Flow-Weight-Sharing"><a href="#PyTorch-Control-Flow-Weight-Sharing" class="headerlink" title="PyTorch: Control Flow + Weight Sharing"></a>PyTorch: Control Flow + Weight Sharing</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> random</div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DynamicNet</span><span class="params">(torch.nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, D_in, H, D_out)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        In the constructor we construct three nn.Linear instances that we will use</div><div class="line">        in the forward pass.</div><div class="line">        """</div><div class="line">        super(DynamicNet, self).__init__()</div><div class="line">        self.input_linear = torch.nn.Linear(D_in, H)</div><div class="line">        self.middle_linear = torch.nn.Linear(H, H)</div><div class="line">        self.output_linear = torch.nn.Linear(H, D_out)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        For the forward pass of the model, we randomly choose either 0, 1, 2, or 3</div><div class="line">        and reuse the middle_linear Module that many times to compute hidden layer</div><div class="line">        representations.</div><div class="line"></div><div class="line">        Since each forward pass builds a dynamic computation graph, we can use normal</div><div class="line">        Python control-flow operators like loops or conditional statements when</div><div class="line">        defining the forward pass of the model.</div><div class="line"></div><div class="line">        Here we also see that it is perfectly safe to reuse the same Module many</div><div class="line">        times when defining a computational graph. This is a big improvement from Lua</div><div class="line">        Torch, where each Module could be used only once.</div><div class="line">        """</div><div class="line">        h_relu = self.input_linear(x).clamp(min=<span class="number">0</span>)</div><div class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(random.randint(<span class="number">0</span>, <span class="number">3</span>)):</div><div class="line">            h_relu = self.middle_linear(h_relu).clamp(min=<span class="number">0</span>)</div><div class="line">        y_pred = self.output_linear(h_relu)</div><div class="line">        <span class="keyword">return</span> y_pred</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create random Tensors to hold inputs and outputs, and wrap them in Variables</span></div><div class="line">x = Variable(torch.randn(N, D_in))</div><div class="line">y = Variable(torch.randn(N, D_out), requires_grad=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="comment"># Construct our model by instantiating the class defined above</span></div><div class="line">model = DynamicNet(D_in, H, D_out)</div><div class="line"></div><div class="line"><span class="comment"># Construct our loss function and an Optimizer. Training this strange model with</span></div><div class="line"><span class="comment"># vanilla stochastic gradient descent is tough, so we use momentum</span></div><div class="line">criterion = torch.nn.MSELoss(size_average=<span class="keyword">False</span>)</div><div class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-4</span>, momentum=<span class="number">0.9</span>)</div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">    <span class="comment"># Forward pass: Compute predicted y by passing x to the model</span></div><div class="line">    y_pred = model(x)</div><div class="line"></div><div class="line">    <span class="comment"># Compute and print loss</span></div><div class="line">    loss = criterion(y_pred, y)</div><div class="line">    print(t, loss.data[<span class="number">0</span>])</div><div class="line"></div><div class="line">    <span class="comment"># Zero gradients, perform a backward pass, and update the weights.</span></div><div class="line">    optimizer.zero_grad()</div><div class="line">    loss.backward()</div><div class="line">    optimizer.step()</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Tensor&quot;&gt;&lt;a href=&quot;#Tensor&quot; class=&quot;headerlink&quot; title=&quot;Tensor&quot;&gt;&lt;/a&gt;Tensor&lt;/h1&gt;&lt;h2 id=&quot;Warm-up-numpy&quot;&gt;&lt;a href=&quot;#Warm-up-numpy&quot; class=&quot;he
    
    </summary>
    
      <category term="PyTorch" scheme="http://ynuwm.github.io/categories/PyTorch/"/>
    
    
      <category term="Python" scheme="http://ynuwm.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>BAT机器学习面试1000题系列</title>
    <link href="http://ynuwm.github.io/2017/10/10/BAT%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%951000%E9%A2%98%E7%B3%BB%E5%88%97/"/>
    <id>http://ynuwm.github.io/2017/10/10/BAT机器学习面试1000题系列/</id>
    <published>2017-10-10T12:03:00.000Z</published>
    <updated>2017-12-01T09:03:46.000Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="机器学习" scheme="http://ynuwm.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="面试" scheme="http://ynuwm.github.io/tags/%E9%9D%A2%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>知乎看山杯夺冠记(转)</title>
    <link href="http://ynuwm.github.io/2017/10/08/%E7%9F%A5%E4%B9%8E%E7%9C%8B%E5%B1%B1%E6%9D%AF%E5%A4%BA%E5%86%A0%E8%AE%B0-%E8%BD%AC/"/>
    <id>http://ynuwm.github.io/2017/10/08/知乎看山杯夺冠记-转/</id>
    <published>2017-10-08T14:14:51.000Z</published>
    <updated>2017-12-01T09:03:46.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/28923961" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/28923961</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/28923961&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://zhuanlan.zhihu.com/p/28923961&lt;/a&gt;&lt;/p&gt;

    
    </summary>
    
      <category term="深度学习" scheme="http://ynuwm.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="文本分类" scheme="http://ynuwm.github.io/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>913周会小结</title>
    <link href="http://ynuwm.github.io/2017/09/13/913%E5%91%A8%E4%BC%9A%E5%B0%8F%E7%BB%93/"/>
    <id>http://ynuwm.github.io/2017/09/13/913周会小结/</id>
    <published>2017-09-13T02:20:19.000Z</published>
    <updated>2017-12-01T09:03:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>1,好的人脉和环境真的使人看起来不一样。<br>脚踏实地的钻研的同时也不要忘了交流的重要性。信息的获取不能局限于一个小的范围内。真的，跟一群整天安于现状不求上进的人在一起，自己也会逐渐磨灭前进的意志，在无声无息中。玩游戏,娱乐肯定是必须的，但如果不追求段位的提升，技术的精湛，那多少年过去了，你有何资格说曾经哥也玩过这个游戏。人始终是环境产物，你不明白为何进入传销的人再也走不出来，其实就像此时此刻的你坚信的一些观念，习惯等等其实也就是你所处大环境的东西。要做的不受外界环境和周围人的影响是难的，这大抵就是成大事者必备的不同常人的意志和信念。为什么我很不喜欢云南更多的是因为这里的环境不是我想要的，平台不够好，资源不够多。</p>
<p>２，</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1,好的人脉和环境真的使人看起来不一样。&lt;br&gt;脚踏实地的钻研的同时也不要忘了交流的重要性。信息的获取不能局限于一个小的范围内。真的，跟一群整天安于现状不求上进的人在一起，自己也会逐渐磨灭前进的意志，在无声无息中。玩游戏,娱乐肯定是必须的，但如果不追求段位的提升，技术的精湛
    
    </summary>
    
      <category term="日记" scheme="http://ynuwm.github.io/categories/%E6%97%A5%E8%AE%B0/"/>
    
    
      <category term="开会记录" scheme="http://ynuwm.github.io/tags/%E5%BC%80%E4%BC%9A%E8%AE%B0%E5%BD%95/"/>
    
  </entry>
  
</feed>
