<!doctype html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>






<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="神经网络,keras,无监督学习," />





  <link rel="alternate" href="/atom.xml" title="王敏的博客" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1" />






<meta name="description" content="自动编码器(Autoencoders，AE)是一种前馈无返回的神经网络，有一个输入层，一个隐含层，一个输出层，典型的自动编码器结构如图1所示，在输入层输入X，同时在输出层得到相应的输出Z，层与层之间都采用S型激活函数进行映射。 图1 典型的自动编码器结构  输入层到隐含层的映射关系可以看作是一个编码过程，通过映射函数f把输出向量x映射到隐含层输出y。从隐含层到输出层的过程相当于一个解码过程，把隐含">
<meta name="keywords" content="神经网络,keras,无监督学习">
<meta property="og:type" content="article">
<meta property="og:title" content="自编码器AutoEncoder">
<meta property="og:url" content="http://yoursite.com/2017/05/14/自编码器AutoEncoder/index.html">
<meta property="og:site_name" content="王敏的博客">
<meta property="og:description" content="自动编码器(Autoencoders，AE)是一种前馈无返回的神经网络，有一个输入层，一个隐含层，一个输出层，典型的自动编码器结构如图1所示，在输入层输入X，同时在输出层得到相应的输出Z，层与层之间都采用S型激活函数进行映射。 图1 典型的自动编码器结构  输入层到隐含层的映射关系可以看作是一个编码过程，通过映射函数f把输出向量x映射到隐含层输出y。从隐含层到输出层的过程相当于一个解码过程，把隐含">
<meta property="og:image" content="http://yoursite.com/2017/05/14/自编码器AutoEncoder/640.png">
<meta property="og:image" content="http://yoursite.com/2017/05/14/自编码器AutoEncoder/autoencoder.jpg">
<meta property="og:image" content="http://yoursite.com/2017/05/14/自编码器AutoEncoder/001.png">
<meta property="og:image" content="http://yoursite.com/2017/05/14/自编码器AutoEncoder/003.png">
<meta property="og:image" content="http://yoursite.com/2017/05/14/自编码器AutoEncoder/denoise.png">
<meta property="og:image" content="http://yoursite.com/2017/05/14/自编码器AutoEncoder/vae001.png">
<meta property="og:image" content="http://yoursite.com/2017/05/14/自编码器AutoEncoder/vae002.png">
<meta property="og:image" content="http://yoursite.com/2017/05/14/自编码器AutoEncoder/vae003.png">
<meta property="og:image" content="http://yoursite.com/2017/05/14/自编码器AutoEncoder/p1.png">
<meta property="og:image" content="http://yoursite.com/2017/05/14/自编码器AutoEncoder/p2.png">
<meta property="og:image" content="http://yoursite.com/2017/05/14/自编码器AutoEncoder/p3.png">
<meta property="og:image" content="http://yoursite.com/2017/05/14/自编码器AutoEncoder/p4.png">
<meta property="og:image" content="http://yoursite.com/2017/05/14/自编码器AutoEncoder/p5.png">
<meta property="og:updated_time" content="2017-12-01T09:03:47.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="自编码器AutoEncoder">
<meta name="twitter:description" content="自动编码器(Autoencoders，AE)是一种前馈无返回的神经网络，有一个输入层，一个隐含层，一个输出层，典型的自动编码器结构如图1所示，在输入层输入X，同时在输出层得到相应的输出Z，层与层之间都采用S型激活函数进行映射。 图1 典型的自动编码器结构  输入层到隐含层的映射关系可以看作是一个编码过程，通过映射函数f把输出向量x映射到隐含层输出y。从隐含层到输出层的过程相当于一个解码过程，把隐含">
<meta name="twitter:image" content="http://yoursite.com/2017/05/14/自编码器AutoEncoder/640.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2017/05/14/自编码器AutoEncoder/"/>





  <title>自编码器AutoEncoder | 王敏的博客</title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  














  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">王敏的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">按照自己的方式去度过人生</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocapitalize="off" autocomplete="off" autocorrect="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/05/14/自编码器AutoEncoder/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="王敏">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/000.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="王敏的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                自编码器AutoEncoder
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-05-14T09:30:37+08:00">
                2017-05-14
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Keras/" itemprop="url" rel="index">
                    <span itemprop="name">Keras</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-comment-o"></i>
              </span>
              
                <a href="/2017/05/14/自编码器AutoEncoder/#SOHUCS" itemprop="discussionUrl">
                  <span id="changyan_count_unit" class="post-comments-count hc-comment-count" data-xid="2017/05/14/自编码器AutoEncoder/" itemprop="commentsCount"></span>
                </a>
              
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>自动编码器(Autoencoders，AE)是一种前馈无返回的神经网络，有一个输入层，一个隐含层，一个输出层，典型的自动编码器结构如图1所示，在输入层输入X，同时在输出层得到相应的输出Z，层与层之间都采用S型激活函数进行映射。<br><img src="/2017/05/14/自编码器AutoEncoder/640.png" alt="img"></p>
<center>图1 典型的自动编码器结构</center>

<p>输入层到隐含层的映射关系可以看作是一个编码过程，通过映射函数f把输出向量x映射到隐含层输出y。从隐含层到输出层的过程相当于一个解码过程，把隐含层输出y映射通过映射函数g回去“重构”向量z。对于每一个输入样本x(i)而言，经过自动编码器之后都会转化为一个对应的输出向量z(i)=g[f(x(i))]。当自动编码器训练完成之后，输入X与输出Z完全相同，则对应的隐含层的输出可以看作是输入X的一种抽象表达，因此它可以用于提取输入数据的特征。此外，因为它的隐含层节点数少于输入节点数，因此自动编码器也可以用于降维和数据压缩。网络参数的训练方面，自动编码器采用反向传播法来进行训练，但自动编码器需要大量的训练样本，随着网络结构越变越复杂，网络计算量也随之增大。</p>
<p>对自动编码器结构进行改进得到其他类型的自动编码器，比较典型的是稀疏自动编码器、降噪自动编码器。降噪自动编码器（Denoising Autoencoder，DAE）是对输入数据进行部分“摧毁”，然后通过训练自动编码器模型，重构出原始输入数据，以提高自动编码器的鲁棒性。对输入数据进行“摧毁”的过程其实类似于对数据加入噪声。稀疏自动编码器则是对自动编码器加入一个正则化项，约束隐含层神经元节点大部分输出0，少部分输出非0。稀疏编码器大大减小了需要训练的参数的数目，降低了训练的难度，同时克服了自动编码器容易陷入局部及小值和存在过拟合的问题。降噪编码器采用有噪声的输入数据来训练网络参数，提高了自动编码器的泛化能力。</p>
<p>搭建一个自动编码器需要完成下面三样工作：搭建编码器，搭建解码器，设定一个损失函数，用以衡量由于压缩而损失掉的信息。编码器和解码器一般都是参数化的方程，并关于损失函数可导，典型情况是使用神经网络。编码器和解码器的参数可以通过最小化损失函数而优化，例如SGD。</p>
<p>自编码器只是一种思想，在具体实现中，encoder和decoder可以由多种深度学习模型构成，例如全连接层、卷积层或LSTM等，以下使用Keras来实现用于图像去噪的卷积自编码器。</p>
<p><img src="/2017/05/14/自编码器AutoEncoder/autoencoder.jpg" alt="img"></p>
<h3 id="单隐藏层的自编码器"><a href="#单隐藏层的自编码器" class="headerlink" title="单隐藏层的自编码器"></a>单隐藏层的自编码器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># coding:utf8</span></div><div class="line"><span class="comment"># runing on python 2.7</span></div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input, Dense</div><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</div><div class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment"># 构建单隐藏层的自编码器</span></div><div class="line"><span class="comment"># this is the size of our encoded representations</span></div><div class="line">encoding_dim = <span class="number">32</span>  <span class="comment"># 32 floats -&gt; compression of factor 24.5, assuming the input is 784 floats</span></div><div class="line"></div><div class="line"><span class="comment"># this is our input placeholder</span></div><div class="line">input_img = Input(shape=(<span class="number">784</span>,))</div><div class="line"><span class="comment"># "encoded" is the encoded representation of the input</span></div><div class="line">encoded = Dense(encoding_dim, activation=<span class="string">'relu'</span>)(input_img)</div><div class="line"><span class="comment"># "decoded" is the lossy reconstruction of the input</span></div><div class="line">decoded = Dense(<span class="number">784</span>, activation=<span class="string">'sigmoid'</span>)(encoded)</div><div class="line"></div><div class="line"><span class="comment"># this model maps an input to its reconstruction</span></div><div class="line">autoencoder = Model(input=input_img, output=decoded)</div><div class="line"></div><div class="line"><span class="comment"># 定义编码器</span></div><div class="line"><span class="comment"># this model maps an input to its encoded representation</span></div><div class="line">encoder = Model(input=input_img, output=encoded)</div><div class="line"></div><div class="line"><span class="comment"># 定义解码器</span></div><div class="line"><span class="comment"># create a placeholder for an encoded (32-dimensional) input</span></div><div class="line">encoded_input = Input(shape=(encoding_dim,))</div><div class="line"><span class="comment"># retrieve the last layer of the autoencoder model</span></div><div class="line">decoder_layer = autoencoder.layers[<span class="number">-1</span>]</div><div class="line"><span class="comment"># create the decoder model</span></div><div class="line">decoder = Model(input=encoded_input, output=decoder_layer(encoded_input))</div><div class="line"></div><div class="line">autoencoder.compile(optimizer=<span class="string">'adadelta'</span>, loss=<span class="string">'binary_crossentropy'</span>)</div><div class="line"></div><div class="line"><span class="comment">#准备数据</span></div><div class="line">(x_train, _), (x_test, _) = mnist.load_data()</div><div class="line">x_train = x_train.astype(<span class="string">'float32'</span>) / <span class="number">255.</span></div><div class="line">x_test = x_test.astype(<span class="string">'float32'</span>) / <span class="number">255.</span></div><div class="line">x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[<span class="number">1</span>:])))</div><div class="line">x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[<span class="number">1</span>:])))</div><div class="line"><span class="keyword">print</span> x_train.shape</div><div class="line"><span class="keyword">print</span> x_test.shape</div><div class="line"></div><div class="line"><span class="comment"># 训练模型</span></div><div class="line">autoencoder.fit(x_train, x_train,</div><div class="line">                nb_epoch=<span class="number">50</span>,</div><div class="line">                batch_size=<span class="number">256</span>,</div><div class="line">                shuffle=<span class="keyword">True</span>,</div><div class="line">                validation_data=(x_test, x_test))</div><div class="line"></div><div class="line"><span class="comment"># 在测试集上进行编码和解码</span></div><div class="line"><span class="comment"># encode and decode some digits</span></div><div class="line"><span class="comment"># note that we take them from the *test* set</span></div><div class="line">encoded_imgs = encoder.predict(x_test)</div><div class="line">decoded_imgs = decoder.predict(encoded_imgs)</div><div class="line"></div><div class="line"><span class="comment"># 显示结果</span></div><div class="line">n = <span class="number">10</span>  <span class="comment"># how many digits we will display</span></div><div class="line">plt.figure(figsize=(<span class="number">20</span>, <span class="number">4</span>))</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</div><div class="line">    <span class="comment"># display original</span></div><div class="line">    ax = plt.subplot(<span class="number">2</span>, n, i + <span class="number">1</span>)</div><div class="line">    plt.imshow(x_test[i].reshape(<span class="number">28</span>, <span class="number">28</span>))</div><div class="line">    plt.gray()</div><div class="line">    ax.get_xaxis().set_visible(<span class="keyword">False</span>)</div><div class="line">    ax.get_yaxis().set_visible(<span class="keyword">False</span>)</div><div class="line"></div><div class="line">    <span class="comment"># display reconstruction</span></div><div class="line">    ax = plt.subplot(<span class="number">2</span>, n, i + <span class="number">1</span> + n)</div><div class="line">    plt.imshow(decoded_imgs[i].reshape(<span class="number">28</span>, <span class="number">28</span>))</div><div class="line">    plt.gray()</div><div class="line">    ax.get_xaxis().set_visible(<span class="keyword">False</span>)</div><div class="line">    ax.get_yaxis().set_visible(<span class="keyword">False</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p>50个epoch后，看起来自编码器优化的不错了，损失是0.10，我们可视化一下重构出来的输出.上面是原始图像，下面为重构图像，用此方法丢失了太多细节。<br><img src="/2017/05/14/自编码器AutoEncoder/001.png" alt="img"></p>
<h3 id="稀疏约束的自编码器"><a href="#稀疏约束的自编码器" class="headerlink" title="稀疏约束的自编码器"></a>稀疏约束的自编码器</h3><p>上面的隐层有32个神经元，这种情况下，一般而言自编码器学到的是PCA的一个近似。但是如果我们对隐层单元施加稀疏性约束的话，会得到更为紧凑的表达，只有一小部分神经元会被激活。在Keras中，可以通过添加一个activity_regularizer达到对某层激活值进行约束的目的：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> regularizers</div><div class="line"></div><div class="line">encoding_dim = <span class="number">32</span></div><div class="line">input_img = Input(shape=(<span class="number">784</span>,))</div><div class="line"><span class="comment"># add a Dense layer with a L1 activity regularizer</span></div><div class="line">encoded = Dense(encoding_dim, activation=<span class="string">'relu'</span>,</div><div class="line">                activity_regularizer=regularizers.l1(<span class="number">10e-5</span>))(input_img)</div><div class="line">decoded = Dense(<span class="number">784</span>, activation=<span class="string">'sigmoid'</span>)(encoded)</div><div class="line">autoencoder = Model(input_img, decoded)</div></pre></td></tr></table></figure></p>
<p>因为添加了正则性约束，所以模型过拟合的风险降低，可以训练多几次，这次训练100个epoch，得到损失为0.11，多出来的0.01基本上是由于正则项造成的。可视化结果如下：</p>
<p>结果上没有什么差别，区别在于编码出来的码字更加稀疏了。稀疏自编码器的在10000个测试图片上的码字均值为3.33，而之前的为7.30</p>
<h3 id="多隐藏层的自编码器"><a href="#多隐藏层的自编码器" class="headerlink" title="多隐藏层的自编码器"></a>多隐藏层的自编码器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># coding:utf8</span></div><div class="line"><span class="comment"># runing on python 2.7</span></div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input, Dense</div><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</div><div class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">encoding_dim = <span class="number">32</span></div><div class="line"></div><div class="line">input_img = Input(shape=(<span class="number">784</span>,))</div><div class="line"></div><div class="line">encoded = Dense(<span class="number">128</span>, activation=<span class="string">'relu'</span>)(input_img)</div><div class="line">encoded = Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>)(encoded)</div><div class="line">encoded = Dense(<span class="number">32</span>, activation=<span class="string">'relu'</span>)(encoded)</div><div class="line"></div><div class="line">decoded = Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>)(encoded)</div><div class="line">decoded = Dense(<span class="number">128</span>, activation=<span class="string">'relu'</span>)(decoded)</div><div class="line">decoded = Dense(<span class="number">784</span>, activation=<span class="string">'sigmoid'</span>)(decoded)</div><div class="line"></div><div class="line">autoencoder = Model(input=input_img, output=decoded)</div><div class="line">autoencoder.compile(optimizer=<span class="string">'adadelta'</span>, loss=<span class="string">'binary_crossentropy'</span>)</div><div class="line"></div><div class="line"><span class="comment">#准备数据</span></div><div class="line">(x_train, _), (x_test, _) = mnist.load_data()</div><div class="line">x_train = x_train.astype(<span class="string">'float32'</span>) / <span class="number">255.</span></div><div class="line">x_test = x_test.astype(<span class="string">'float32'</span>) / <span class="number">255.</span></div><div class="line">x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[<span class="number">1</span>:])))</div><div class="line">x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[<span class="number">1</span>:])))</div><div class="line"><span class="keyword">print</span> x_train.shape</div><div class="line"><span class="keyword">print</span> x_test.shape</div><div class="line"></div><div class="line"><span class="comment"># 训练模型</span></div><div class="line">autoencoder.fit(x_train, x_train,</div><div class="line">                nb_epoch=<span class="number">100</span>,</div><div class="line">                batch_size=<span class="number">256</span>,</div><div class="line">                shuffle=<span class="keyword">True</span>,</div><div class="line">                validation_data=(x_test, x_test))</div><div class="line"></div><div class="line">decoded_imgs = autoencoder.predict(x_test)</div><div class="line"></div><div class="line"><span class="comment"># 显示结果</span></div><div class="line"></div><div class="line">n = <span class="number">10</span>  <span class="comment"># how many digits we will display</span></div><div class="line">plt.figure(figsize=(<span class="number">20</span>, <span class="number">4</span>))</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</div><div class="line">    <span class="comment"># display original</span></div><div class="line">    ax = plt.subplot(<span class="number">2</span>, n, i + <span class="number">1</span>)</div><div class="line">    plt.imshow(x_test[i].reshape(<span class="number">28</span>, <span class="number">28</span>))</div><div class="line">    plt.gray()</div><div class="line">    ax.get_xaxis().set_visible(<span class="keyword">False</span>)</div><div class="line">    ax.get_yaxis().set_visible(<span class="keyword">False</span>)</div><div class="line"></div><div class="line">    <span class="comment"># display reconstruction</span></div><div class="line">    ax = plt.subplot(<span class="number">2</span>, n, i + <span class="number">1</span> + n)</div><div class="line">    plt.imshow(decoded_imgs[i].reshape(<span class="number">28</span>, <span class="number">28</span>))</div><div class="line">    plt.gray()</div><div class="line">    ax.get_xaxis().set_visible(<span class="keyword">False</span>)</div><div class="line">    ax.get_yaxis().set_visible(<span class="keyword">False</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p>100个epoch后，loss大概是0.097，比之前的模型好那么一点点。<br><img src="/2017/05/14/自编码器AutoEncoder/003.png" alt="img"></p>
<h3 id="卷积自编码器"><a href="#卷积自编码器" class="headerlink" title="卷积自编码器"></a>卷积自编码器</h3><p>由于输入是图像，因此使用卷积神经网络（convnets）作为编码器和解码器是有意义的。在实际设置中，应用于图像的自动编码器始终是卷积自动编码器 - 它们的性能要好得多。编码器将由栈Conv2D和MaxPooling2D层组成（最大池用于空间下采样），而解码器将由Conv2D和UpSampling2D层组成。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"></div></pre></td></tr></table></figure></p>
<h3 id="自编码器图像去噪"><a href="#自编码器图像去噪" class="headerlink" title="自编码器图像去噪"></a>自编码器图像去噪</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># coding:utf8</span></div><div class="line"><span class="comment"># runing on python 2.7</span></div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input, Dense, Convolution2D, MaxPooling2D, UpSampling2D</div><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</div><div class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> TensorBoard</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment"># 获取数据集MNIST，将像素点值转化到0-1区间，并且重塑为N×1×28×28的四维tensor</span></div><div class="line">(x_train, _), (x_test, _) = mnist.load_data()</div><div class="line"></div><div class="line">x_train = x_train.astype(<span class="string">'float32'</span>) / <span class="number">255.</span></div><div class="line">x_test = x_test.astype(<span class="string">'float32'</span>) / <span class="number">255.</span></div><div class="line">x_train = np.reshape(x_train, (len(x_train),<span class="number">28</span>, <span class="number">28</span>,<span class="number">1</span>))</div><div class="line">x_test = np.reshape(x_test, (len(x_test),<span class="number">28</span>, <span class="number">28</span>,<span class="number">1</span>))</div><div class="line"></div><div class="line"><span class="comment"># 添加噪声，即叠加一个随机的高斯白噪声，并限制加噪之后的值仍处于0-1区间</span></div><div class="line">noise_factor = <span class="number">0.5</span></div><div class="line">x_train_noisy = x_train + noise_factor * np.random.normal(loc=<span class="number">0.0</span>, scale=<span class="number">1.0</span>, size=x_train.shape)</div><div class="line">x_test_noisy = x_test + noise_factor * np.random.normal(loc=<span class="number">0.0</span>, scale=<span class="number">1.0</span>, size=x_test.shape)</div><div class="line"></div><div class="line">x_train_noisy = np.clip(x_train_noisy, <span class="number">0.</span>, <span class="number">1.</span>)</div><div class="line">x_test_noisy = np.clip(x_test_noisy, <span class="number">0.</span>, <span class="number">1.</span>)</div><div class="line"></div><div class="line">input_img = Input(shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>))  <span class="comment"># adapt this if using `channels_first` image data format</span></div><div class="line"></div><div class="line"><span class="comment"># 定义encoder部分，由两个32×3×3的卷积层和两个2×2的最大池化层组成</span></div><div class="line">x = Convolution2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>)(input_img)</div><div class="line">x = MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>), padding=<span class="string">'same'</span>)(x)</div><div class="line">x = Convolution2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>)(x)</div><div class="line">encoded = MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>), padding=<span class="string">'same'</span>)(x)</div><div class="line"></div><div class="line"><span class="comment"># at this point the representation is (7, 7, 32)</span></div><div class="line"><span class="comment"># 定义decoder部分，由两个32×3×3的卷积层和两个2×2的上采样层组成。</span></div><div class="line">x = Convolution2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>)(encoded)</div><div class="line">x = UpSampling2D((<span class="number">2</span>, <span class="number">2</span>))(x)</div><div class="line">x = Convolution2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, padding=<span class="string">'same'</span>)(x)</div><div class="line">x = UpSampling2D((<span class="number">2</span>, <span class="number">2</span>))(x)</div><div class="line">decoded = Convolution2D(<span class="number">1</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'sigmoid'</span>, padding=<span class="string">'same'</span>)(x)</div><div class="line"></div><div class="line"><span class="comment"># 输入和输出连接起来，构成autoencoder并compile</span></div><div class="line">autoencoder = Model(input_img, decoded)</div><div class="line">autoencoder.compile(optimizer=<span class="string">'adadelta'</span>, loss=<span class="string">'binary_crossentropy'</span>)</div><div class="line"></div><div class="line"><span class="comment"># 使用x_train作为输入和输出来训练我们的autoencoder，并使用x_test进行validation</span></div><div class="line">autoencoder.fit(x_train_noisy, x_train,</div><div class="line">                epochs=<span class="number">100</span>,</div><div class="line">                batch_size=<span class="number">128</span>,</div><div class="line">                shuffle=<span class="keyword">True</span>,</div><div class="line">                validation_data=(x_test_noisy, x_test),</div><div class="line">                callbacks=[TensorBoard(log_dir=<span class="string">'/tmp/tb'</span>, histogram_freq=<span class="number">0</span>, write_graph=<span class="keyword">False</span>)])</div><div class="line"></div><div class="line"><span class="comment"># 使用autoencoder对x_test预测，并将预测结果绘制出来，和原始加噪图像进行对比</span></div><div class="line">decoded_imgs = autoencoder.predict(x_test_noisy)</div><div class="line"></div><div class="line"></div><div class="line">n = <span class="number">10</span></div><div class="line">plt.figure(figsize=(<span class="number">20</span>, <span class="number">4</span>))</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</div><div class="line">    <span class="comment"># display original</span></div><div class="line">    ax = plt.subplot(<span class="number">2</span>, n, i + <span class="number">1</span>)</div><div class="line">    plt.imshow(x_test_noisy[i].reshape(<span class="number">28</span>, <span class="number">28</span>))</div><div class="line">    plt.gray()</div><div class="line">    ax.get_xaxis().set_visible(<span class="keyword">False</span>)</div><div class="line">    ax.get_yaxis().set_visible(<span class="keyword">False</span>)</div><div class="line"></div><div class="line">    <span class="comment"># display reconstruction</span></div><div class="line">    ax = plt.subplot(<span class="number">2</span>, n, i + <span class="number">1</span> + n)</div><div class="line">    plt.imshow(decoded_imgs[i].reshape(<span class="number">28</span>, <span class="number">28</span>))</div><div class="line">    plt.gray()</div><div class="line">    ax.get_xaxis().set_visible(<span class="keyword">False</span>)</div><div class="line">    ax.get_yaxis().set_visible(<span class="keyword">False</span>)</div><div class="line">plt.show()</div><div class="line"></div><div class="line"><span class="comment"># 加噪之后的结果</span></div><div class="line"><span class="comment"># n = 10</span></div><div class="line"><span class="comment"># plt.figure(figsize=(20, 2))</span></div><div class="line"><span class="comment"># for i in range(n):</span></div><div class="line"><span class="comment">#     ax = plt.subplot(1, n, i + 1)</span></div><div class="line"><span class="comment">#     plt.imshow(x_test_noisy[i].reshape(28, 28))</span></div><div class="line"><span class="comment">#     plt.gray()</span></div><div class="line"><span class="comment">#     ax.get_xaxis().set_visible(False)</span></div><div class="line"><span class="comment">#     ax.get_yaxis().set_visible(False)</span></div><div class="line"><span class="comment"># plt.show()</span></div></pre></td></tr></table></figure>
<p>运行结果：上面一行是添加噪音的图像，下面一行是去噪之后的结果<br><img src="/2017/05/14/自编码器AutoEncoder/denoise.png" alt="img"></p>
<h3 id="Seq2Seq自编码器"><a href="#Seq2Seq自编码器" class="headerlink" title="Seq2Seq自编码器"></a>Seq2Seq自编码器</h3><p>如果输入的是序列，而不是向量或2D图像，首先使用LSTM编码器将输入序列转换成包含整个序列信息的单个向量，然后重复该向量n时间（n输出序列中的时间步长数），并运行一个LSTM解码器将该恒定序列转换成目标序列。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input, LSTM, RepeatVector</div><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</div><div class="line"></div><div class="line">inputs = Input(shape=(timesteps, input_dim))</div><div class="line">encoded = LSTM(latent_dim)(inputs)</div><div class="line"></div><div class="line">decoded = RepeatVector(timesteps)(encoded)</div><div class="line">decoded = LSTM(input_dim, return_sequences=<span class="keyword">True</span>)(decoded)</div><div class="line"></div><div class="line">sequence_autoencoder = Model(inputs, decoded)</div><div class="line">encoder = Model(inputs, encoded)</div></pre></td></tr></table></figure></p>
<h3 id="变分自编码-VAE"><a href="#变分自编码-VAE" class="headerlink" title="变分自编码 VAE"></a>变分自编码 VAE</h3><h4 id="VAE结构"><a href="#VAE结构" class="headerlink" title="VAE结构"></a>VAE结构</h4><p>概率解释的神经网络通过假设每个参数的概率分布来降低网络中每个参数的单个值的刚性约束。例如，在经典神经网络中计算权重w_i=0.7，在概率版本中，计算均值大约为u_i = 0.7和方差为v_i = 0.1的高斯分布，即w_i =N（0.7,0.1）。这个假设将输入，隐藏表示以及神经网络的输出转换为概率随机变量。这类网络被称为贝叶斯神经网络或BNN。</p>
<p>encoder、decoder:均可为任意结构<br>encoder 又称 recognition model<br>decoder 又称 generative model<br>encoder 的输出（2×m 个数）视作分别为 m 个高斯分布的均值（z_mean）和方差的对数<br><img src="/2017/05/14/自编码器AutoEncoder/vae001.png" alt="img"><br>一段VAE伪代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">network= &#123;</div><div class="line"></div><div class="line">  <span class="comment"># encoder</span></div><div class="line">  encoder_x = Input_layer(size=input_size, input=data)</div><div class="line">  encoder_h = Dense_layer(size=hidden_size, input= encoder_x)</div><div class="line"></div><div class="line">  <span class="comment"># the re-parameterized distributions that are inferred from data</span></div><div class="line">  z_mean = Dense(size=number_of_distributions, input=encoder_h)</div><div class="line">  z_variance = Dense(size=number_of_distributions, input=encoder_h)</div><div class="line">  epsilon= random(size=number_of_distributions)</div><div class="line"></div><div class="line">  <span class="comment"># decoder network needs a sample from the code distribution</span></div><div class="line">  z_sample= z_mean + exp(z_variance / <span class="number">2</span>) * epsilon</div><div class="line"></div><div class="line">  <span class="comment">#decoder</span></div><div class="line">  decoder_h = Dense_layer(size=hidden_size, input=z_sample)</div><div class="line">  decoder_output = Dense_layer(size=input_size, input=decoder_h)</div><div class="line">&#125;</div><div class="line"></div><div class="line">cost=&#123;</div><div class="line">  reconstruction_loss = input_size * crossentropy(data, decoder_output)</div><div class="line">  kl_loss = - <span class="number">0.5</span> * sum(<span class="number">1</span> + z_variance - square(z_mean) - exp(z_variance))</div><div class="line">  cost_total= reconstruction_loss + kl_loss</div><div class="line">&#125;</div><div class="line"></div><div class="line">stochastic_gradient_descent(data, network, cost_total)</div></pre></td></tr></table></figure></p>
<h4 id="采样（sampling）"><a href="#采样（sampling）" class="headerlink" title="采样（sampling）"></a>采样（sampling）</h4><p>根据 encoder 输出的均值与方差，生成服从相应高斯分布的随机数：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">epsilon = K.random_normal(shape=(batch_size, m), mean=0.,std=epsilon_std) # 标准高斯分布</div><div class="line">z = z_mean + exp(z_log_var / 2) * epsilon</div></pre></td></tr></table></figure></p>
<p>z 就可以作为上面定义的 decoder 的输入，进而产生 n 维的输出 x^<br><img src="/2017/05/14/自编码器AutoEncoder/vae002.png" alt="img"><br>这里运用了 reparemerization 的技巧。由于 z∼N(μ,σ)，我们应该从 N(μ,σ) 采样，但这个采样操作对 μ 和 σ 是不可导的，导致常规的通过误差反传的梯度下降法（GD）不能使用。通过 reparemerization，我们首先从 N(0,1) 上采样 ϵ，然后，z=σ⋅ϵ+μ。这样，z∼N(μ,σ)，而且，从 encoder 输出到 z，只涉及线性操作，（ϵ 对神经网络而言只是常数），因此，可以正常使用 GD 进行优化。</p>
<p><img src="/2017/05/14/自编码器AutoEncoder/vae003.png" alt="img"></p>
<h4 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h4><p>encoder 和 decoder 组合在一起，我们能够对每个 x∈X，输出一个相同维度的 x^。我们目标是，令 x^ 与 x 自身尽量的接近。即 x 经过编码（encode）后，能够通过解码（decode）尽可能多的恢复出原来的信息。<br>由于 x∈[0,1]，因此用交叉熵（cross entropy）度量 x 与 x^ 差异：<br>$$xent = \sum_{i=1}^n-[x_i\cdot\log(\hat{x}_i)+(1-x_i)\cdot\log(1-\hat{x}_i)]$$<br>xent 越小，x 与 x^ 越接近。</p>
<p>也可以用均方误差来度量：<br>$$mse=\sum_{i=1}^n(x_i - \hat{x}_i)^2$$<br>mse 越小，两者越接近。</p>
<p>另外，需要对 encoder 的输出 z_mean（μ）及 z_log_var（logσ2）加以约束。这里使用的是 KL 散度：</p>
<p>$$KL = -0.5 * (1+\log\sigma^2-\mu^2-\sigma^2)=-0.5(1+\log\sigma^2-\mu^2-exp(\log\sigma^2))$$<br>总的优化目标（最小化）为：<br>$$loss = xent + KL$$或者<br>$$loss = mse + KL$$</p>
<p>综上所述，有了目标函数，并且从输入到输出的所有运算都可导，就可以通过 SGD 或其改进方法来训练这个网络了。训练过程只用到 x（同时作为输入和目标输出），而与 x 的标签无关，因此，这是无监督学习。</p>
<p>代码实现：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div></pre></td><td class="code"><pre><div class="line">import numpy as np  </div><div class="line">import matplotlib.pyplot as plt  </div><div class="line">from scipy.stats import norm  </div><div class="line">  </div><div class="line">from keras.layers import Input, Dense, Lambda  </div><div class="line">from keras.models import Model  </div><div class="line">from keras import backend as K  </div><div class="line">from keras import objectives  </div><div class="line">from keras.datasets import mnist  </div><div class="line">from keras.utils import plot_model</div><div class="line">import sys  </div><div class="line">  </div><div class="line">saveout = sys.stdout  </div><div class="line">file = open(&apos;variational_autoencoder.txt&apos;,&apos;w&apos;)  </div><div class="line">sys.stdout = file  </div><div class="line">  </div><div class="line">batch_size = 100  </div><div class="line">original_dim = 784   #28*28  </div><div class="line">latent_dim = 2  </div><div class="line">intermediate_dim = 256  </div><div class="line">nb_epoch = 50  </div><div class="line">epsilon_std = 1.0  </div><div class="line">  </div><div class="line">#my tips:encoding  </div><div class="line">x = Input(batch_shape=(batch_size, original_dim))  </div><div class="line">h = Dense(intermediate_dim, activation=&apos;relu&apos;)(x)  </div><div class="line">z_mean = Dense(latent_dim)(h)  </div><div class="line">z_log_var = Dense(latent_dim)(h)  </div><div class="line">  </div><div class="line">#my tips:Gauss sampling,sample Z  </div><div class="line">def sampling(args):   </div><div class="line">    z_mean, z_log_var = args  </div><div class="line">    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0.,  </div><div class="line">                              stddev=epsilon_std)  </div><div class="line">    return z_mean + K.exp(z_log_var / 2) * epsilon  </div><div class="line">  </div><div class="line"># note that &quot;output_shape&quot; isn&apos;t necessary with the TensorFlow backend  </div><div class="line"># my tips:get sample z(encoded)  </div><div class="line">z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])  </div><div class="line">  </div><div class="line"># we instantiate these layers separately so as to reuse them later  </div><div class="line">decoder_h = Dense(intermediate_dim, activation=&apos;relu&apos;)  </div><div class="line">decoder_mean = Dense(original_dim, activation=&apos;sigmoid&apos;)  </div><div class="line">h_decoded = decoder_h(z)  </div><div class="line">x_decoded_mean = decoder_mean(h_decoded)  </div><div class="line">  </div><div class="line">#my tips:loss(restruct X)+KL  </div><div class="line">def vae_loss(x, x_decoded_mean):  </div><div class="line">      #my tips:logloss  </div><div class="line">    xent_loss = original_dim * objectives.binary_crossentropy(x, x_decoded_mean)  </div><div class="line">    #my tips:see paper&apos;s appendix B  </div><div class="line">    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)  </div><div class="line">    return xent_loss + kl_loss  </div><div class="line">  </div><div class="line">vae = Model(x, x_decoded_mean)  </div><div class="line">vae.compile(optimizer=&apos;rmsprop&apos;, loss=vae_loss)  </div><div class="line">  </div><div class="line"># train the VAE on MNIST digits  </div><div class="line">(x_train, y_train), (x_test, y_test) = mnist.load_data(path=&apos;mnist.pkl.gz&apos;)  </div><div class="line">  </div><div class="line">x_train = x_train.astype(&apos;float32&apos;) / 255.  </div><div class="line">x_test = x_test.astype(&apos;float32&apos;) / 255.  </div><div class="line">x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))  </div><div class="line">x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))  </div><div class="line">  </div><div class="line">vae.fit(x_train, x_train,  </div><div class="line">        shuffle=True,  </div><div class="line">        nb_epoch=nb_epoch,  </div><div class="line">        verbose=2,  </div><div class="line">        batch_size=batch_size,  </div><div class="line">        validation_data=(x_test, x_test))  </div><div class="line">  </div><div class="line"># build a model to project inputs on the latent space  </div><div class="line">encoder = Model(x, z_mean)  </div><div class="line">  </div><div class="line"># display a 2D plot of the digit classes in the latent space  </div><div class="line">x_test_encoded = encoder.predict(x_test, batch_size=batch_size)  </div><div class="line">plt.figure(figsize=(6, 6))  </div><div class="line">plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)  </div><div class="line">plt.colorbar()  </div><div class="line">plt.show()  </div><div class="line">  </div><div class="line"># build a digit generator that can sample from the learned distribution  </div><div class="line">decoder_input = Input(shape=(latent_dim,))  </div><div class="line">_h_decoded = decoder_h(decoder_input)  </div><div class="line">_x_decoded_mean = decoder_mean(_h_decoded)  </div><div class="line">generator = Model(decoder_input, _x_decoded_mean)  </div><div class="line">  </div><div class="line"># display a 2D manifold of the digits  </div><div class="line">n = 15  # figure with 15x15 digits  </div><div class="line">digit_size = 28  </div><div class="line">figure = np.zeros((digit_size * n, digit_size * n))  </div><div class="line"># linearly spaced coordinates on the unit square were transformed through the inverse CDF (ppf) of the Gaussian  </div><div class="line"># to produce values of the latent variables z, since the prior of the latent space is Gaussian  </div><div class="line">grid_x = norm.ppf(np.linspace(0.05, 0.95, n))  </div><div class="line">grid_y = norm.ppf(np.linspace(0.05, 0.95, n))  </div><div class="line">  </div><div class="line">for i, yi in enumerate(grid_x):  </div><div class="line">    for j, xi in enumerate(grid_y):  </div><div class="line">        z_sample = np.array([[xi, yi]])  </div><div class="line">        x_decoded = generator.predict(z_sample)  </div><div class="line">        digit = x_decoded[0].reshape(digit_size, digit_size)  </div><div class="line">        figure[i * digit_size: (i + 1) * digit_size,  </div><div class="line">               j * digit_size: (j + 1) * digit_size] = digit  </div><div class="line">  </div><div class="line">plt.figure(figsize=(10, 10))  </div><div class="line">plt.imshow(figure, cmap=&apos;Greys_r&apos;)  </div><div class="line">plt.show()  </div><div class="line"></div><div class="line">plot_model(vae,to_file=&apos;variational_autoencoder_vae.png&apos;,show_shapes=True)  </div><div class="line">plot_model(encoder,to_file=&apos;variational_autoencoder_encoder.png&apos;,show_shapes=True)  </div><div class="line">plot_model(generator,to_file=&apos;variational_autoencoder_generator.png&apos;,show_shapes=True)  </div><div class="line">  </div><div class="line">sys.stdout.close()  </div><div class="line">sys.stdout = saveout</div></pre></td></tr></table></figure></p>
<p>VAE形状：<br><img src="/2017/05/14/自编码器AutoEncoder/p1.png" alt="img"></p>
<p>编码器形状：<br><img src="/2017/05/14/自编码器AutoEncoder/p2.png" alt="img"></p>
<p>代码中将编码得到的均值U可视化结果：<br><img src="/2017/05/14/自编码器AutoEncoder/p3.png" alt="img"></p>
<p>生成器形状：<br><img src="/2017/05/14/自编码器AutoEncoder/p4.png" alt="img"></p>
<p>可将从二维高斯分布中随机采样得到的Z，解码成手写数字图片<br>代码中将解码得到的图像可视化：<br><img src="/2017/05/14/自编码器AutoEncoder/p5.png" alt="img"></p>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>学习算法的最好方式还是读代码，网上有许多基于不同框架的 VAE 参考实现，如<br><a href="https://jmetzen.github.io/2015-11-27/vae.html" rel="external" color="blue" target="_blank">tensorflow </a>、<a href="https://github.com/y0ast/Variational-Autoencoder/blob/master/VAE.py" rel="external" color="blue" target="_blank">theano</a>、<a href="https://github.com/fchollet/keras/blob/master/examples/variational_autoencoder.py" rel="external" target="_blank">keras</a>、<a href="https://github.com/y0ast/VAE-Torch" rel="external" target="_blank">torch</a></p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        
  <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
    <div>如果文章对您有用请随意打赏，谢谢支持！</div>
    <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
      <span>赏</span>
    </button>
    <div id="QR" style="display: none;">
      
        <div id="wechat" style="display: inline-block">
          <img id="wechat_qr" src="/images/w.png" alt="王敏 WeChat Pay"/>
          <p>微信打赏</p>
        </div>
      
      
        <div id="alipay" style="display: inline-block">
          <img id="alipay_qr" src="/images/z.png" alt="王敏 Alipay"/>
          <p>支付宝打赏</p>
        </div>
      
    </div>
  </div>


      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/神经网络/" rel="tag"># 神经网络</a>
          
            <a href="/tags/keras/" rel="tag"># keras</a>
          
            <a href="/tags/无监督学习/" rel="tag"># 无监督学习</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/05/10/R语言之ggplot绘图/" rel="next" title="R语言之ggplot绘图">
                <i class="fa fa-chevron-left"></i> R语言之ggplot绘图
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/05/15/没有故事的男生-转/" rel="prev" title="没有故事的男生(转)">
                没有故事的男生(转) <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="SOHUCS"></div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/000.jpg"
               alt="王敏" />
          <p class="site-author-name" itemprop="name">王敏</p>
           
              <p class="site-description motion-element" itemprop="description">一蓑烟雨任平生</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">38</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">15</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">42</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/ynuwm" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://twitter.com/ynuwm" target="_blank" title="Twitter">
                  
                    <i class="fa fa-fw fa-twitter"></i>
                  
                  Twitter
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://www.weibo.com/jcshaozhu" target="_blank" title="Weibo">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                  Weibo
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.zhihu.com/people/justanonymous/answers" target="_blank" title="Zhihu">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  Zhihu
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              友情链接
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://www.jeyzhang.com/" title="JEY" target="_blank">JEY</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://www.flickering.cn/" title="FLICK" target="_blank">FLICK</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://www.52ml.net/" title="52ML" target="_blank">52ML</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://www.52nlp.cn/" title="52NLP" target="_blank">52NLP</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://52opencourse.com/" title="52OPENC" target="_blank">52OPENC</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://blog.csdn.net/wangxinginnlp/article/details/44890553" title="NLPGROUP" target="_blank">NLPGROUP</a>
                </li>
              
            </ul>
          </div>
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#单隐藏层的自编码器"><span class="nav-number">1.</span> <span class="nav-text">单隐藏层的自编码器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#稀疏约束的自编码器"><span class="nav-number">2.</span> <span class="nav-text">稀疏约束的自编码器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#多隐藏层的自编码器"><span class="nav-number">3.</span> <span class="nav-text">多隐藏层的自编码器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#卷积自编码器"><span class="nav-number">4.</span> <span class="nav-text">卷积自编码器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#自编码器图像去噪"><span class="nav-number">5.</span> <span class="nav-text">自编码器图像去噪</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Seq2Seq自编码器"><span class="nav-number">6.</span> <span class="nav-text">Seq2Seq自编码器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#变分自编码-VAE"><span class="nav-number">7.</span> <span class="nav-text">变分自编码 VAE</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#VAE结构"><span class="nav-number">7.1.</span> <span class="nav-text">VAE结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#采样（sampling）"><span class="nav-number">7.2.</span> <span class="nav-text">采样（sampling）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#优化目标"><span class="nav-number">7.3.</span> <span class="nav-text">优化目标</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#小结"><span class="nav-number">7.4.</span> <span class="nav-text">小结</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">王敏</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
  <span>&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;</span>
  <span id="showDays"></span>
</div>


<script>
	var birthDay = new Date('04/30/2017');
	var now = new Date();
	var duration = now.getTime() - birthDay.getTime();
	var total= Math.floor(duration / (1000 * 60 * 60 * 24));
	document.getElementById('showDays').innerHTML='本站已运行' + total + '天';
</script>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  




	





  





  





  




  
    <script type="text/javascript">
    (function(){
      var appid = 'cysYjnueo';
      var conf = 'b92520c5cf51563daa727ea70256e46d';
      var width = window.innerWidth || document.documentElement.clientWidth;
      if (width < 960) {
      window.document.write('<script id="changyan_mobile_js" charset="utf-8" type="text/javascript" src="https://changyan.sohu.com/upload/mobile/wap-js/changyan_mobile.js?client_id=' + appid + '&conf=' + conf + '"><\/script>'); } else { var loadJs=function(d,a){var c=document.getElementsByTagName("head")[0]||document.head||document.documentElement;var b=document.createElement("script");b.setAttribute("type","text/javascript");b.setAttribute("charset","UTF-8");b.setAttribute("src",d);if(typeof a==="function"){if(window.attachEvent){b.onreadystatechange=function(){var e=b.readyState;if(e==="loaded"||e==="complete"){b.onreadystatechange=null;a()}}}else{b.onload=a}}c.appendChild(b)};loadJs("https://changyan.sohu.com/upload/changyan.js",function(){
        window.changyan.api.config({appid:appid,conf:conf})});
      }
    })();
    </script>
    <script type="text/javascript" src="https://assets.changyan.sohu.com/upload/plugins/plugins.count.js"></script>
  



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (search_path.endsWith("json")) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      $('#local-search-input').focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
